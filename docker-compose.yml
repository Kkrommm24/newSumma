services:
  backend:
    build: ./backend
    env_file:
      - .env
    ports:
      - "8000:8000"
    volumes:
      - ./backend:/app
      - ./backend/llama_finetune_model:/app/llama_finetune_model
      - static_volume:/app/static
    working_dir: /app
    depends_on:
      db:
        condition: service_healthy
    command: gunicorn --bind 0.0.0.0:8000 backend.wsgi:application
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  frontend:
    build: ./frontend
    ports:
      - "5173:5173"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    depends_on:
      - backend

  db:
    image: postgres:latest
    restart: always
    env_file:
      - .env
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $POSTGRES_USER -d $POSTGRES_DB"]
      interval: 5s
      retries: 5
  
  redis:
    image: redis:latest
    restart: always
    ports:
      - "6379:6379"

  celery_worker:
    build: ./backend
    env_file:
      - .env
    volumes:
      - ./backend:/app
      - ./backend/llama_finetune_model:/app/llama_finetune_model
    working_dir: /app
    environment:
      - PYTHONPATH=/app
      - DJANGO_SETTINGS_MODULE=backend.settings
      - LLAMA_MODEL_PATH=/app/llama_finetune_model
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      - backend
      - redis
    command: python -m celery -A backend worker --loglevel=info

  celery_beat:
    build: ./backend
    env_file:
      - .env
    volumes:
      - ./backend:/app
      - ./backend/llama_finetune_model:/app/llama_finetune_model
    working_dir: /app
    environment:
      - PYTHONPATH=/app
      - DJANGO_SETTINGS_MODULE=backend.settings
      - LLAMA_MODEL_PATH=/app/llama_finetune_model
    depends_on:
      - backend
      - redis
      - celery_worker
    command: python -m celery -A backend beat --loglevel=info

volumes:
  pgdata:
  static_volume:
