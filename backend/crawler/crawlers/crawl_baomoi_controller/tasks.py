import logging

from crawler.crawlers.crawl_baomoi_controller.crawler import BaomoiCrawler
from crawler.services.news_service import save_articles_with_categories
from celery import shared_task

logger = logging.getLogger(__name__)


@shared_task
def crawl_baomoi_articles(limit=10):
    logger.info("üîç B·∫Øt ƒë·∫ßu crawl b√†i vi·∫øt t·ª´ B√°o M·ªõi...")
    crawler = BaomoiCrawler()

    articles = crawler.crawl(limit=limit)
    urls_processed = getattr(crawler, 'urls_processed', [])

    if not articles:
        logger.warning("‚ö†Ô∏è Kh√¥ng c√≥ b√†i vi·∫øt n√†o ƒë∆∞·ª£c crawl.")
        return 0, urls_processed

    count = save_articles_with_categories(
        "B√°o M·ªõi", "https://baomoi.com", articles)
    logger.info("‚úÖ Ho√†n t·∫•t qu√° tr√¨nh crawl v√† l∆∞u b√†i vi·∫øt.")
    return count, urls_processed
