{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-25T18:07:38.451067Z",
     "iopub.status.busy": "2025-05-25T18:07:38.450755Z",
     "iopub.status.idle": "2025-05-25T18:07:48.874379Z",
     "shell.execute_reply": "2025-05-25T18:07:48.873467Z",
     "shell.execute_reply.started": "2025-05-25T18:07:38.451045Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge_score) (2024.2.0)\n",
      "Collecting bert_score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.5.1+cu121)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.2.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.47.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.26.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.5)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2025.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.29.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bert_score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bert_score) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->bert_score) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->bert_score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->bert_score) (2024.2.0)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bert_score\n",
      "Successfully installed bert_score-0.3.13\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate peft datasets bitsandbytes evaluate -q\n",
    "!pip install rouge_score\n",
    "!pip install bert_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import library and load module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T18:07:19.113099Z",
     "iopub.status.busy": "2025-05-25T18:07:19.112795Z",
     "iopub.status.idle": "2025-05-25T18:07:19.117654Z",
     "shell.execute_reply": "2025-05-25T18:07:19.116772Z",
     "shell.execute_reply.started": "2025-05-25T18:07:19.113076Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import json\n",
    "import shutil\n",
    "from IPython.display import FileLink\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    default_data_collator,\n",
    ")\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:20:18.951669Z",
     "iopub.status.busy": "2025-05-25T17:20:18.951392Z",
     "iopub.status.idle": "2025-05-25T17:20:19.082264Z",
     "shell.execute_reply": "2025-05-25T17:20:19.081625Z",
     "shell.execute_reply.started": "2025-05-25T17:20:18.951647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:20:21.294561Z",
     "iopub.status.busy": "2025-05-25T17:20:21.294289Z",
     "iopub.status.idle": "2025-05-25T17:20:21.298077Z",
     "shell.execute_reply": "2025-05-25T17:20:21.297330Z",
     "shell.execute_reply.started": "2025-05-25T17:20:21.294539Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-25T17:20:08.512793Z",
     "iopub.status.idle": "2025-05-25T17:20:08.513199Z",
     "shell.execute_reply": "2025-05-25T17:20:08.513022Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:20:45.425971Z",
     "iopub.status.busy": "2025-05-25T17:20:45.425621Z",
     "iopub.status.idle": "2025-05-25T17:20:59.049219Z",
     "shell.execute_reply": "2025-05-25T17:20:59.048153Z",
     "shell.execute_reply.started": "2025-05-25T17:20:45.425945Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06c29e3ba1b4b6f9c64d5babe4fad71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b754f72e7a2a48c7b167b234043366c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cadcf00a493b40e48b8fd782efdd8fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# Thiết lập pad_token và padding side\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:21:28.062969Z",
     "iopub.status.busy": "2025-05-25T17:21:28.062655Z",
     "iopub.status.idle": "2025-05-25T17:21:28.068110Z",
     "shell.execute_reply": "2025-05-25T17:21:28.067152Z",
     "shell.execute_reply.started": "2025-05-25T17:21:28.062947Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    attention_masks = [item[\"attention_mask\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_masks,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:21:29.797994Z",
     "iopub.status.busy": "2025-05-25T17:21:29.797652Z",
     "iopub.status.idle": "2025-05-25T17:21:29.812506Z",
     "shell.execute_reply": "2025-05-25T17:21:29.811879Z",
     "shell.execute_reply.started": "2025-05-25T17:21:29.797969Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Continue Pretrain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:02:51.782934Z",
     "iopub.status.busy": "2025-05-25T03:02:51.782613Z",
     "iopub.status.idle": "2025-05-25T03:02:54.889569Z",
     "shell.execute_reply": "2025-05-25T03:02:54.888919Z",
     "shell.execute_reply.started": "2025-05-25T03:02:51.782904Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab96146c2024a1b8855d8c16e9c3b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/635 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d935df233d17414bb79a1183eba11d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/35.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4b1ff0664844eb932b08699f7a635d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/4.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c73f47f1d014b5880278e9d5623f2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/4.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319482b65aa44d1594d7fe4c1e76b86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/12806 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4548d22c245c41919618d4a62cab75f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca939bd27d244c088575c7b80bb8b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"vukhai248/vietnamese_news_16k\", split=\"train\")\n",
    "dataset = dataset.select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:02:54.891173Z",
     "iopub.status.busy": "2025-05-25T03:02:54.890933Z",
     "iopub.status.idle": "2025-05-25T03:02:54.894619Z",
     "shell.execute_reply": "2025-05-25T03:02:54.893717Z",
     "shell.execute_reply.started": "2025-05-25T03:02:54.891150Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    return text.replace(\"\\n\", \" \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:02:54.895648Z",
     "iopub.status.busy": "2025-05-25T03:02:54.895450Z",
     "iopub.status.idle": "2025-05-25T03:03:04.441895Z",
     "shell.execute_reply": "2025-05-25T03:03:04.440976Z",
     "shell.execute_reply.started": "2025-05-25T03:02:54.895632Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_fn(text, max_length=2048):\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "    attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "    labels = input_ids.clone()\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:03:04.443066Z",
     "iopub.status.busy": "2025-05-25T03:03:04.442775Z",
     "iopub.status.idle": "2025-05-25T03:03:10.608088Z",
     "shell.execute_reply": "2025-05-25T03:03:10.607451Z",
     "shell.execute_reply.started": "2025-05-25T03:03:04.443043Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "texts = dataset[\"content\"]\n",
    "tokenized_data = [tokenize_fn(text) for text in texts]\n",
    "train_loader = DataLoader(\n",
    "    tokenized_data,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:03:10.609190Z",
     "iopub.status.busy": "2025-05-25T03:03:10.608887Z",
     "iopub.status.idle": "2025-05-25T10:51:54.609669Z",
     "shell.execute_reply": "2025-05-25T10:51:54.608737Z",
     "shell.execute_reply.started": "2025-05-25T03:03:10.609162Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 1/3: 100%|██████████| 1000/1000 [2:35:22<00:00,  9.32s/it, loss=2.72] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Loss: 2.3339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1000/1000 [2:37:08<00:00,  9.43s/it, loss=2.06] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Loss: 2.2864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1000/1000 [2:36:12<00:00,  9.37s/it, loss=2.25] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Loss: 2.2713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, device, n_epochs=3, lr=1e-5):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\", leave=True)\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            # Forward pass with checkpointing\n",
    "            def forward_with_checkpoint(*args, **kwargs):\n",
    "                return torch.utils.checkpoint.checkpoint(\n",
    "                    lambda *args, **kwargs: model(*args, **kwargs),\n",
    "                    *args,\n",
    "                    **kwargs,\n",
    "                    use_reentrant=False\n",
    "                )\n",
    "\n",
    "            outputs = forward_with_checkpoint(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} - Loss: {avg_loss:.4f}\")\n",
    "train(model, train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T10:51:54.611200Z",
     "iopub.status.busy": "2025-05-25T10:51:54.610857Z",
     "iopub.status.idle": "2025-05-25T10:51:55.038424Z",
     "shell.execute_reply": "2025-05-25T10:51:55.037648Z",
     "shell.execute_reply.started": "2025-05-25T10:51:54.611169Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pretrained model saved at: /kaggle/working/llama3-qlora-continued-pretrain\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-continued-pretrain\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"✅ Pretrained model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T10:51:55.039477Z",
     "iopub.status.busy": "2025-05-25T10:51:55.039197Z",
     "iopub.status.idle": "2025-05-25T10:51:55.626208Z",
     "shell.execute_reply": "2025-05-25T10:51:55.625554Z",
     "shell.execute_reply.started": "2025-05-25T10:51:55.039447Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='llama3-qlora-continued-pretrain.zip' target='_blank'>llama3-qlora-continued-pretrain.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/llama3-qlora-continued-pretrain.zip"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Đường dẫn tới thư mục chứa mô hình\n",
    "model_dir = '/kaggle/working/llama3-qlora-continued-pretrain'\n",
    "\n",
    "# Tạo file nén .zip từ thư mục\n",
    "zip_name = 'llama3-qlora-continued-pretrain'  # Tên file nén (không cần đuôi .zip)\n",
    "shutil.make_archive(zip_name, 'zip', model_dir)\n",
    "\n",
    "# Tạo liên kết tải xuống cho file .zip\n",
    "display(FileLink(f'{zip_name}.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finetune with QA dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:20:37.520991Z",
     "iopub.status.busy": "2025-05-25T17:20:37.520658Z",
     "iopub.status.idle": "2025-05-25T17:20:37.524611Z",
     "shell.execute_reply": "2025-05-25T17:20:37.523676Z",
     "shell.execute_reply.started": "2025-05-25T17:20:37.520964Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"/kaggle/input/llama-continued-pretrain-model/transformers/default/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:06.686944Z",
     "iopub.status.busy": "2025-05-25T11:06:06.686643Z",
     "iopub.status.idle": "2025-05-25T11:06:10.813181Z",
     "shell.execute_reply": "2025-05-25T11:06:10.812250Z",
     "shell.execute_reply.started": "2025-05-25T11:06:06.686922Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/vietnamese-squad/train-v2.0-translated.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    vi_squad_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:10.814439Z",
     "iopub.status.busy": "2025-05-25T11:06:10.814139Z",
     "iopub.status.idle": "2025-05-25T11:06:10.819212Z",
     "shell.execute_reply": "2025-05-25T11:06:10.818379Z",
     "shell.execute_reply.started": "2025-05-25T11:06:10.814410Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Train samples: 10000\n",
      "📦 Validation samples: 2000\n"
     ]
    }
   ],
   "source": [
    "train_data = vi_squad_data[:10000]\n",
    "\n",
    "# Lấy 2k mẫu tiếp theo cho validation\n",
    "val_data = vi_squad_data[10000:12000]\n",
    "\n",
    "print(f\"📦 Train samples: {len(train_data)}\")\n",
    "print(f\"📦 Validation samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:10.820518Z",
     "iopub.status.busy": "2025-05-25T11:06:10.820173Z",
     "iopub.status.idle": "2025-05-25T11:06:15.870562Z",
     "shell.execute_reply": "2025-05-25T11:06:15.869725Z",
     "shell.execute_reply.started": "2025-05-25T11:06:10.820470Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Train samples: ['Beyoncé Giselle Knowles-Carter (/ b i gì ɒ n s eɪ / bee-YON-say) (sinh ngày 04 tháng 9 1981) là một ca sĩ, nhạc sĩ, nhà sản xuất thu âm và nữ diễn viên người Mỹ. Sinh ra và lớn lên ở Houston, Texas, cô đã biểu diễn trong các cuộc thi ca hát và nhảy múa khác nhau khi còn nhỏ, và nổi tiếng vào cuối những năm 1990 với tư cách là ca sĩ chính của nhóm nhạc nữ R & B Destiny\\'s Child. Được quản lý bởi cha cô, Mathew Knowles, nhóm đã trở thành một trong những nhóm nhạc nữ bán chạy nhất thế giới mọi thời đại. Sự gián đoạn của họ đã chứng kiến việc phát hành album đầu tay của Beyoncé, Dangerously in Love (2003), giúp cô trở thành một nghệ sĩ solo trên toàn thế giới, giành được năm giải Grammy và có đĩa đơn quán quân Billboard Hot 100 \"Crazy in Love\" và \"Baby Boy\".', 'Beyonce bắt đầu nổi tiếng từ khi nào?', 'Vào cuối những năm 1990']\n"
     ]
    }
   ],
   "source": [
    "print(f\"📦 Train samples: {train_data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:15.871686Z",
     "iopub.status.busy": "2025-05-25T11:06:15.871441Z",
     "iopub.status.idle": "2025-05-25T11:06:16.056357Z",
     "shell.execute_reply": "2025-05-25T11:06:16.055708Z",
     "shell.execute_reply.started": "2025-05-25T11:06:15.871666Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_list([{\"context\": item[0], \"question\": item[1], \"answer\": item[2]} for item in train_data])\n",
    "val_dataset = Dataset.from_list([{\"context\": item[0], \"question\": item[1], \"answer\": item[2]} for item in val_data])\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": val_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:16.057228Z",
     "iopub.status.busy": "2025-05-25T11:06:16.057027Z",
     "iopub.status.idle": "2025-05-25T11:06:16.067113Z",
     "shell.execute_reply": "2025-05-25T11:06:16.066179Z",
     "shell.execute_reply.started": "2025-05-25T11:06:16.057210Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. {'context': 'Beyoncé Giselle Knowles-Carter (/ b i gì ɒ n s eɪ / bee-YON-say) (sinh ngày 04 tháng 9 1981) là một ca sĩ, nhạc sĩ, nhà sản xuất thu âm và nữ diễn viên người Mỹ. Sinh ra và lớn lên ở Houston, Texas, cô đã biểu diễn trong các cuộc thi ca hát và nhảy múa khác nhau khi còn nhỏ, và nổi tiếng vào cuối những năm 1990 với tư cách là ca sĩ chính của nhóm nhạc nữ R & B Destiny\\'s Child. Được quản lý bởi cha cô, Mathew Knowles, nhóm đã trở thành một trong những nhóm nhạc nữ bán chạy nhất thế giới mọi thời đại. Sự gián đoạn của họ đã chứng kiến việc phát hành album đầu tay của Beyoncé, Dangerously in Love (2003), giúp cô trở thành một nghệ sĩ solo trên toàn thế giới, giành được năm giải Grammy và có đĩa đơn quán quân Billboard Hot 100 \"Crazy in Love\" và \"Baby Boy\".', 'question': 'Beyonce bắt đầu nổi tiếng từ khi nào?', 'answer': 'Vào cuối những năm 1990'}\n",
      "2. Beyoncé Giselle Knowles-Carter (/ b i gì ɒ n s eɪ / bee-YON-say) (sinh ngày 04 tháng 9 1981) là một ca sĩ, nhạc sĩ, nhà sản xuất thu âm và nữ diễn viên người Mỹ. Sinh ra và lớn lên ở Houston, Texas, cô đã biểu diễn trong các cuộc thi ca hát và nhảy múa khác nhau khi còn nhỏ, và nổi tiếng vào cuối những năm 1990 với tư cách là ca sĩ chính của nhóm nhạc nữ R & B Destiny's Child. Được quản lý bởi cha cô, Mathew Knowles, nhóm đã trở thành một trong những nhóm nhạc nữ bán chạy nhất thế giới mọi thời đại. Sự gián đoạn của họ đã chứng kiến việc phát hành album đầu tay của Beyoncé, Dangerously in Love (2003), giúp cô trở thành một nghệ sĩ solo trên toàn thế giới, giành được năm giải Grammy và có đĩa đơn quán quân Billboard Hot 100 \"Crazy in Love\" và \"Baby Boy\".\n",
      "3. Beyonce bắt đầu nổi tiếng từ khi nào?\n",
      "4. Vào cuối những năm 1990\n"
     ]
    }
   ],
   "source": [
    "print(f\"1. {dataset_dict['train'][0]}\")\n",
    "print(f\"2. {dataset_dict['train'][0]['context']}\")\n",
    "print(f\"3. {dataset_dict['train'][0]['question']}\")\n",
    "print(f\"4. {dataset_dict['train'][0]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:16.068330Z",
     "iopub.status.busy": "2025-05-25T11:06:16.068070Z",
     "iopub.status.idle": "2025-05-25T11:06:33.217119Z",
     "shell.execute_reply": "2025-05-25T11:06:33.216196Z",
     "shell.execute_reply.started": "2025-05-25T11:06:16.068298Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27c694acfcc45e48aef08fa794fe98a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742c0882c4a8464787c11b82c639a0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_qa(example, max_length=512):\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"]\n",
    "    prompt = f\"### Đây là dạng câu hỏi và trả lời dựa trên nội dung ###\\n\\nCâu hỏi: {question}\\n\\nNội dung: {context}\\n\\nTrả lời:\"\n",
    "    completion = f\" {answer}\"\n",
    "    \n",
    "    prompt_ids = tokenizer(prompt, add_special_tokens=False).input_ids\n",
    "    completion_ids = tokenizer(completion, add_special_tokens=False).input_ids\n",
    "\n",
    "    input_ids = prompt_ids + completion_ids\n",
    "\n",
    "    labels = [-100] * len(prompt_ids) + completion_ids\n",
    "\n",
    "    if len(input_ids) > max_length:\n",
    "        input_ids = input_ids[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    \n",
    "    input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "    attention_mask = attention_mask + [0] * padding_length\n",
    "    labels = labels + [-100] * padding_length\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids),\n",
    "        \"attention_mask\": torch.tensor(attention_mask),\n",
    "        \"labels\": torch.tensor(labels)\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset_dict.map(preprocess_qa)\n",
    "\n",
    "tokenized_dataset.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:33.218537Z",
     "iopub.status.busy": "2025-05-25T11:06:33.218201Z",
     "iopub.status.idle": "2025-05-25T11:06:33.223084Z",
     "shell.execute_reply": "2025-05-25T11:06:33.222309Z",
     "shell.execute_reply.started": "2025-05-25T11:06:33.218500Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"], batch_size=4, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_dataset[\"test\"], batch_size=4, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-25T17:18:25.136Z",
     "iopub.execute_input": "2025-05-25T11:06:33.224338Z",
     "iopub.status.busy": "2025-05-25T11:06:33.224038Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:   0%|          | 0/2500 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 1/2: 100%|██████████| 2500/2500 [3:22:25<00:00,  4.86s/it, loss=1.42]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1/2 - Train Loss: 1.7102\n",
      "📊 Validation Loss: 1.3921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  25%|██▍       | 621/2500 [50:22<2:32:00,  4.85s/it, loss=2.36] "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "n_epochs = 2\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\", leave=True)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"✅ Epoch {epoch+1}/{n_epochs} - Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"📊 Validation Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-25T17:18:25.136Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-qa-finetune\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"✅ Pretrained model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-25T17:18:25.137Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_dir = '/kaggle/working/llama3-qlora-qa-finetune'\n",
    "\n",
    "# Tạo file nén .zip từ thư mục\n",
    "zip_name = 'llama3-qlora-qa-finetune'  # Tên file nén (không cần đuôi .zip)\n",
    "shutil.make_archive(zip_name, 'zip', model_dir)\n",
    "\n",
    "# Tạo liên kết tải xuống cho file .zip\n",
    "display(FileLink(f'{zip_name}.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finetune with BBC News**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-25T10:52:16.017889Z",
     "iopub.status.idle": "2025-05-25T10:52:16.018413Z",
     "shell.execute_reply": "2025-05-25T10:52:16.018086Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-25T10:52:16.019272Z",
     "iopub.status.idle": "2025-05-25T10:52:16.019645Z",
     "shell.execute_reply": "2025-05-25T10:52:16.019486Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/kaggle/input/data-llama-finetune/bbc_data_llama_finetune.json\"\n",
    "\n",
    "full_dataset = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
    "dataset = full_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-25T10:52:16.020450Z",
     "iopub.status.idle": "2025-05-25T10:52:16.020843Z",
     "shell.execute_reply": "2025-05-25T10:52:16.020669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize(example, max_length=2048):\n",
    "    prompt = f\"### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Lệnh:\\n{example['prompt']}\\n\\nTóm tắt:\\n\"\n",
    "    summary = example[\"summary\"]\n",
    "    prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    summary_ids = tokenizer.encode(summary, add_special_tokens=False)\n",
    "    \n",
    "    # Kiểm tra tổng số token\n",
    "    total_length = len(prompt_ids) + len(summary_ids)\n",
    "    if total_length > max_length:\n",
    "        overflow = total_length - max_length\n",
    "        if overflow < len(prompt_ids):\n",
    "            prompt_ids = prompt_ids[:-overflow]\n",
    "        else:\n",
    "            prompt_ids = []\n",
    "    input_ids = prompt_ids + summary_ids\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    labels = [-100] * len(prompt_ids) + summary_ids\n",
    "\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "        attention_mask = attention_mask + [0] * padding_length\n",
    "        labels = labels + [-100] * padding_length\n",
    "    else:\n",
    "        input_ids = input_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "tokenized_dataset = dataset.map(tokenize, batched=False, fn_kwargs={\"max_length\": 2048})\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "collator = default_data_collator\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_dataset[\"test\"],\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-25T10:52:16.021384Z",
     "iopub.status.idle": "2025-05-25T10:52:16.021763Z",
     "shell.execute_reply": "2025-05-25T10:52:16.021592Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-25T10:52:16.022474Z",
     "iopub.status.idle": "2025-05-25T10:52:16.022860Z",
     "shell.execute_reply": "2025-05-25T10:52:16.022692Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in pbar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"✅ Epoch {epoch+1} completed — Avg loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-25T10:52:16.023979Z",
     "iopub.status.idle": "2025-05-25T10:52:16.024365Z",
     "shell.execute_reply": "2025-05-25T10:52:16.024203Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-bbc-finetuned\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"✅ Finetuned model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-25T10:52:16.025408Z",
     "iopub.status.idle": "2025-05-25T10:52:16.025791Z",
     "shell.execute_reply": "2025-05-25T10:52:16.025609Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_dir = '/kaggle/working/llama3-qlora-bbc-finetuned'\n",
    "\n",
    "# Tạo file nén .zip từ thư mục\n",
    "zip_name = 'llama3-qlora-qa-finetune'  # Tên file nén (không cần đuôi .zip)\n",
    "shutil.make_archive(zip_name, 'zip', model_dir)\n",
    "\n",
    "# Tạo liên kết tải xuống cho file .zip\n",
    "display(FileLink(f'{zip_name}.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finetune with donvanban**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-25T10:52:16.026572Z",
     "iopub.status.idle": "2025-05-25T10:52:16.026934Z",
     "shell.execute_reply": "2025-05-25T10:52:16.026776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#model_id = \"/kaggle/input/llama_finetune_v1.3/transformers/default/1/data_llama_finetune_v1.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:22:29.581846Z",
     "iopub.status.busy": "2025-05-25T17:22:29.581510Z",
     "iopub.status.idle": "2025-05-25T17:22:29.951594Z",
     "shell.execute_reply": "2025-05-25T17:22:29.950980Z",
     "shell.execute_reply.started": "2025-05-25T17:22:29.581820Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce268e87e19d4f8db2d54debd7d26ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = \"/kaggle/input/data-llama-finetune/vi_data_llama_finetune.json\"\n",
    "\n",
    "full_dataset = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
    "dataset = full_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:22:31.438066Z",
     "iopub.status.busy": "2025-05-25T17:22:31.437779Z",
     "iopub.status.idle": "2025-05-25T17:22:32.473477Z",
     "shell.execute_reply": "2025-05-25T17:22:32.472566Z",
     "shell.execute_reply.started": "2025-05-25T17:22:31.438043Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a4d26f611d4a3ab33d4a1479e5feb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9818c8e5f3d24b90b6789c9b0a8bb105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(example, max_length=2048):\n",
    "    prompt = f\"### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Lệnh:\\n{example['prompt']}\\n\\nTóm tắt:\\n\"\n",
    "    summary = example[\"summary\"]\n",
    "    \n",
    "    # Mã hóa riêng prompt và summary (không thêm special tokens)\n",
    "    prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    summary_ids = tokenizer.encode(summary, add_special_tokens=False)\n",
    "    \n",
    "    # Kiểm tra tổng số token\n",
    "    total_length = len(prompt_ids) + len(summary_ids)\n",
    "    if total_length > max_length:\n",
    "        overflow = total_length - max_length\n",
    "        # Ưu tiên giữ lại phần summary; cắt bớt prompt\n",
    "        if overflow < len(prompt_ids):\n",
    "            prompt_ids = prompt_ids[:-overflow]\n",
    "        else:\n",
    "            prompt_ids = []  # Nếu quá tràn, bỏ hết prompt\n",
    "    \n",
    "    # Nối prompt và summary\n",
    "    input_ids = prompt_ids + summary_ids\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    \n",
    "    # Tạo labels: phần prompt được mask bằng -100, phần summary giữ nguyên token IDs\n",
    "    labels = [-100] * len(prompt_ids) + summary_ids\n",
    "    \n",
    "    # Padding tất cả các trường về độ dài max_length\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "        attention_mask = attention_mask + [0] * padding_length\n",
    "        labels = labels + [-100] * padding_length\n",
    "    else:\n",
    "        # Nếu quá dài, cắt bớt (nên không xảy ra nhờ truncation ở trên)\n",
    "        input_ids = input_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "tokenized_dataset = dataset.map(tokenize, batched=False, fn_kwargs={\"max_length\": 2048})\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "collator = default_data_collator\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_dataset[\"test\"],\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:22:34.806048Z",
     "iopub.status.busy": "2025-05-25T17:22:34.805723Z",
     "iopub.status.idle": "2025-05-25T17:22:34.823068Z",
     "shell.execute_reply": "2025-05-25T17:22:34.822212Z",
     "shell.execute_reply.started": "2025-05-25T17:22:34.806021Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:22:37.392908Z",
     "iopub.status.busy": "2025-05-25T17:22:37.392605Z",
     "iopub.status.idle": "2025-05-25T18:06:52.107451Z",
     "shell.execute_reply": "2025-05-25T18:06:52.106608Z",
     "shell.execute_reply.started": "2025-05-25T17:22:37.392886Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/72 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Loss: 1.2355: 100%|██████████| 72/72 [14:43<00:00, 12.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1 completed — Avg loss: 0.5272\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.6456: 100%|██████████| 72/72 [14:46<00:00, 12.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2 completed — Avg loss: 0.4752\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1448: 100%|██████████| 72/72 [14:45<00:00, 12.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 3 completed — Avg loss: 0.4345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in pbar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"✅ Epoch {epoch+1} completed — Avg loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T18:06:52.109000Z",
     "iopub.status.busy": "2025-05-25T18:06:52.108703Z",
     "iopub.status.idle": "2025-05-25T18:06:52.599533Z",
     "shell.execute_reply": "2025-05-25T18:06:52.598727Z",
     "shell.execute_reply.started": "2025-05-25T18:06:52.108977Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finetuned model saved at: /kaggle/working/llama3-qlora-finetuned-all\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-finetuned-all\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"✅ Finetuned model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T18:07:23.111501Z",
     "iopub.status.busy": "2025-05-25T18:07:23.111220Z",
     "iopub.status.idle": "2025-05-25T18:07:23.716002Z",
     "shell.execute_reply": "2025-05-25T18:07:23.715118Z",
     "shell.execute_reply.started": "2025-05-25T18:07:23.111479Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='llama3-qlora-donvanban-finetune.zip' target='_blank'>llama3-qlora-donvanban-finetune.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/llama3-qlora-donvanban-finetune.zip"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_dir = '/kaggle/working/llama3-qlora-finetuned-all'\n",
    "\n",
    "# Tạo file nén .zip từ thư mục\n",
    "zip_name = 'llama3-qlora-donvanban-finetune'  # Tên file nén (không cần đuôi .zip)\n",
    "shutil.make_archive(zip_name, 'zip', model_dir)\n",
    "\n",
    "# Tạo liên kết tải xuống cho file .zip\n",
    "display(FileLink(f'{zip_name}.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T18:07:25.381781Z",
     "iopub.status.busy": "2025-05-25T18:07:25.381370Z",
     "iopub.status.idle": "2025-05-25T18:07:25.394653Z",
     "shell.execute_reply": "2025-05-25T18:07:25.393768Z",
     "shell.execute_reply.started": "2025-05-25T18:07:25.381738Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_after_training(model, val_loader, tokenizer, device, num_samples=3):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    samples_to_print = []\n",
    "\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bert_score = evaluate.load(\"bertscore\")\n",
    "\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=128,\n",
    "                num_beams=4,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "\n",
    "            generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            label_texts = []\n",
    "            for label in labels:\n",
    "                label_ids = [token_id for token_id in label.tolist() if token_id != -100]\n",
    "                label_texts.append(tokenizer.decode(label_ids, skip_special_tokens=True))\n",
    "\n",
    "            predictions.extend(generated_texts)\n",
    "            references.extend(label_texts)\n",
    "\n",
    "            if len(samples_to_print) < num_samples:\n",
    "                for src, ref, pred in zip(tokenizer.batch_decode(input_ids, skip_special_tokens=True), label_texts, generated_texts):\n",
    "                    samples_to_print.append((src, ref, pred))\n",
    "                    if len(samples_to_print) >= num_samples:\n",
    "                        break\n",
    "\n",
    "    # ROUGE\n",
    "    rouge_results = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "\n",
    "    # BERTScore (tiếng Việt)\n",
    "    bert_results = bert_score.compute(predictions=predictions, references=references, lang=\"vi\")\n",
    "    bert_f1 = np.mean(bert_results[\"f1\"])\n",
    "\n",
    "    # In kết quả\n",
    "    print(\"\\n📊 ROUGE Scores:\")\n",
    "    for key in rouge_results:\n",
    "        print(f\"{key}: {rouge_results[key]:.4f}\")\n",
    "\n",
    "    print(f\"\\n📈 BERTScore F1: {bert_f1:.4f}\")\n",
    "\n",
    "    print(\"\\n📝 Sample Results:\")\n",
    "    for i, (src, ref, pred) in enumerate(samples_to_print):\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        print(f\"[Prompt]    {src[:300]}...\")  # rút gọn prompt nếu quá dài\n",
    "        print(f\"[Reference] {ref}\")\n",
    "        print(f\"[Generated] {pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T18:08:02.175720Z",
     "iopub.status.busy": "2025-05-25T18:08:02.175392Z",
     "iopub.status.idle": "2025-05-25T18:13:56.809448Z",
     "shell.execute_reply": "2025-05-25T18:13:56.808518Z",
     "shell.execute_reply.started": "2025-05-25T18:08:02.175676Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/9 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  11%|█         | 1/9 [00:41<05:33, 41.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 2/9 [01:19<04:37, 39.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 3/9 [01:59<03:59, 39.84s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  44%|████▍     | 4/9 [02:39<03:18, 39.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  56%|█████▌    | 5/9 [03:19<02:39, 39.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 6/9 [03:59<01:59, 39.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 7/9 [04:38<01:18, 39.50s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating: 100%|██████████| 9/9 [05:42<00:00, 38.09s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b759dd5c68141d6a8112e0d8b5c9cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10416111f9f04d1ea6651cd46921afb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0e5678ec9a48008e4f878d4cd6e447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2547c939e6b48de893490998ad33417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8d2aa809c14d4287ea296bac45253b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 ROUGE Scores:\n",
      "rouge1: 0.2543\n",
      "rouge2: 0.2515\n",
      "rougeL: 0.2529\n",
      "rougeLsum: 0.2533\n",
      "\n",
      "📈 BERTScore F1: 0.7446\n",
      "\n",
      "📝 Sample Results:\n",
      "\n",
      "--- Sample 1 ---\n",
      "[Prompt]    ### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Lệnh:\n",
      "Bạn là một trợ lý tóm tắt văn bản. Hãy cung cấp bản tóm tắt ngắn gọn và chính xác trong 150 chữ cho bài viết sau. Bài viết: ﻿Hoàng Anh Gia Lai tái cấu trúc\n",
      "Công ty của bầu Đức sẽ bán các dự án thuộc lĩnh vực ...\n",
      "[Reference] ﻿Công ty cổ phần Hoàng Anh Gia Lai vừa thông qua chủ trương tái cấu trúc các đơn vị thuộc ngành thủy điện.\n",
      "Đồng thời, cũng thống nhất thông qua chủ trương tái cấu trúc các đơn vị thuộc ngành bất động sản.\n",
      " Mục đích, nhằm thu tiền mặt về dự trữ và giảm nợ vay.\n",
      "Tỷ trọng doanh thu từ bất động sản trong năm 2013 của công ty sẽ giảm từ 64% xuống còn 14%.\n",
      "[Generated] ### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Lệnh:\n",
      "Bạn là một trợ lý tóm tắt văn bản. Hãy cung cấp bản tóm tắt ngắn gọn và chính xác trong 150 chữ cho bài viết sau. Bài viết: ﻿Hoàng Anh Gia Lai tái cấu trúc\n",
      "Công ty của bầu Đức sẽ bán các dự án thuộc lĩnh vực thủy điện và bất động sản nhằm dự trữ tiền mặt và giảm nợ vay. \n",
      "Công ty cổ phần Hoàng Anh Gia Lai (Mã CK: HAG) vừa công bố nghị quyết HĐQT. Theo đó, công ty thông qua chủ trương tái cấu trúc các đơn vị thuộc ngành thủy điện bằng cách bán các dự án đã đi vào hoạt động và đang trong giai đoạn đầu tư.\n",
      "Đồng thời, Hoàng Anh Gia Lai cũng thống nhất thông qua chủ trương tái cấu trúc các đơn vị thuộc ngành bất động sản bằng hình thức bán sỉ căn hộ và bán các dự án hoặc cổ phiếu của các công ty con đang sở hữu dự án. Mục đích bán, theo doanh nghiệp, nhằm thu tiền mặt về dự trữ và giảm nợ vay.\n",
      "Trước đó, trong báo cáo tài chính hợp nhất quý I, công ty do ông Đoàn Nguyên Đức làm Chủ tịch có 2.444 tỷ đồng tiền và các khoản tương đương tiền, nợ ngắn hạn là 3.446 tỷ đồng. Tại ĐHCĐ, bầu Đức cho biết, tỷ trọng doanh thu từ bất động sản trong năm 2013 sẽ giảm từ 64% xuống còn 14%. Tóm tắt: \n",
      "\n",
      "Tóm tắt:\n",
      "﻿Công ty cổ phần Hoàng Anh Gia Lai vừa thông qua chủ trương tái cấu trúc các đơn vị thuộc ngành thủy điện.\n",
      "Đồng thời, cũng thống nhất thông qua chủ trương tái cấu trúc các đơn vị thuộc ngành bất động sản.\n",
      " Mục đích, nhằm thu tiền mặt về dự trữ và giảm nợ vay.\n",
      "Tỷ trọng doanh thu từ bất động sản trong năm 2013 của công ty sẽ giảm từ 64% xuống còn 14%.Question:\n",
      "﻿Công ty cổ phần Hoàng Anh Gia Lai vừa thông qua chủ trương tái cấu trúc các đơn vị thuộc ngành thủy điện.\n",
      "Đồng thời, cũng thống nhất thông qua chủ trương tái cấu trúc các đơn vị thuộc ngành bất động sản.\n",
      "Mục đích, nhằm thu tiền mặt về dự trữ và giảm nợ vay.\n",
      "Tỷ trọng doanh thu từ bất động sản trong năm 2013 của công ty sẽ giảm từ 64% xuống còn 14%.\n",
      "﻿Công ty cổ phần Hoàng Anh Gia Lai vừa thông qua chủ trương tái cấu trúc các đơn vị thuộc ngành thủy điện.\n",
      "Đồng thời\n",
      "\n",
      "--- Sample 2 ---\n",
      "[Prompt]    ### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Lệnh:\n",
      "Bạn là một trợ lý tóm tắt văn bản. Hãy cung cấp bản tóm tắt ngắn gọn và chính xác trong 150 chữ cho bài viết sau. Bài viết: ﻿Tạp chí Mỹ bị lên án vì chụp ảnh \"thời trang tự tử\"\n",
      " Tạp chí Vice của Mỹ vừa bị lên...\n",
      "[Reference] ﻿Tạp chí Vice của Mỹ  vừa bị lên án vì thực hiện bộ ảnh \"thời trang tự tử\", tái hiện lại giây phút cuối đời của những nữ văn sĩ nổi tiếng. Trong những năm gần dây, các tờ tạp chí thườnng mạnh dạn đưa ra những ý tưởng mới lạ, nhiều khi trở thành quá đà. Sau khi những bức hình tự tử này bị xóa đi, Vice đã đăng tải lời xin lỗi, bị cho là không thuyết phục. Khoa học đã chứng minh, đối với những ai từng xuất hiện trong đầu ý nghĩ đen tối về việc tự tử, việc báo chí hoặc truyền hình đưa ra những hình ảnh trực quan về hành động này không khác gì nhen nhóm lại ý tưởng đó trong đầu người xem. Cách tiếp cận đề tài  bị đánh giá là thiếu tôn trọng người đã khuất.\n",
      "[Generated] ### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Lệnh:\n",
      "Bạn là một trợ lý tóm tắt văn bản. Hãy cung cấp bản tóm tắt ngắn gọn và chính xác trong 150 chữ cho bài viết sau. Bài viết: ﻿Tạp chí Mỹ bị lên án vì chụp ảnh \"thời trang tự tử\"\n",
      " Tạp chí Vice của Mỹ vừa bị lên án vì thực hiện bộ ảnh \"thời trang tự tử\", tái hiện lại giây phút cuối đời của những nữ văn sĩ nổi tiếng. Trong những năm gần đây, để cạnh tranh với những báo khác, các tờ tạp chí thường mạnh dạn đưa ra những ý tưởng mới lạ, nhiều khi trở thành quá đà.  Đối với ý tưởng sáng tạo \"thảm họa\" lần này của tạp chí Vice, tờ Huffington Post đánh giá rằng: \"Vice nghĩ họ đang tạo ra những bức ảnh nghệ thuật nhưng cuối cùng người ta chỉ thấy đó là những bức ảnh thiếu suy nghĩ và phản cảm\". Sau khi những bức hình tự tử này bị xóa đi, Vice đã đăng tải lời xin lỗi: \"Những bức ảnh thời trang của Vice luôn phi truyền thống và tiếp cận từ góc nhìn nghệ thuật hơn là những bức ảnh thuần túy giới thiệu xu hướng thời trang. Mục tiêu của chúng tôi là tạo ra những bức hình nghệ thuật để từ đó thông điệp thời trang sẽ đi theo sau, thay vì đi trước như nhiều tờ báo khác. Bộ hình \"Last Words\" (Những lời cuối cùng) được tạo ra theo tư duy này và tập trung vào cái chết của những nữ văn sĩ nổi tiếng mà các độc giả yêu quý họ hẳn đều mong rằng giá như cuộc đời của những tài năng này đừng ngắn ngủi như vậy\". Lời xin lỗi của Vice bị cho là không thuyết phục. Thật khó hiểu tại sao ban biên tập của báo có thể để những bức ảnh này xuất hiện trên trang của mình. Tờ Huffington Post nhận định thêm: Vice đang cố tình giải thích lòng vòng xung quanh việc thực hiện một bộ ảnh vô nghĩa. Đối với một nhà văn, họ được công chúng nhớ tới nhờ những tác phẩm hay. Tại sao Vice không tôn vinh những tác phẩm đó mà lại nhấn mạnh vào những giây phút cuối đời đen tối, cô độc, với đầy nỗi sợ hãi và buồn khổ như vậy? Đối với tất cả những ai từng trải qua tâm trạng bi đát tới mức muốn tự tử hoặc có người thân từng tự tử, hẳn họ sẽ cảm thấy vô cùng khó chịu trước những bức ảnh này. \n",
      "Khoa học đã chứng minh rằng, đối với những ai từng xuất hiện trong đầu ý nghĩ đen tối về việc tự tử, việc báo chí hoặc truyền hình đưa ra những hình ảnh trực quan về hành động này không khác gì nhen nhóm lại ý tưởng đó trong đầu người xem. Đối với những nữ nhà văn mà tờ Vice đề cập tới, cách tiếp cận đề tài của họ bị đánh giá là thiếu tôn trọng người đã khuất. Tóm tắt: \n",
      "\n",
      "Tóm tắt:\n",
      "﻿Tạp chí Vice của Mỹ  vừa bị lên án vì thực hiện bộ ảnh \"thời trang tự tử\", tái hiện lại giây phút cuối đời của những nữ văn sĩ nổi tiếng. Trong những năm gần dây, các tờ tạp chí thườnng mạnh dạn đưa ra những ý tưởng mới lạ, nhiều khi trở thành quá đà. Sau khi những bức hình tự tử này bị xóa đi, Vice đã đăng tải lời xin lỗi, bị cho là không thuyết phục. Khoa học đã chứng minh, đối với những ai từng xuất hiện trong đầu ý nghĩ đen tối về việc tự tử, việc báo chí hoặc truyền hình đưa ra những hình ảnh trực quan về hành động này không khác gì nhen nhóm lại ý tưởng đó trong đầu người xem. Cách tiếp cận đề tài  bị đánh giá là thiếu tôn trọng người đã khuất.Question:\n",
      "﻿Tạp chí Vice của Mỹ vừa bị lên án vì thực hiện bộ ảnh \"thời trang tự tử\", tái hiện lại giây phút cuối đời của những nữ văn sĩ nổi tiếng. Trong những năm gần đây, các tờ tạp chí thường mạnh dạn đưa ra những ý tưởng mới lạ, nhiều khi trở thành quá đà. Sau khi những bức hình tự tử này bị xóa đi, Vice đã đăng tải lời xin lỗi, bị cho là không thuyết phục. Khoa học đã chứng minh, đối với những ai từng xuất hiện trong đầu ý nghĩ đen tối về việc tự tử, việc báo chí hoặc\n",
      "\n",
      "--- Sample 3 ---\n",
      "[Prompt]    ### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Lệnh:\n",
      "Bạn là một trợ lý tóm tắt văn bản. Hãy cung cấp bản tóm tắt ngắn gọn và chính xác trong 150 chữ cho bài viết sau. Bài viết: ﻿ 110 chú rùa con về với biển\n",
      " Tối 21/7, tại khu bãi Ngòi thuộc thôn Hải Giang, xã N...\n",
      "[Reference] ﻿Tối 21/7, tại khu bãi Ngòi thuộc thôn Hậu Giang, xã Nhơn Hải, TP Quy Nhơn, hơn 100 chú rùa con được về với biển sau 48 ngày trong bọc trứng.\n",
      "Trước đó, khoảng 20 giờ ngày 21/7, trong lúc ra biển hóng mát, một người dân sống gần khu vực này đã phát hiện nhiều chú rùa con bò hướng ra biển, báo cho nhóm tình nguyện viên bảo vệ rùa biển tại thôn Hải Giang. Các thành viên trong nhóm đã đến khu vực ổ trứng để kiểm tra ổ trứng, đo đạc kích thước rùa con để thả chúng về với đại dương. Giải cứu những chú rùa còn mắc kệt chưa nở được.\n",
      "[Generated] ### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Lệnh:\n",
      "Bạn là một trợ lý tóm tắt văn bản. Hãy cung cấp bản tóm tắt ngắn gọn và chính xác trong 150 chữ cho bài viết sau. Bài viết: ﻿ 110 chú rùa con về với biển\n",
      " Tối 21/7, tại khu bãi Ngòi thuộc thôn Hải Giang, xã Nhơn Hải, TP Quy Nhơn, tỉnh Bình Định, hơn 100 chú rùa con được về với biển sau 48 ngày trong bọc trứng. Đây là ổ trứng rùa đẻ đầu tiên trong năm 2013 tại khu vực thôn Hải Giang.\n",
      "Trước đó, khoảng 20 giờ ngày 21/7, trong lúc ra biển hóng mát, một người dân sống gần khu vực này đã phát hiện nhiều chú rùa con bò lon ton hướng ra biển, nên liền báo cho nhóm tình nguyện viên bảo về rùa biển tại thôn Hải Giang.\n",
      "Nhận được tin báo, các thành viên trong nhóm đã đến khu vực ổ trứng để kiểm tra ổ trứng, đo đạc kích thước rùa con để thả chúng về với đại dương. Đồng thời, giải cứu những chú rùa còn mắc kẹt chưa nở được. \n",
      "Qua thống kê, nhóm tình nguyện bảo vệ rùa biển xã Nhơn Hải cho biết, có 110 chú rùa con được nở thành công, trong tổng số 120 trứng, bị hư 10 trứng. Kích thước rùa con đo được, chiều dài tính từ đầu đến đuôi khoảng 8 cm, chiều ngang khoảng 5 cm. \n",
      "Theo Ông Nguyễn Văn Minh – Trưởng nhóm tình nguyện viên bảo vệ rùa biển xã Nhơn Hải, cho biết: \"Ổ trứng này đã được rùa mẹ đẻ rồi ủ trong thời gian 48 ngày thì nở thành rùa con. Số lượng trứng nở như vậy thì ổ trứng rùa này đạt tỷ lệ cao. Tuy nhiên, số lượng rùa con sau khi được về với biển thì nguy cơ sống sót chỉ khoảng từ 30 – 40%. Vì rùa con thường đối mặt với những loại cá lớn sẽ ăn thịt chúng, rùa con sức đề kháng yếu cũng có thể bị chết và bị sóng cuốn trôi...\"\n",
      "Trước đó, rạng sáng ngày 3/6, tại khu vực bãi Ngòi (thôn Hải Giang, xã Nhơn Hải, TP Quy Nhơn, Bình Định) một cá thể rùa mẹ có chiều dài khoảng 1,2m, chiều ngang 0,9m bò lên bờ cách mép nước biển trên 30m đẻ một ổ trứng với số lượng 120 trứng. \n",
      "Từ đầu năm đến nay, có 4 lần rùa mẹ bò lên bờ biển thôn Hải Giang, xã Nhơn Hải đẻ trứng với số lượng trên 500 trứng. Và đây ổ trứng đầu tiên đã nở 110 rùa con.\n",
      "Được biết, xã Nhơn Hải có nhiều rạng san hô và gành đá, môi trường biển trong lành là nơi rùa biển thường xuất hiện lên bờ đẻ trứng tại khu vực đảo Hòn Khô lớn (Thôn Hải Đông, xã Nhơn Hải ). \n",
      "Rùa biển là một trong những loài động vật quý hiếm nằm trong sách đỏ Việt Nam có nguy cơ bị tuyệt chủng nên hiện đang được bẻo vệ nghiêm ngặt. Tóm tắt: \n",
      "\n",
      "Tóm tắt:\n",
      "﻿Tối 21/7, tại khu bãi Ngòi thuộc thôn Hậu Giang, xã Nhơn Hải, TP Quy Nhơn, hơn 100 chú rùa con được về với biển sau 48 ngày trong bọc trứng.\n",
      "Trước đó, khoảng 20 giờ ngày 21/7, trong lúc ra biển hóng mát, một người dân sống gần khu vực này đã phát hiện nhiều chú rùa con bò hướng ra biển, báo cho nhóm tình nguyện viên bảo vệ rùa biển tại thôn Hải Giang. Các thành viên trong nhóm đã đến khu vực ổ trứng để kiểm tra ổ trứng, đo đạc kích thước rùa con để thả chúng về với đại dương. Giải cứu những chú rùa còn mắc kệt chưa nở được.Question: ﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿\n"
     ]
    }
   ],
   "source": [
    "evaluate_after_training(model, val_loader, tokenizer, device)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2627880,
     "sourceId": 4701735,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6984301,
     "sourceId": 11188105,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6959873,
     "sourceId": 11188231,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 356905,
     "modelInstanceId": 335897,
     "sourceId": 411412,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
