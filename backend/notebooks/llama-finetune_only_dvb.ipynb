{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-25T18:07:38.451067Z",
     "iopub.status.busy": "2025-05-25T18:07:38.450755Z",
     "iopub.status.idle": "2025-05-25T18:07:48.874379Z",
     "shell.execute_reply": "2025-05-25T18:07:48.873467Z",
     "shell.execute_reply.started": "2025-05-25T18:07:38.451045Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge_score) (2024.2.0)\n",
      "Collecting bert_score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.5.1+cu121)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.2.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.47.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.26.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.5)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2025.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.29.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bert_score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bert_score) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->bert_score) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->bert_score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->bert_score) (2024.2.0)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bert_score\n",
      "Successfully installed bert_score-0.3.13\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate peft datasets bitsandbytes evaluate -q\n",
    "!pip install rouge_score\n",
    "!pip install bert_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import library and load module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T18:07:19.113099Z",
     "iopub.status.busy": "2025-05-25T18:07:19.112795Z",
     "iopub.status.idle": "2025-05-25T18:07:19.117654Z",
     "shell.execute_reply": "2025-05-25T18:07:19.116772Z",
     "shell.execute_reply.started": "2025-05-25T18:07:19.113076Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import json\n",
    "import shutil\n",
    "from IPython.display import FileLink\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    default_data_collator,\n",
    ")\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:20:18.951669Z",
     "iopub.status.busy": "2025-05-25T17:20:18.951392Z",
     "iopub.status.idle": "2025-05-25T17:20:19.082264Z",
     "shell.execute_reply": "2025-05-25T17:20:19.081625Z",
     "shell.execute_reply.started": "2025-05-25T17:20:18.951647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:20:21.294561Z",
     "iopub.status.busy": "2025-05-25T17:20:21.294289Z",
     "iopub.status.idle": "2025-05-25T17:20:21.298077Z",
     "shell.execute_reply": "2025-05-25T17:20:21.297330Z",
     "shell.execute_reply.started": "2025-05-25T17:20:21.294539Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-25T17:20:08.512793Z",
     "iopub.status.idle": "2025-05-25T17:20:08.513199Z",
     "shell.execute_reply": "2025-05-25T17:20:08.513022Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:20:45.425971Z",
     "iopub.status.busy": "2025-05-25T17:20:45.425621Z",
     "iopub.status.idle": "2025-05-25T17:20:59.049219Z",
     "shell.execute_reply": "2025-05-25T17:20:59.048153Z",
     "shell.execute_reply.started": "2025-05-25T17:20:45.425945Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06c29e3ba1b4b6f9c64d5babe4fad71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b754f72e7a2a48c7b167b234043366c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cadcf00a493b40e48b8fd782efdd8fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# Thiáº¿t láº­p pad_token vÃ  padding side\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:21:28.062969Z",
     "iopub.status.busy": "2025-05-25T17:21:28.062655Z",
     "iopub.status.idle": "2025-05-25T17:21:28.068110Z",
     "shell.execute_reply": "2025-05-25T17:21:28.067152Z",
     "shell.execute_reply.started": "2025-05-25T17:21:28.062947Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    attention_masks = [item[\"attention_mask\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_masks,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:21:29.797994Z",
     "iopub.status.busy": "2025-05-25T17:21:29.797652Z",
     "iopub.status.idle": "2025-05-25T17:21:29.812506Z",
     "shell.execute_reply": "2025-05-25T17:21:29.811879Z",
     "shell.execute_reply.started": "2025-05-25T17:21:29.797969Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Continue Pretrain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:02:51.782934Z",
     "iopub.status.busy": "2025-05-25T03:02:51.782613Z",
     "iopub.status.idle": "2025-05-25T03:02:54.889569Z",
     "shell.execute_reply": "2025-05-25T03:02:54.888919Z",
     "shell.execute_reply.started": "2025-05-25T03:02:51.782904Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab96146c2024a1b8855d8c16e9c3b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/635 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d935df233d17414bb79a1183eba11d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/35.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4b1ff0664844eb932b08699f7a635d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/4.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c73f47f1d014b5880278e9d5623f2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/4.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319482b65aa44d1594d7fe4c1e76b86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/12806 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4548d22c245c41919618d4a62cab75f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca939bd27d244c088575c7b80bb8b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"vukhai248/vietnamese_news_16k\", split=\"train\")\n",
    "dataset = dataset.select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:02:54.891173Z",
     "iopub.status.busy": "2025-05-25T03:02:54.890933Z",
     "iopub.status.idle": "2025-05-25T03:02:54.894619Z",
     "shell.execute_reply": "2025-05-25T03:02:54.893717Z",
     "shell.execute_reply.started": "2025-05-25T03:02:54.891150Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    return text.replace(\"\\n\", \" \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:02:54.895648Z",
     "iopub.status.busy": "2025-05-25T03:02:54.895450Z",
     "iopub.status.idle": "2025-05-25T03:03:04.441895Z",
     "shell.execute_reply": "2025-05-25T03:03:04.440976Z",
     "shell.execute_reply.started": "2025-05-25T03:02:54.895632Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_fn(text, max_length=2048):\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "    attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "    labels = input_ids.clone()\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:03:04.443066Z",
     "iopub.status.busy": "2025-05-25T03:03:04.442775Z",
     "iopub.status.idle": "2025-05-25T03:03:10.608088Z",
     "shell.execute_reply": "2025-05-25T03:03:10.607451Z",
     "shell.execute_reply.started": "2025-05-25T03:03:04.443043Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "texts = dataset[\"content\"]\n",
    "tokenized_data = [tokenize_fn(text) for text in texts]\n",
    "train_loader = DataLoader(\n",
    "    tokenized_data,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:03:10.609190Z",
     "iopub.status.busy": "2025-05-25T03:03:10.608887Z",
     "iopub.status.idle": "2025-05-25T10:51:54.609669Z",
     "shell.execute_reply": "2025-05-25T10:51:54.608737Z",
     "shell.execute_reply.started": "2025-05-25T03:03:10.609162Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [2:35:22<00:00,  9.32s/it, loss=2.72] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Loss: 2.3339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [2:37:08<00:00,  9.43s/it, loss=2.06] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Loss: 2.2864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [2:36:12<00:00,  9.37s/it, loss=2.25] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Loss: 2.2713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, device, n_epochs=3, lr=1e-5):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\", leave=True)\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            # Forward pass with checkpointing\n",
    "            def forward_with_checkpoint(*args, **kwargs):\n",
    "                return torch.utils.checkpoint.checkpoint(\n",
    "                    lambda *args, **kwargs: model(*args, **kwargs),\n",
    "                    *args,\n",
    "                    **kwargs,\n",
    "                    use_reentrant=False\n",
    "                )\n",
    "\n",
    "            outputs = forward_with_checkpoint(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} - Loss: {avg_loss:.4f}\")\n",
    "train(model, train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T10:51:54.611200Z",
     "iopub.status.busy": "2025-05-25T10:51:54.610857Z",
     "iopub.status.idle": "2025-05-25T10:51:55.038424Z",
     "shell.execute_reply": "2025-05-25T10:51:55.037648Z",
     "shell.execute_reply.started": "2025-05-25T10:51:54.611169Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Pretrained model saved at: /kaggle/working/llama3-qlora-continued-pretrain\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-continued-pretrain\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"âœ… Pretrained model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T10:51:55.039477Z",
     "iopub.status.busy": "2025-05-25T10:51:55.039197Z",
     "iopub.status.idle": "2025-05-25T10:51:55.626208Z",
     "shell.execute_reply": "2025-05-25T10:51:55.625554Z",
     "shell.execute_reply.started": "2025-05-25T10:51:55.039447Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='llama3-qlora-continued-pretrain.zip' target='_blank'>llama3-qlora-continued-pretrain.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/llama3-qlora-continued-pretrain.zip"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ÄÆ°á»ng dáº«n tá»›i thÆ° má»¥c chá»©a mÃ´ hÃ¬nh\n",
    "model_dir = '/kaggle/working/llama3-qlora-continued-pretrain'\n",
    "\n",
    "# Táº¡o file nÃ©n .zip tá»« thÆ° má»¥c\n",
    "zip_name = 'llama3-qlora-continued-pretrain'  # TÃªn file nÃ©n (khÃ´ng cáº§n Ä‘uÃ´i .zip)\n",
    "shutil.make_archive(zip_name, 'zip', model_dir)\n",
    "\n",
    "# Táº¡o liÃªn káº¿t táº£i xuá»‘ng cho file .zip\n",
    "display(FileLink(f'{zip_name}.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finetune with QA dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:20:37.520991Z",
     "iopub.status.busy": "2025-05-25T17:20:37.520658Z",
     "iopub.status.idle": "2025-05-25T17:20:37.524611Z",
     "shell.execute_reply": "2025-05-25T17:20:37.523676Z",
     "shell.execute_reply.started": "2025-05-25T17:20:37.520964Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"/kaggle/input/llama-continued-pretrain-model/transformers/default/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:06.686944Z",
     "iopub.status.busy": "2025-05-25T11:06:06.686643Z",
     "iopub.status.idle": "2025-05-25T11:06:10.813181Z",
     "shell.execute_reply": "2025-05-25T11:06:10.812250Z",
     "shell.execute_reply.started": "2025-05-25T11:06:06.686922Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/vietnamese-squad/train-v2.0-translated.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    vi_squad_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:10.814439Z",
     "iopub.status.busy": "2025-05-25T11:06:10.814139Z",
     "iopub.status.idle": "2025-05-25T11:06:10.819212Z",
     "shell.execute_reply": "2025-05-25T11:06:10.818379Z",
     "shell.execute_reply.started": "2025-05-25T11:06:10.814410Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Train samples: 10000\n",
      "ğŸ“¦ Validation samples: 2000\n"
     ]
    }
   ],
   "source": [
    "train_data = vi_squad_data[:10000]\n",
    "\n",
    "# Láº¥y 2k máº«u tiáº¿p theo cho validation\n",
    "val_data = vi_squad_data[10000:12000]\n",
    "\n",
    "print(f\"ğŸ“¦ Train samples: {len(train_data)}\")\n",
    "print(f\"ğŸ“¦ Validation samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:10.820518Z",
     "iopub.status.busy": "2025-05-25T11:06:10.820173Z",
     "iopub.status.idle": "2025-05-25T11:06:15.870562Z",
     "shell.execute_reply": "2025-05-25T11:06:15.869725Z",
     "shell.execute_reply.started": "2025-05-25T11:06:10.820470Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Train samples: ['BeyoncÃ© Giselle Knowles-Carter (/ b i gÃ¬ É’ n s eÉª / bee-YON-say) (sinh ngÃ y 04 thÃ¡ng 9 1981) lÃ  má»™t ca sÄ©, nháº¡c sÄ©, nhÃ  sáº£n xuáº¥t thu Ã¢m vÃ  ná»¯ diá»…n viÃªn ngÆ°á»i Má»¹. Sinh ra vÃ  lá»›n lÃªn á»Ÿ Houston, Texas, cÃ´ Ä‘Ã£ biá»ƒu diá»…n trong cÃ¡c cuá»™c thi ca hÃ¡t vÃ  nháº£y mÃºa khÃ¡c nhau khi cÃ²n nhá», vÃ  ná»•i tiáº¿ng vÃ o cuá»‘i nhá»¯ng nÄƒm 1990 vá»›i tÆ° cÃ¡ch lÃ  ca sÄ© chÃ­nh cá»§a nhÃ³m nháº¡c ná»¯ R & B Destiny\\'s Child. ÄÆ°á»£c quáº£n lÃ½ bá»Ÿi cha cÃ´, Mathew Knowles, nhÃ³m Ä‘Ã£ trá»Ÿ thÃ nh má»™t trong nhá»¯ng nhÃ³m nháº¡c ná»¯ bÃ¡n cháº¡y nháº¥t tháº¿ giá»›i má»i thá»i Ä‘áº¡i. Sá»± giÃ¡n Ä‘oáº¡n cá»§a há» Ä‘Ã£ chá»©ng kiáº¿n viá»‡c phÃ¡t hÃ nh album Ä‘áº§u tay cá»§a BeyoncÃ©, Dangerously in Love (2003), giÃºp cÃ´ trá»Ÿ thÃ nh má»™t nghá»‡ sÄ© solo trÃªn toÃ n tháº¿ giá»›i, giÃ nh Ä‘Æ°á»£c nÄƒm giáº£i Grammy vÃ  cÃ³ Ä‘Ä©a Ä‘Æ¡n quÃ¡n quÃ¢n Billboard Hot 100 \"Crazy in Love\" vÃ  \"Baby Boy\".', 'Beyonce báº¯t Ä‘áº§u ná»•i tiáº¿ng tá»« khi nÃ o?', 'VÃ o cuá»‘i nhá»¯ng nÄƒm 1990']\n"
     ]
    }
   ],
   "source": [
    "print(f\"ğŸ“¦ Train samples: {train_data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:15.871686Z",
     "iopub.status.busy": "2025-05-25T11:06:15.871441Z",
     "iopub.status.idle": "2025-05-25T11:06:16.056357Z",
     "shell.execute_reply": "2025-05-25T11:06:16.055708Z",
     "shell.execute_reply.started": "2025-05-25T11:06:15.871666Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_list([{\"context\": item[0], \"question\": item[1], \"answer\": item[2]} for item in train_data])\n",
    "val_dataset = Dataset.from_list([{\"context\": item[0], \"question\": item[1], \"answer\": item[2]} for item in val_data])\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": val_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:16.057228Z",
     "iopub.status.busy": "2025-05-25T11:06:16.057027Z",
     "iopub.status.idle": "2025-05-25T11:06:16.067113Z",
     "shell.execute_reply": "2025-05-25T11:06:16.066179Z",
     "shell.execute_reply.started": "2025-05-25T11:06:16.057210Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. {'context': 'BeyoncÃ© Giselle Knowles-Carter (/ b i gÃ¬ É’ n s eÉª / bee-YON-say) (sinh ngÃ y 04 thÃ¡ng 9 1981) lÃ  má»™t ca sÄ©, nháº¡c sÄ©, nhÃ  sáº£n xuáº¥t thu Ã¢m vÃ  ná»¯ diá»…n viÃªn ngÆ°á»i Má»¹. Sinh ra vÃ  lá»›n lÃªn á»Ÿ Houston, Texas, cÃ´ Ä‘Ã£ biá»ƒu diá»…n trong cÃ¡c cuá»™c thi ca hÃ¡t vÃ  nháº£y mÃºa khÃ¡c nhau khi cÃ²n nhá», vÃ  ná»•i tiáº¿ng vÃ o cuá»‘i nhá»¯ng nÄƒm 1990 vá»›i tÆ° cÃ¡ch lÃ  ca sÄ© chÃ­nh cá»§a nhÃ³m nháº¡c ná»¯ R & B Destiny\\'s Child. ÄÆ°á»£c quáº£n lÃ½ bá»Ÿi cha cÃ´, Mathew Knowles, nhÃ³m Ä‘Ã£ trá»Ÿ thÃ nh má»™t trong nhá»¯ng nhÃ³m nháº¡c ná»¯ bÃ¡n cháº¡y nháº¥t tháº¿ giá»›i má»i thá»i Ä‘áº¡i. Sá»± giÃ¡n Ä‘oáº¡n cá»§a há» Ä‘Ã£ chá»©ng kiáº¿n viá»‡c phÃ¡t hÃ nh album Ä‘áº§u tay cá»§a BeyoncÃ©, Dangerously in Love (2003), giÃºp cÃ´ trá»Ÿ thÃ nh má»™t nghá»‡ sÄ© solo trÃªn toÃ n tháº¿ giá»›i, giÃ nh Ä‘Æ°á»£c nÄƒm giáº£i Grammy vÃ  cÃ³ Ä‘Ä©a Ä‘Æ¡n quÃ¡n quÃ¢n Billboard Hot 100 \"Crazy in Love\" vÃ  \"Baby Boy\".', 'question': 'Beyonce báº¯t Ä‘áº§u ná»•i tiáº¿ng tá»« khi nÃ o?', 'answer': 'VÃ o cuá»‘i nhá»¯ng nÄƒm 1990'}\n",
      "2. BeyoncÃ© Giselle Knowles-Carter (/ b i gÃ¬ É’ n s eÉª / bee-YON-say) (sinh ngÃ y 04 thÃ¡ng 9 1981) lÃ  má»™t ca sÄ©, nháº¡c sÄ©, nhÃ  sáº£n xuáº¥t thu Ã¢m vÃ  ná»¯ diá»…n viÃªn ngÆ°á»i Má»¹. Sinh ra vÃ  lá»›n lÃªn á»Ÿ Houston, Texas, cÃ´ Ä‘Ã£ biá»ƒu diá»…n trong cÃ¡c cuá»™c thi ca hÃ¡t vÃ  nháº£y mÃºa khÃ¡c nhau khi cÃ²n nhá», vÃ  ná»•i tiáº¿ng vÃ o cuá»‘i nhá»¯ng nÄƒm 1990 vá»›i tÆ° cÃ¡ch lÃ  ca sÄ© chÃ­nh cá»§a nhÃ³m nháº¡c ná»¯ R & B Destiny's Child. ÄÆ°á»£c quáº£n lÃ½ bá»Ÿi cha cÃ´, Mathew Knowles, nhÃ³m Ä‘Ã£ trá»Ÿ thÃ nh má»™t trong nhá»¯ng nhÃ³m nháº¡c ná»¯ bÃ¡n cháº¡y nháº¥t tháº¿ giá»›i má»i thá»i Ä‘áº¡i. Sá»± giÃ¡n Ä‘oáº¡n cá»§a há» Ä‘Ã£ chá»©ng kiáº¿n viá»‡c phÃ¡t hÃ nh album Ä‘áº§u tay cá»§a BeyoncÃ©, Dangerously in Love (2003), giÃºp cÃ´ trá»Ÿ thÃ nh má»™t nghá»‡ sÄ© solo trÃªn toÃ n tháº¿ giá»›i, giÃ nh Ä‘Æ°á»£c nÄƒm giáº£i Grammy vÃ  cÃ³ Ä‘Ä©a Ä‘Æ¡n quÃ¡n quÃ¢n Billboard Hot 100 \"Crazy in Love\" vÃ  \"Baby Boy\".\n",
      "3. Beyonce báº¯t Ä‘áº§u ná»•i tiáº¿ng tá»« khi nÃ o?\n",
      "4. VÃ o cuá»‘i nhá»¯ng nÄƒm 1990\n"
     ]
    }
   ],
   "source": [
    "print(f\"1. {dataset_dict['train'][0]}\")\n",
    "print(f\"2. {dataset_dict['train'][0]['context']}\")\n",
    "print(f\"3. {dataset_dict['train'][0]['question']}\")\n",
    "print(f\"4. {dataset_dict['train'][0]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:16.068330Z",
     "iopub.status.busy": "2025-05-25T11:06:16.068070Z",
     "iopub.status.idle": "2025-05-25T11:06:33.217119Z",
     "shell.execute_reply": "2025-05-25T11:06:33.216196Z",
     "shell.execute_reply.started": "2025-05-25T11:06:16.068298Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27c694acfcc45e48aef08fa794fe98a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742c0882c4a8464787c11b82c639a0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_qa(example, max_length=512):\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"]\n",
    "    prompt = f\"### ÄÃ¢y lÃ  dáº¡ng cÃ¢u há»i vÃ  tráº£ lá»i dá»±a trÃªn ná»™i dung ###\\n\\nCÃ¢u há»i: {question}\\n\\nNá»™i dung: {context}\\n\\nTráº£ lá»i:\"\n",
    "    completion = f\" {answer}\"\n",
    "    \n",
    "    prompt_ids = tokenizer(prompt, add_special_tokens=False).input_ids\n",
    "    completion_ids = tokenizer(completion, add_special_tokens=False).input_ids\n",
    "\n",
    "    input_ids = prompt_ids + completion_ids\n",
    "\n",
    "    labels = [-100] * len(prompt_ids) + completion_ids\n",
    "\n",
    "    if len(input_ids) > max_length:\n",
    "        input_ids = input_ids[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    \n",
    "    input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "    attention_mask = attention_mask + [0] * padding_length\n",
    "    labels = labels + [-100] * padding_length\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids),\n",
    "        \"attention_mask\": torch.tensor(attention_mask),\n",
    "        \"labels\": torch.tensor(labels)\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset_dict.map(preprocess_qa)\n",
    "\n",
    "tokenized_dataset.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:33.218537Z",
     "iopub.status.busy": "2025-05-25T11:06:33.218201Z",
     "iopub.status.idle": "2025-05-25T11:06:33.223084Z",
     "shell.execute_reply": "2025-05-25T11:06:33.222309Z",
     "shell.execute_reply.started": "2025-05-25T11:06:33.218500Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"], batch_size=4, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_dataset[\"test\"], batch_size=4, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-25T17:18:25.136Z",
     "iopub.execute_input": "2025-05-25T11:06:33.224338Z",
     "iopub.status.busy": "2025-05-25T11:06:33.224038Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:   0%|          | 0/2500 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [3:22:25<00:00,  4.86s/it, loss=1.42]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 1/2 - Train Loss: 1.7102\n",
      "ğŸ“Š Validation Loss: 1.3921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  25%|â–ˆâ–ˆâ–       | 621/2500 [50:22<2:32:00,  4.85s/it, loss=2.36] "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "n_epochs = 2\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\", leave=True)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"âœ… Epoch {epoch+1}/{n_epochs} - Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"ğŸ“Š Validation Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-25T17:18:25.136Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-qa-finetune\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"âœ… Pretrained model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-25T17:18:25.137Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_dir = '/kaggle/working/llama3-qlora-qa-finetune'\n",
    "\n",
    "# Táº¡o file nÃ©n .zip tá»« thÆ° má»¥c\n",
    "zip_name = 'llama3-qlora-qa-finetune'  # TÃªn file nÃ©n (khÃ´ng cáº§n Ä‘uÃ´i .zip)\n",
    "shutil.make_archive(zip_name, 'zip', model_dir)\n",
    "\n",
    "# Táº¡o liÃªn káº¿t táº£i xuá»‘ng cho file .zip\n",
    "display(FileLink(f'{zip_name}.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finetune with BBC News**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-25T10:52:16.017889Z",
     "iopub.status.idle": "2025-05-25T10:52:16.018413Z",
     "shell.execute_reply": "2025-05-25T10:52:16.018086Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-25T10:52:16.019272Z",
     "iopub.status.idle": "2025-05-25T10:52:16.019645Z",
     "shell.execute_reply": "2025-05-25T10:52:16.019486Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/kaggle/input/data-llama-finetune/bbc_data_llama_finetune.json\"\n",
    "\n",
    "full_dataset = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
    "dataset = full_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-25T10:52:16.020450Z",
     "iopub.status.idle": "2025-05-25T10:52:16.020843Z",
     "shell.execute_reply": "2025-05-25T10:52:16.020669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize(example, max_length=2048):\n",
    "    prompt = f\"### ÄÃ¢y lÃ  dáº¡ng tÃ³m táº¯t vÄƒn báº£n tin tá»©c vá»›i Ä‘á»™ dÃ i tÃ³m táº¯t Ä‘áº§u ra khoáº£ng 150 tá»«:  ### Lá»‡nh:\\n{example['prompt']}\\n\\nTÃ³m táº¯t:\\n\"\n",
    "    summary = example[\"summary\"]\n",
    "    prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    summary_ids = tokenizer.encode(summary, add_special_tokens=False)\n",
    "    \n",
    "    # Kiá»ƒm tra tá»•ng sá»‘ token\n",
    "    total_length = len(prompt_ids) + len(summary_ids)\n",
    "    if total_length > max_length:\n",
    "        overflow = total_length - max_length\n",
    "        if overflow < len(prompt_ids):\n",
    "            prompt_ids = prompt_ids[:-overflow]\n",
    "        else:\n",
    "            prompt_ids = []\n",
    "    input_ids = prompt_ids + summary_ids\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    labels = [-100] * len(prompt_ids) + summary_ids\n",
    "\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "        attention_mask = attention_mask + [0] * padding_length\n",
    "        labels = labels + [-100] * padding_length\n",
    "    else:\n",
    "        input_ids = input_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "tokenized_dataset = dataset.map(tokenize, batched=False, fn_kwargs={\"max_length\": 2048})\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "collator = default_data_collator\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_dataset[\"test\"],\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-25T10:52:16.021384Z",
     "iopub.status.idle": "2025-05-25T10:52:16.021763Z",
     "shell.execute_reply": "2025-05-25T10:52:16.021592Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-25T10:52:16.022474Z",
     "iopub.status.idle": "2025-05-25T10:52:16.022860Z",
     "shell.execute_reply": "2025-05-25T10:52:16.022692Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in pbar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"âœ… Epoch {epoch+1} completed â€” Avg loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-25T10:52:16.023979Z",
     "iopub.status.idle": "2025-05-25T10:52:16.024365Z",
     "shell.execute_reply": "2025-05-25T10:52:16.024203Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-bbc-finetuned\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"âœ… Finetuned model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-25T10:52:16.025408Z",
     "iopub.status.idle": "2025-05-25T10:52:16.025791Z",
     "shell.execute_reply": "2025-05-25T10:52:16.025609Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_dir = '/kaggle/working/llama3-qlora-bbc-finetuned'\n",
    "\n",
    "# Táº¡o file nÃ©n .zip tá»« thÆ° má»¥c\n",
    "zip_name = 'llama3-qlora-qa-finetune'  # TÃªn file nÃ©n (khÃ´ng cáº§n Ä‘uÃ´i .zip)\n",
    "shutil.make_archive(zip_name, 'zip', model_dir)\n",
    "\n",
    "# Táº¡o liÃªn káº¿t táº£i xuá»‘ng cho file .zip\n",
    "display(FileLink(f'{zip_name}.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finetune with donvanban**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-25T10:52:16.026572Z",
     "iopub.status.idle": "2025-05-25T10:52:16.026934Z",
     "shell.execute_reply": "2025-05-25T10:52:16.026776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#model_id = \"/kaggle/input/llama_finetune_v1.3/transformers/default/1/data_llama_finetune_v1.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:22:29.581846Z",
     "iopub.status.busy": "2025-05-25T17:22:29.581510Z",
     "iopub.status.idle": "2025-05-25T17:22:29.951594Z",
     "shell.execute_reply": "2025-05-25T17:22:29.950980Z",
     "shell.execute_reply.started": "2025-05-25T17:22:29.581820Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce268e87e19d4f8db2d54debd7d26ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = \"/kaggle/input/data-llama-finetune/vi_data_llama_finetune.json\"\n",
    "\n",
    "full_dataset = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
    "dataset = full_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:22:31.438066Z",
     "iopub.status.busy": "2025-05-25T17:22:31.437779Z",
     "iopub.status.idle": "2025-05-25T17:22:32.473477Z",
     "shell.execute_reply": "2025-05-25T17:22:32.472566Z",
     "shell.execute_reply.started": "2025-05-25T17:22:31.438043Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a4d26f611d4a3ab33d4a1479e5feb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9818c8e5f3d24b90b6789c9b0a8bb105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(example, max_length=2048):\n",
    "    prompt = f\"### ÄÃ¢y lÃ  dáº¡ng tÃ³m táº¯t vÄƒn báº£n tin tá»©c vá»›i Ä‘á»™ dÃ i tÃ³m táº¯t Ä‘áº§u ra khoáº£ng 150 tá»«:  ### Lá»‡nh:\\n{example['prompt']}\\n\\nTÃ³m táº¯t:\\n\"\n",
    "    summary = example[\"summary\"]\n",
    "    \n",
    "    # MÃ£ hÃ³a riÃªng prompt vÃ  summary (khÃ´ng thÃªm special tokens)\n",
    "    prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    summary_ids = tokenizer.encode(summary, add_special_tokens=False)\n",
    "    \n",
    "    # Kiá»ƒm tra tá»•ng sá»‘ token\n",
    "    total_length = len(prompt_ids) + len(summary_ids)\n",
    "    if total_length > max_length:\n",
    "        overflow = total_length - max_length\n",
    "        # Æ¯u tiÃªn giá»¯ láº¡i pháº§n summary; cáº¯t bá»›t prompt\n",
    "        if overflow < len(prompt_ids):\n",
    "            prompt_ids = prompt_ids[:-overflow]\n",
    "        else:\n",
    "            prompt_ids = []  # Náº¿u quÃ¡ trÃ n, bá» háº¿t prompt\n",
    "    \n",
    "    # Ná»‘i prompt vÃ  summary\n",
    "    input_ids = prompt_ids + summary_ids\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    \n",
    "    # Táº¡o labels: pháº§n prompt Ä‘Æ°á»£c mask báº±ng -100, pháº§n summary giá»¯ nguyÃªn token IDs\n",
    "    labels = [-100] * len(prompt_ids) + summary_ids\n",
    "    \n",
    "    # Padding táº¥t cáº£ cÃ¡c trÆ°á»ng vá» Ä‘á»™ dÃ i max_length\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "        attention_mask = attention_mask + [0] * padding_length\n",
    "        labels = labels + [-100] * padding_length\n",
    "    else:\n",
    "        # Náº¿u quÃ¡ dÃ i, cáº¯t bá»›t (nÃªn khÃ´ng xáº£y ra nhá» truncation á»Ÿ trÃªn)\n",
    "        input_ids = input_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "tokenized_dataset = dataset.map(tokenize, batched=False, fn_kwargs={\"max_length\": 2048})\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "collator = default_data_collator\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_dataset[\"test\"],\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:22:34.806048Z",
     "iopub.status.busy": "2025-05-25T17:22:34.805723Z",
     "iopub.status.idle": "2025-05-25T17:22:34.823068Z",
     "shell.execute_reply": "2025-05-25T17:22:34.822212Z",
     "shell.execute_reply.started": "2025-05-25T17:22:34.806021Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:22:37.392908Z",
     "iopub.status.busy": "2025-05-25T17:22:37.392605Z",
     "iopub.status.idle": "2025-05-25T18:06:52.107451Z",
     "shell.execute_reply": "2025-05-25T18:06:52.106608Z",
     "shell.execute_reply.started": "2025-05-25T17:22:37.392886Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/72 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Loss: 1.2355: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [14:43<00:00, 12.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 1 completed â€” Avg loss: 0.5272\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.6456: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [14:46<00:00, 12.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 2 completed â€” Avg loss: 0.4752\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1448: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [14:45<00:00, 12.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 3 completed â€” Avg loss: 0.4345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in pbar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"âœ… Epoch {epoch+1} completed â€” Avg loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T18:06:52.109000Z",
     "iopub.status.busy": "2025-05-25T18:06:52.108703Z",
     "iopub.status.idle": "2025-05-25T18:06:52.599533Z",
     "shell.execute_reply": "2025-05-25T18:06:52.598727Z",
     "shell.execute_reply.started": "2025-05-25T18:06:52.108977Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Finetuned model saved at: /kaggle/working/llama3-qlora-finetuned-all\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-finetuned-all\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"âœ… Finetuned model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T18:07:23.111501Z",
     "iopub.status.busy": "2025-05-25T18:07:23.111220Z",
     "iopub.status.idle": "2025-05-25T18:07:23.716002Z",
     "shell.execute_reply": "2025-05-25T18:07:23.715118Z",
     "shell.execute_reply.started": "2025-05-25T18:07:23.111479Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='llama3-qlora-donvanban-finetune.zip' target='_blank'>llama3-qlora-donvanban-finetune.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/llama3-qlora-donvanban-finetune.zip"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_dir = '/kaggle/working/llama3-qlora-finetuned-all'\n",
    "\n",
    "# Táº¡o file nÃ©n .zip tá»« thÆ° má»¥c\n",
    "zip_name = 'llama3-qlora-donvanban-finetune'  # TÃªn file nÃ©n (khÃ´ng cáº§n Ä‘uÃ´i .zip)\n",
    "shutil.make_archive(zip_name, 'zip', model_dir)\n",
    "\n",
    "# Táº¡o liÃªn káº¿t táº£i xuá»‘ng cho file .zip\n",
    "display(FileLink(f'{zip_name}.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T18:07:25.381781Z",
     "iopub.status.busy": "2025-05-25T18:07:25.381370Z",
     "iopub.status.idle": "2025-05-25T18:07:25.394653Z",
     "shell.execute_reply": "2025-05-25T18:07:25.393768Z",
     "shell.execute_reply.started": "2025-05-25T18:07:25.381738Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_after_training(model, val_loader, tokenizer, device, num_samples=3):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    samples_to_print = []\n",
    "\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bert_score = evaluate.load(\"bertscore\")\n",
    "\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=128,\n",
    "                num_beams=4,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "\n",
    "            generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            label_texts = []\n",
    "            for label in labels:\n",
    "                label_ids = [token_id for token_id in label.tolist() if token_id != -100]\n",
    "                label_texts.append(tokenizer.decode(label_ids, skip_special_tokens=True))\n",
    "\n",
    "            predictions.extend(generated_texts)\n",
    "            references.extend(label_texts)\n",
    "\n",
    "            if len(samples_to_print) < num_samples:\n",
    "                for src, ref, pred in zip(tokenizer.batch_decode(input_ids, skip_special_tokens=True), label_texts, generated_texts):\n",
    "                    samples_to_print.append((src, ref, pred))\n",
    "                    if len(samples_to_print) >= num_samples:\n",
    "                        break\n",
    "\n",
    "    # ROUGE\n",
    "    rouge_results = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "\n",
    "    # BERTScore (tiáº¿ng Viá»‡t)\n",
    "    bert_results = bert_score.compute(predictions=predictions, references=references, lang=\"vi\")\n",
    "    bert_f1 = np.mean(bert_results[\"f1\"])\n",
    "\n",
    "    # In káº¿t quáº£\n",
    "    print(\"\\nğŸ“Š ROUGE Scores:\")\n",
    "    for key in rouge_results:\n",
    "        print(f\"{key}: {rouge_results[key]:.4f}\")\n",
    "\n",
    "    print(f\"\\nğŸ“ˆ BERTScore F1: {bert_f1:.4f}\")\n",
    "\n",
    "    print(\"\\nğŸ“ Sample Results:\")\n",
    "    for i, (src, ref, pred) in enumerate(samples_to_print):\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        print(f\"[Prompt]    {src[:300]}...\")  # rÃºt gá»n prompt náº¿u quÃ¡ dÃ i\n",
    "        print(f\"[Reference] {ref}\")\n",
    "        print(f\"[Generated] {pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T18:08:02.175720Z",
     "iopub.status.busy": "2025-05-25T18:08:02.175392Z",
     "iopub.status.idle": "2025-05-25T18:13:56.809448Z",
     "shell.execute_reply": "2025-05-25T18:13:56.808518Z",
     "shell.execute_reply.started": "2025-05-25T18:08:02.175676Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/9 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  11%|â–ˆ         | 1/9 [00:41<05:33, 41.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|â–ˆâ–ˆâ–       | 2/9 [01:19<04:37, 39.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–      | 3/9 [01:59<03:59, 39.84s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [02:39<03:18, 39.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [03:19<02:39, 39.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [03:59<01:59, 39.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [04:38<01:18, 39.50s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [05:42<00:00, 38.09s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b759dd5c68141d6a8112e0d8b5c9cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10416111f9f04d1ea6651cd46921afb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0e5678ec9a48008e4f878d4cd6e447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2547c939e6b48de893490998ad33417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8d2aa809c14d4287ea296bac45253b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ROUGE Scores:\n",
      "rouge1: 0.2543\n",
      "rouge2: 0.2515\n",
      "rougeL: 0.2529\n",
      "rougeLsum: 0.2533\n",
      "\n",
      "ğŸ“ˆ BERTScore F1: 0.7446\n",
      "\n",
      "ğŸ“ Sample Results:\n",
      "\n",
      "--- Sample 1 ---\n",
      "[Prompt]    ### ÄÃ¢y lÃ  dáº¡ng tÃ³m táº¯t vÄƒn báº£n tin tá»©c vá»›i Ä‘á»™ dÃ i tÃ³m táº¯t Ä‘áº§u ra khoáº£ng 150 tá»«:  ### Lá»‡nh:\n",
      "Báº¡n lÃ  má»™t trá»£ lÃ½ tÃ³m táº¯t vÄƒn báº£n. HÃ£y cung cáº¥p báº£n tÃ³m táº¯t ngáº¯n gá»n vÃ  chÃ­nh xÃ¡c trong 150 chá»¯ cho bÃ i viáº¿t sau. BÃ i viáº¿t: ï»¿HoÃ ng Anh Gia Lai tÃ¡i cáº¥u trÃºc\n",
      "CÃ´ng ty cá»§a báº§u Äá»©c sáº½ bÃ¡n cÃ¡c dá»± Ã¡n thuá»™c lÄ©nh vá»±c ...\n",
      "[Reference] ï»¿CÃ´ng ty cá»• pháº§n HoÃ ng Anh Gia Lai vá»«a thÃ´ng qua chá»§ trÆ°Æ¡ng tÃ¡i cáº¥u trÃºc cÃ¡c Ä‘Æ¡n vá»‹ thuá»™c ngÃ nh thá»§y Ä‘iá»‡n.\n",
      "Äá»“ng thá»i, cÅ©ng thá»‘ng nháº¥t thÃ´ng qua chá»§ trÆ°Æ¡ng tÃ¡i cáº¥u trÃºc cÃ¡c Ä‘Æ¡n vá»‹ thuá»™c ngÃ nh báº¥t Ä‘á»™ng sáº£n.\n",
      " Má»¥c Ä‘Ã­ch, nháº±m thu tiá»n máº·t vá» dá»± trá»¯ vÃ  giáº£m ná»£ vay.\n",
      "Tá»· trá»ng doanh thu tá»« báº¥t Ä‘á»™ng sáº£n trong nÄƒm 2013 cá»§a cÃ´ng ty sáº½ giáº£m tá»« 64% xuá»‘ng cÃ²n 14%.\n",
      "[Generated] ### ÄÃ¢y lÃ  dáº¡ng tÃ³m táº¯t vÄƒn báº£n tin tá»©c vá»›i Ä‘á»™ dÃ i tÃ³m táº¯t Ä‘áº§u ra khoáº£ng 150 tá»«:  ### Lá»‡nh:\n",
      "Báº¡n lÃ  má»™t trá»£ lÃ½ tÃ³m táº¯t vÄƒn báº£n. HÃ£y cung cáº¥p báº£n tÃ³m táº¯t ngáº¯n gá»n vÃ  chÃ­nh xÃ¡c trong 150 chá»¯ cho bÃ i viáº¿t sau. BÃ i viáº¿t: ï»¿HoÃ ng Anh Gia Lai tÃ¡i cáº¥u trÃºc\n",
      "CÃ´ng ty cá»§a báº§u Äá»©c sáº½ bÃ¡n cÃ¡c dá»± Ã¡n thuá»™c lÄ©nh vá»±c thá»§y Ä‘iá»‡n vÃ  báº¥t Ä‘á»™ng sáº£n nháº±m dá»± trá»¯ tiá»n máº·t vÃ  giáº£m ná»£ vay. \n",
      "CÃ´ng ty cá»• pháº§n HoÃ ng Anh Gia Lai (MÃ£ CK: HAG) vá»«a cÃ´ng bá»‘ nghá»‹ quyáº¿t HÄQT. Theo Ä‘Ã³, cÃ´ng ty thÃ´ng qua chá»§ trÆ°Æ¡ng tÃ¡i cáº¥u trÃºc cÃ¡c Ä‘Æ¡n vá»‹ thuá»™c ngÃ nh thá»§y Ä‘iá»‡n báº±ng cÃ¡ch bÃ¡n cÃ¡c dá»± Ã¡n Ä‘Ã£ Ä‘i vÃ o hoáº¡t Ä‘á»™ng vÃ  Ä‘ang trong giai Ä‘oáº¡n Ä‘áº§u tÆ°.\n",
      "Äá»“ng thá»i, HoÃ ng Anh Gia Lai cÅ©ng thá»‘ng nháº¥t thÃ´ng qua chá»§ trÆ°Æ¡ng tÃ¡i cáº¥u trÃºc cÃ¡c Ä‘Æ¡n vá»‹ thuá»™c ngÃ nh báº¥t Ä‘á»™ng sáº£n báº±ng hÃ¬nh thá»©c bÃ¡n sá»‰ cÄƒn há»™ vÃ  bÃ¡n cÃ¡c dá»± Ã¡n hoáº·c cá»• phiáº¿u cá»§a cÃ¡c cÃ´ng ty con Ä‘ang sá»Ÿ há»¯u dá»± Ã¡n. Má»¥c Ä‘Ã­ch bÃ¡n, theo doanh nghiá»‡p, nháº±m thu tiá»n máº·t vá» dá»± trá»¯ vÃ  giáº£m ná»£ vay.\n",
      "TrÆ°á»›c Ä‘Ã³, trong bÃ¡o cÃ¡o tÃ i chÃ­nh há»£p nháº¥t quÃ½ I, cÃ´ng ty do Ã´ng ÄoÃ n NguyÃªn Äá»©c lÃ m Chá»§ tá»‹ch cÃ³ 2.444 tá»· Ä‘á»“ng tiá»n vÃ  cÃ¡c khoáº£n tÆ°Æ¡ng Ä‘Æ°Æ¡ng tiá»n, ná»£ ngáº¯n háº¡n lÃ  3.446 tá»· Ä‘á»“ng. Táº¡i ÄHCÄ, báº§u Äá»©c cho biáº¿t, tá»· trá»ng doanh thu tá»« báº¥t Ä‘á»™ng sáº£n trong nÄƒm 2013 sáº½ giáº£m tá»« 64% xuá»‘ng cÃ²n 14%. TÃ³m táº¯t: \n",
      "\n",
      "TÃ³m táº¯t:\n",
      "ï»¿CÃ´ng ty cá»• pháº§n HoÃ ng Anh Gia Lai vá»«a thÃ´ng qua chá»§ trÆ°Æ¡ng tÃ¡i cáº¥u trÃºc cÃ¡c Ä‘Æ¡n vá»‹ thuá»™c ngÃ nh thá»§y Ä‘iá»‡n.\n",
      "Äá»“ng thá»i, cÅ©ng thá»‘ng nháº¥t thÃ´ng qua chá»§ trÆ°Æ¡ng tÃ¡i cáº¥u trÃºc cÃ¡c Ä‘Æ¡n vá»‹ thuá»™c ngÃ nh báº¥t Ä‘á»™ng sáº£n.\n",
      " Má»¥c Ä‘Ã­ch, nháº±m thu tiá»n máº·t vá» dá»± trá»¯ vÃ  giáº£m ná»£ vay.\n",
      "Tá»· trá»ng doanh thu tá»« báº¥t Ä‘á»™ng sáº£n trong nÄƒm 2013 cá»§a cÃ´ng ty sáº½ giáº£m tá»« 64% xuá»‘ng cÃ²n 14%.Question:\n",
      "ï»¿CÃ´ng ty cá»• pháº§n HoÃ ng Anh Gia Lai vá»«a thÃ´ng qua chá»§ trÆ°Æ¡ng tÃ¡i cáº¥u trÃºc cÃ¡c Ä‘Æ¡n vá»‹ thuá»™c ngÃ nh thá»§y Ä‘iá»‡n.\n",
      "Äá»“ng thá»i, cÅ©ng thá»‘ng nháº¥t thÃ´ng qua chá»§ trÆ°Æ¡ng tÃ¡i cáº¥u trÃºc cÃ¡c Ä‘Æ¡n vá»‹ thuá»™c ngÃ nh báº¥t Ä‘á»™ng sáº£n.\n",
      "Má»¥c Ä‘Ã­ch, nháº±m thu tiá»n máº·t vá» dá»± trá»¯ vÃ  giáº£m ná»£ vay.\n",
      "Tá»· trá»ng doanh thu tá»« báº¥t Ä‘á»™ng sáº£n trong nÄƒm 2013 cá»§a cÃ´ng ty sáº½ giáº£m tá»« 64% xuá»‘ng cÃ²n 14%.\n",
      "ï»¿CÃ´ng ty cá»• pháº§n HoÃ ng Anh Gia Lai vá»«a thÃ´ng qua chá»§ trÆ°Æ¡ng tÃ¡i cáº¥u trÃºc cÃ¡c Ä‘Æ¡n vá»‹ thuá»™c ngÃ nh thá»§y Ä‘iá»‡n.\n",
      "Äá»“ng thá»i\n",
      "\n",
      "--- Sample 2 ---\n",
      "[Prompt]    ### ÄÃ¢y lÃ  dáº¡ng tÃ³m táº¯t vÄƒn báº£n tin tá»©c vá»›i Ä‘á»™ dÃ i tÃ³m táº¯t Ä‘áº§u ra khoáº£ng 150 tá»«:  ### Lá»‡nh:\n",
      "Báº¡n lÃ  má»™t trá»£ lÃ½ tÃ³m táº¯t vÄƒn báº£n. HÃ£y cung cáº¥p báº£n tÃ³m táº¯t ngáº¯n gá»n vÃ  chÃ­nh xÃ¡c trong 150 chá»¯ cho bÃ i viáº¿t sau. BÃ i viáº¿t: ï»¿Táº¡p chÃ­ Má»¹ bá»‹ lÃªn Ã¡n vÃ¬ chá»¥p áº£nh \"thá»i trang tá»± tá»­\"\n",
      " Táº¡p chÃ­ Vice cá»§a Má»¹ vá»«a bá»‹ lÃªn...\n",
      "[Reference] ï»¿Táº¡p chÃ­ Vice cá»§a Má»¹  vá»«a bá»‹ lÃªn Ã¡n vÃ¬ thá»±c hiá»‡n bá»™ áº£nh \"thá»i trang tá»± tá»­\", tÃ¡i hiá»‡n láº¡i giÃ¢y phÃºt cuá»‘i Ä‘á»i cá»§a nhá»¯ng ná»¯ vÄƒn sÄ© ná»•i tiáº¿ng. Trong nhá»¯ng nÄƒm gáº§n dÃ¢y, cÃ¡c tá» táº¡p chÃ­ thÆ°á»nng máº¡nh dáº¡n Ä‘Æ°a ra nhá»¯ng Ã½ tÆ°á»Ÿng má»›i láº¡, nhiá»u khi trá»Ÿ thÃ nh quÃ¡ Ä‘Ã . Sau khi nhá»¯ng bá»©c hÃ¬nh tá»± tá»­ nÃ y bá»‹ xÃ³a Ä‘i, Vice Ä‘Ã£ Ä‘Äƒng táº£i lá»i xin lá»—i, bá»‹ cho lÃ  khÃ´ng thuyáº¿t phá»¥c. Khoa há»c Ä‘Ã£ chá»©ng minh, Ä‘á»‘i vá»›i nhá»¯ng ai tá»«ng xuáº¥t hiá»‡n trong Ä‘áº§u Ã½ nghÄ© Ä‘en tá»‘i vá» viá»‡c tá»± tá»­, viá»‡c bÃ¡o chÃ­ hoáº·c truyá»n hÃ¬nh Ä‘Æ°a ra nhá»¯ng hÃ¬nh áº£nh trá»±c quan vá» hÃ nh Ä‘á»™ng nÃ y khÃ´ng khÃ¡c gÃ¬ nhen nhÃ³m láº¡i Ã½ tÆ°á»Ÿng Ä‘Ã³ trong Ä‘áº§u ngÆ°á»i xem. CÃ¡ch tiáº¿p cáº­n Ä‘á» tÃ i  bá»‹ Ä‘Ã¡nh giÃ¡ lÃ  thiáº¿u tÃ´n trá»ng ngÆ°á»i Ä‘Ã£ khuáº¥t.\n",
      "[Generated] ### ÄÃ¢y lÃ  dáº¡ng tÃ³m táº¯t vÄƒn báº£n tin tá»©c vá»›i Ä‘á»™ dÃ i tÃ³m táº¯t Ä‘áº§u ra khoáº£ng 150 tá»«:  ### Lá»‡nh:\n",
      "Báº¡n lÃ  má»™t trá»£ lÃ½ tÃ³m táº¯t vÄƒn báº£n. HÃ£y cung cáº¥p báº£n tÃ³m táº¯t ngáº¯n gá»n vÃ  chÃ­nh xÃ¡c trong 150 chá»¯ cho bÃ i viáº¿t sau. BÃ i viáº¿t: ï»¿Táº¡p chÃ­ Má»¹ bá»‹ lÃªn Ã¡n vÃ¬ chá»¥p áº£nh \"thá»i trang tá»± tá»­\"\n",
      " Táº¡p chÃ­ Vice cá»§a Má»¹ vá»«a bá»‹ lÃªn Ã¡n vÃ¬ thá»±c hiá»‡n bá»™ áº£nh \"thá»i trang tá»± tá»­\", tÃ¡i hiá»‡n láº¡i giÃ¢y phÃºt cuá»‘i Ä‘á»i cá»§a nhá»¯ng ná»¯ vÄƒn sÄ© ná»•i tiáº¿ng. Trong nhá»¯ng nÄƒm gáº§n Ä‘Ã¢y, Ä‘á»ƒ cáº¡nh tranh vá»›i nhá»¯ng bÃ¡o khÃ¡c, cÃ¡c tá» táº¡p chÃ­ thÆ°á»ng máº¡nh dáº¡n Ä‘Æ°a ra nhá»¯ng Ã½ tÆ°á»Ÿng má»›i láº¡, nhiá»u khi trá»Ÿ thÃ nh quÃ¡ Ä‘Ã .  Äá»‘i vá»›i Ã½ tÆ°á»Ÿng sÃ¡ng táº¡o \"tháº£m há»a\" láº§n nÃ y cá»§a táº¡p chÃ­ Vice, tá» Huffington Post Ä‘Ã¡nh giÃ¡ ráº±ng: \"Vice nghÄ© há» Ä‘ang táº¡o ra nhá»¯ng bá»©c áº£nh nghá»‡ thuáº­t nhÆ°ng cuá»‘i cÃ¹ng ngÆ°á»i ta chá»‰ tháº¥y Ä‘Ã³ lÃ  nhá»¯ng bá»©c áº£nh thiáº¿u suy nghÄ© vÃ  pháº£n cáº£m\". Sau khi nhá»¯ng bá»©c hÃ¬nh tá»± tá»­ nÃ y bá»‹ xÃ³a Ä‘i, Vice Ä‘Ã£ Ä‘Äƒng táº£i lá»i xin lá»—i: \"Nhá»¯ng bá»©c áº£nh thá»i trang cá»§a Vice luÃ´n phi truyá»n thá»‘ng vÃ  tiáº¿p cáº­n tá»« gÃ³c nhÃ¬n nghá»‡ thuáº­t hÆ¡n lÃ  nhá»¯ng bá»©c áº£nh thuáº§n tÃºy giá»›i thiá»‡u xu hÆ°á»›ng thá»i trang. Má»¥c tiÃªu cá»§a chÃºng tÃ´i lÃ  táº¡o ra nhá»¯ng bá»©c hÃ¬nh nghá»‡ thuáº­t Ä‘á»ƒ tá»« Ä‘Ã³ thÃ´ng Ä‘iá»‡p thá»i trang sáº½ Ä‘i theo sau, thay vÃ¬ Ä‘i trÆ°á»›c nhÆ° nhiá»u tá» bÃ¡o khÃ¡c. Bá»™ hÃ¬nh \"Last Words\" (Nhá»¯ng lá»i cuá»‘i cÃ¹ng) Ä‘Æ°á»£c táº¡o ra theo tÆ° duy nÃ y vÃ  táº­p trung vÃ o cÃ¡i cháº¿t cá»§a nhá»¯ng ná»¯ vÄƒn sÄ© ná»•i tiáº¿ng mÃ  cÃ¡c Ä‘á»™c giáº£ yÃªu quÃ½ há» háº³n Ä‘á»u mong ráº±ng giÃ¡ nhÆ° cuá»™c Ä‘á»i cá»§a nhá»¯ng tÃ i nÄƒng nÃ y Ä‘á»«ng ngáº¯n ngá»§i nhÆ° váº­y\". Lá»i xin lá»—i cá»§a Vice bá»‹ cho lÃ  khÃ´ng thuyáº¿t phá»¥c. Tháº­t khÃ³ hiá»ƒu táº¡i sao ban biÃªn táº­p cá»§a bÃ¡o cÃ³ thá»ƒ Ä‘á»ƒ nhá»¯ng bá»©c áº£nh nÃ y xuáº¥t hiá»‡n trÃªn trang cá»§a mÃ¬nh. Tá» Huffington Post nháº­n Ä‘á»‹nh thÃªm: Vice Ä‘ang cá»‘ tÃ¬nh giáº£i thÃ­ch lÃ²ng vÃ²ng xung quanh viá»‡c thá»±c hiá»‡n má»™t bá»™ áº£nh vÃ´ nghÄ©a. Äá»‘i vá»›i má»™t nhÃ  vÄƒn, há» Ä‘Æ°á»£c cÃ´ng chÃºng nhá»› tá»›i nhá» nhá»¯ng tÃ¡c pháº©m hay. Táº¡i sao Vice khÃ´ng tÃ´n vinh nhá»¯ng tÃ¡c pháº©m Ä‘Ã³ mÃ  láº¡i nháº¥n máº¡nh vÃ o nhá»¯ng giÃ¢y phÃºt cuá»‘i Ä‘á»i Ä‘en tá»‘i, cÃ´ Ä‘á»™c, vá»›i Ä‘áº§y ná»—i sá»£ hÃ£i vÃ  buá»“n khá»• nhÆ° váº­y? Äá»‘i vá»›i táº¥t cáº£ nhá»¯ng ai tá»«ng tráº£i qua tÃ¢m tráº¡ng bi Ä‘Ã¡t tá»›i má»©c muá»‘n tá»± tá»­ hoáº·c cÃ³ ngÆ°á»i thÃ¢n tá»«ng tá»± tá»­, háº³n há» sáº½ cáº£m tháº¥y vÃ´ cÃ¹ng khÃ³ chá»‹u trÆ°á»›c nhá»¯ng bá»©c áº£nh nÃ y. \n",
      "Khoa há»c Ä‘Ã£ chá»©ng minh ráº±ng, Ä‘á»‘i vá»›i nhá»¯ng ai tá»«ng xuáº¥t hiá»‡n trong Ä‘áº§u Ã½ nghÄ© Ä‘en tá»‘i vá» viá»‡c tá»± tá»­, viá»‡c bÃ¡o chÃ­ hoáº·c truyá»n hÃ¬nh Ä‘Æ°a ra nhá»¯ng hÃ¬nh áº£nh trá»±c quan vá» hÃ nh Ä‘á»™ng nÃ y khÃ´ng khÃ¡c gÃ¬ nhen nhÃ³m láº¡i Ã½ tÆ°á»Ÿng Ä‘Ã³ trong Ä‘áº§u ngÆ°á»i xem. Äá»‘i vá»›i nhá»¯ng ná»¯ nhÃ  vÄƒn mÃ  tá» Vice Ä‘á» cáº­p tá»›i, cÃ¡ch tiáº¿p cáº­n Ä‘á» tÃ i cá»§a há» bá»‹ Ä‘Ã¡nh giÃ¡ lÃ  thiáº¿u tÃ´n trá»ng ngÆ°á»i Ä‘Ã£ khuáº¥t. TÃ³m táº¯t: \n",
      "\n",
      "TÃ³m táº¯t:\n",
      "ï»¿Táº¡p chÃ­ Vice cá»§a Má»¹  vá»«a bá»‹ lÃªn Ã¡n vÃ¬ thá»±c hiá»‡n bá»™ áº£nh \"thá»i trang tá»± tá»­\", tÃ¡i hiá»‡n láº¡i giÃ¢y phÃºt cuá»‘i Ä‘á»i cá»§a nhá»¯ng ná»¯ vÄƒn sÄ© ná»•i tiáº¿ng. Trong nhá»¯ng nÄƒm gáº§n dÃ¢y, cÃ¡c tá» táº¡p chÃ­ thÆ°á»nng máº¡nh dáº¡n Ä‘Æ°a ra nhá»¯ng Ã½ tÆ°á»Ÿng má»›i láº¡, nhiá»u khi trá»Ÿ thÃ nh quÃ¡ Ä‘Ã . Sau khi nhá»¯ng bá»©c hÃ¬nh tá»± tá»­ nÃ y bá»‹ xÃ³a Ä‘i, Vice Ä‘Ã£ Ä‘Äƒng táº£i lá»i xin lá»—i, bá»‹ cho lÃ  khÃ´ng thuyáº¿t phá»¥c. Khoa há»c Ä‘Ã£ chá»©ng minh, Ä‘á»‘i vá»›i nhá»¯ng ai tá»«ng xuáº¥t hiá»‡n trong Ä‘áº§u Ã½ nghÄ© Ä‘en tá»‘i vá» viá»‡c tá»± tá»­, viá»‡c bÃ¡o chÃ­ hoáº·c truyá»n hÃ¬nh Ä‘Æ°a ra nhá»¯ng hÃ¬nh áº£nh trá»±c quan vá» hÃ nh Ä‘á»™ng nÃ y khÃ´ng khÃ¡c gÃ¬ nhen nhÃ³m láº¡i Ã½ tÆ°á»Ÿng Ä‘Ã³ trong Ä‘áº§u ngÆ°á»i xem. CÃ¡ch tiáº¿p cáº­n Ä‘á» tÃ i  bá»‹ Ä‘Ã¡nh giÃ¡ lÃ  thiáº¿u tÃ´n trá»ng ngÆ°á»i Ä‘Ã£ khuáº¥t.Question:\n",
      "ï»¿Táº¡p chÃ­ Vice cá»§a Má»¹ vá»«a bá»‹ lÃªn Ã¡n vÃ¬ thá»±c hiá»‡n bá»™ áº£nh \"thá»i trang tá»± tá»­\", tÃ¡i hiá»‡n láº¡i giÃ¢y phÃºt cuá»‘i Ä‘á»i cá»§a nhá»¯ng ná»¯ vÄƒn sÄ© ná»•i tiáº¿ng. Trong nhá»¯ng nÄƒm gáº§n Ä‘Ã¢y, cÃ¡c tá» táº¡p chÃ­ thÆ°á»ng máº¡nh dáº¡n Ä‘Æ°a ra nhá»¯ng Ã½ tÆ°á»Ÿng má»›i láº¡, nhiá»u khi trá»Ÿ thÃ nh quÃ¡ Ä‘Ã . Sau khi nhá»¯ng bá»©c hÃ¬nh tá»± tá»­ nÃ y bá»‹ xÃ³a Ä‘i, Vice Ä‘Ã£ Ä‘Äƒng táº£i lá»i xin lá»—i, bá»‹ cho lÃ  khÃ´ng thuyáº¿t phá»¥c. Khoa há»c Ä‘Ã£ chá»©ng minh, Ä‘á»‘i vá»›i nhá»¯ng ai tá»«ng xuáº¥t hiá»‡n trong Ä‘áº§u Ã½ nghÄ© Ä‘en tá»‘i vá» viá»‡c tá»± tá»­, viá»‡c bÃ¡o chÃ­ hoáº·c\n",
      "\n",
      "--- Sample 3 ---\n",
      "[Prompt]    ### ÄÃ¢y lÃ  dáº¡ng tÃ³m táº¯t vÄƒn báº£n tin tá»©c vá»›i Ä‘á»™ dÃ i tÃ³m táº¯t Ä‘áº§u ra khoáº£ng 150 tá»«:  ### Lá»‡nh:\n",
      "Báº¡n lÃ  má»™t trá»£ lÃ½ tÃ³m táº¯t vÄƒn báº£n. HÃ£y cung cáº¥p báº£n tÃ³m táº¯t ngáº¯n gá»n vÃ  chÃ­nh xÃ¡c trong 150 chá»¯ cho bÃ i viáº¿t sau. BÃ i viáº¿t: ï»¿ 110 chÃº rÃ¹a con vá» vá»›i biá»ƒn\n",
      " Tá»‘i 21/7, táº¡i khu bÃ£i NgÃ²i thuá»™c thÃ´n Háº£i Giang, xÃ£ N...\n",
      "[Reference] ï»¿Tá»‘i 21/7, táº¡i khu bÃ£i NgÃ²i thuá»™c thÃ´n Háº­u Giang, xÃ£ NhÆ¡n Háº£i, TP Quy NhÆ¡n, hÆ¡n 100 chÃº rÃ¹a con Ä‘Æ°á»£c vá» vá»›i biá»ƒn sau 48 ngÃ y trong bá»c trá»©ng.\n",
      "TrÆ°á»›c Ä‘Ã³, khoáº£ng 20 giá» ngÃ y 21/7, trong lÃºc ra biá»ƒn hÃ³ng mÃ¡t, má»™t ngÆ°á»i dÃ¢n sá»‘ng gáº§n khu vá»±c nÃ y Ä‘Ã£ phÃ¡t hiá»‡n nhiá»u chÃº rÃ¹a con bÃ² hÆ°á»›ng ra biá»ƒn, bÃ¡o cho nhÃ³m tÃ¬nh nguyá»‡n viÃªn báº£o vá»‡ rÃ¹a biá»ƒn táº¡i thÃ´n Háº£i Giang. CÃ¡c thÃ nh viÃªn trong nhÃ³m Ä‘Ã£ Ä‘áº¿n khu vá»±c á»• trá»©ng Ä‘á»ƒ kiá»ƒm tra á»• trá»©ng, Ä‘o Ä‘áº¡c kÃ­ch thÆ°á»›c rÃ¹a con Ä‘á»ƒ tháº£ chÃºng vá» vá»›i Ä‘áº¡i dÆ°Æ¡ng. Giáº£i cá»©u nhá»¯ng chÃº rÃ¹a cÃ²n máº¯c ká»‡t chÆ°a ná»Ÿ Ä‘Æ°á»£c.\n",
      "[Generated] ### ÄÃ¢y lÃ  dáº¡ng tÃ³m táº¯t vÄƒn báº£n tin tá»©c vá»›i Ä‘á»™ dÃ i tÃ³m táº¯t Ä‘áº§u ra khoáº£ng 150 tá»«:  ### Lá»‡nh:\n",
      "Báº¡n lÃ  má»™t trá»£ lÃ½ tÃ³m táº¯t vÄƒn báº£n. HÃ£y cung cáº¥p báº£n tÃ³m táº¯t ngáº¯n gá»n vÃ  chÃ­nh xÃ¡c trong 150 chá»¯ cho bÃ i viáº¿t sau. BÃ i viáº¿t: ï»¿ 110 chÃº rÃ¹a con vá» vá»›i biá»ƒn\n",
      " Tá»‘i 21/7, táº¡i khu bÃ£i NgÃ²i thuá»™c thÃ´n Háº£i Giang, xÃ£ NhÆ¡n Háº£i, TP Quy NhÆ¡n, tá»‰nh BÃ¬nh Äá»‹nh, hÆ¡n 100 chÃº rÃ¹a con Ä‘Æ°á»£c vá» vá»›i biá»ƒn sau 48 ngÃ y trong bá»c trá»©ng. ÄÃ¢y lÃ  á»• trá»©ng rÃ¹a Ä‘áº» Ä‘áº§u tiÃªn trong nÄƒm 2013 táº¡i khu vá»±c thÃ´n Háº£i Giang.\n",
      "TrÆ°á»›c Ä‘Ã³, khoáº£ng 20 giá» ngÃ y 21/7, trong lÃºc ra biá»ƒn hÃ³ng mÃ¡t, má»™t ngÆ°á»i dÃ¢n sá»‘ng gáº§n khu vá»±c nÃ y Ä‘Ã£ phÃ¡t hiá»‡n nhiá»u chÃº rÃ¹a con bÃ² lon ton hÆ°á»›ng ra biá»ƒn, nÃªn liá»n bÃ¡o cho nhÃ³m tÃ¬nh nguyá»‡n viÃªn báº£o vá» rÃ¹a biá»ƒn táº¡i thÃ´n Háº£i Giang.\n",
      "Nháº­n Ä‘Æ°á»£c tin bÃ¡o, cÃ¡c thÃ nh viÃªn trong nhÃ³m Ä‘Ã£ Ä‘áº¿n khu vá»±c á»• trá»©ng Ä‘á»ƒ kiá»ƒm tra á»• trá»©ng, Ä‘o Ä‘áº¡c kÃ­ch thÆ°á»›c rÃ¹a con Ä‘á»ƒ tháº£ chÃºng vá» vá»›i Ä‘áº¡i dÆ°Æ¡ng. Äá»“ng thá»i, giáº£i cá»©u nhá»¯ng chÃº rÃ¹a cÃ²n máº¯c káº¹t chÆ°a ná»Ÿ Ä‘Æ°á»£c. \n",
      "Qua thá»‘ng kÃª, nhÃ³m tÃ¬nh nguyá»‡n báº£o vá»‡ rÃ¹a biá»ƒn xÃ£ NhÆ¡n Háº£i cho biáº¿t, cÃ³ 110 chÃº rÃ¹a con Ä‘Æ°á»£c ná»Ÿ thÃ nh cÃ´ng, trong tá»•ng sá»‘ 120 trá»©ng, bá»‹ hÆ° 10 trá»©ng. KÃ­ch thÆ°á»›c rÃ¹a con Ä‘o Ä‘Æ°á»£c, chiá»u dÃ i tÃ­nh tá»« Ä‘áº§u Ä‘áº¿n Ä‘uÃ´i khoáº£ng 8 cm, chiá»u ngang khoáº£ng 5 cm. \n",
      "Theo Ã”ng Nguyá»…n VÄƒn Minh â€“ TrÆ°á»Ÿng nhÃ³m tÃ¬nh nguyá»‡n viÃªn báº£o vá»‡ rÃ¹a biá»ƒn xÃ£ NhÆ¡n Háº£i, cho biáº¿t: \"á»” trá»©ng nÃ y Ä‘Ã£ Ä‘Æ°á»£c rÃ¹a máº¹ Ä‘áº» rá»“i á»§ trong thá»i gian 48 ngÃ y thÃ¬ ná»Ÿ thÃ nh rÃ¹a con. Sá»‘ lÆ°á»£ng trá»©ng ná»Ÿ nhÆ° váº­y thÃ¬ á»• trá»©ng rÃ¹a nÃ y Ä‘áº¡t tá»· lá»‡ cao. Tuy nhiÃªn, sá»‘ lÆ°á»£ng rÃ¹a con sau khi Ä‘Æ°á»£c vá» vá»›i biá»ƒn thÃ¬ nguy cÆ¡ sá»‘ng sÃ³t chá»‰ khoáº£ng tá»« 30 â€“ 40%. VÃ¬ rÃ¹a con thÆ°á»ng Ä‘á»‘i máº·t vá»›i nhá»¯ng loáº¡i cÃ¡ lá»›n sáº½ Äƒn thá»‹t chÃºng, rÃ¹a con sá»©c Ä‘á» khÃ¡ng yáº¿u cÅ©ng cÃ³ thá»ƒ bá»‹ cháº¿t vÃ  bá»‹ sÃ³ng cuá»‘n trÃ´i...\"\n",
      "TrÆ°á»›c Ä‘Ã³, ráº¡ng sÃ¡ng ngÃ y 3/6, táº¡i khu vá»±c bÃ£i NgÃ²i (thÃ´n Háº£i Giang, xÃ£ NhÆ¡n Háº£i, TP Quy NhÆ¡n, BÃ¬nh Äá»‹nh) má»™t cÃ¡ thá»ƒ rÃ¹a máº¹ cÃ³ chiá»u dÃ i khoáº£ng 1,2m, chiá»u ngang 0,9m bÃ² lÃªn bá» cÃ¡ch mÃ©p nÆ°á»›c biá»ƒn trÃªn 30m Ä‘áº» má»™t á»• trá»©ng vá»›i sá»‘ lÆ°á»£ng 120 trá»©ng. \n",
      "Tá»« Ä‘áº§u nÄƒm Ä‘áº¿n nay, cÃ³ 4 láº§n rÃ¹a máº¹ bÃ² lÃªn bá» biá»ƒn thÃ´n Háº£i Giang, xÃ£ NhÆ¡n Háº£i Ä‘áº» trá»©ng vá»›i sá»‘ lÆ°á»£ng trÃªn 500 trá»©ng. VÃ  Ä‘Ã¢y á»• trá»©ng Ä‘áº§u tiÃªn Ä‘Ã£ ná»Ÿ 110 rÃ¹a con.\n",
      "ÄÆ°á»£c biáº¿t, xÃ£ NhÆ¡n Háº£i cÃ³ nhiá»u ráº¡ng san hÃ´ vÃ  gÃ nh Ä‘Ã¡, mÃ´i trÆ°á»ng biá»ƒn trong lÃ nh lÃ  nÆ¡i rÃ¹a biá»ƒn thÆ°á»ng xuáº¥t hiá»‡n lÃªn bá» Ä‘áº» trá»©ng táº¡i khu vá»±c Ä‘áº£o HÃ²n KhÃ´ lá»›n (ThÃ´n Háº£i ÄÃ´ng, xÃ£ NhÆ¡n Háº£i ). \n",
      "RÃ¹a biá»ƒn lÃ  má»™t trong nhá»¯ng loÃ i Ä‘á»™ng váº­t quÃ½ hiáº¿m náº±m trong sÃ¡ch Ä‘á» Viá»‡t Nam cÃ³ nguy cÆ¡ bá»‹ tuyá»‡t chá»§ng nÃªn hiá»‡n Ä‘ang Ä‘Æ°á»£c báº»o vá»‡ nghiÃªm ngáº·t. TÃ³m táº¯t: \n",
      "\n",
      "TÃ³m táº¯t:\n",
      "ï»¿Tá»‘i 21/7, táº¡i khu bÃ£i NgÃ²i thuá»™c thÃ´n Háº­u Giang, xÃ£ NhÆ¡n Háº£i, TP Quy NhÆ¡n, hÆ¡n 100 chÃº rÃ¹a con Ä‘Æ°á»£c vá» vá»›i biá»ƒn sau 48 ngÃ y trong bá»c trá»©ng.\n",
      "TrÆ°á»›c Ä‘Ã³, khoáº£ng 20 giá» ngÃ y 21/7, trong lÃºc ra biá»ƒn hÃ³ng mÃ¡t, má»™t ngÆ°á»i dÃ¢n sá»‘ng gáº§n khu vá»±c nÃ y Ä‘Ã£ phÃ¡t hiá»‡n nhiá»u chÃº rÃ¹a con bÃ² hÆ°á»›ng ra biá»ƒn, bÃ¡o cho nhÃ³m tÃ¬nh nguyá»‡n viÃªn báº£o vá»‡ rÃ¹a biá»ƒn táº¡i thÃ´n Háº£i Giang. CÃ¡c thÃ nh viÃªn trong nhÃ³m Ä‘Ã£ Ä‘áº¿n khu vá»±c á»• trá»©ng Ä‘á»ƒ kiá»ƒm tra á»• trá»©ng, Ä‘o Ä‘áº¡c kÃ­ch thÆ°á»›c rÃ¹a con Ä‘á»ƒ tháº£ chÃºng vá» vá»›i Ä‘áº¡i dÆ°Æ¡ng. Giáº£i cá»©u nhá»¯ng chÃº rÃ¹a cÃ²n máº¯c ká»‡t chÆ°a ná»Ÿ Ä‘Æ°á»£c.Question: ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿ï»¿\n"
     ]
    }
   ],
   "source": [
    "evaluate_after_training(model, val_loader, tokenizer, device)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2627880,
     "sourceId": 4701735,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6984301,
     "sourceId": 11188105,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6959873,
     "sourceId": 11188231,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 356905,
     "modelInstanceId": 335897,
     "sourceId": 411412,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
