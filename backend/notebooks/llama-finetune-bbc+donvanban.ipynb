{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-26T15:38:19.070778Z",
     "iopub.status.busy": "2025-05-26T15:38:19.070466Z",
     "iopub.status.idle": "2025-05-26T15:38:29.847686Z",
     "shell.execute_reply": "2025-05-26T15:38:29.846553Z",
     "shell.execute_reply.started": "2025-05-26T15:38:19.070753Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge_score) (2024.2.0)\n",
      "Requirement already satisfied: bert_score in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.5.1+cu121)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.2.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.47.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.26.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.5)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2025.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.29.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bert_score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bert_score) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->bert_score) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->bert_score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->bert_score) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate peft datasets bitsandbytes evaluate -q\n",
    "!pip install rouge_score\n",
    "!pip install bert_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import library and load module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:38:29.849490Z",
     "iopub.status.busy": "2025-05-26T15:38:29.849120Z",
     "iopub.status.idle": "2025-05-26T15:38:49.456491Z",
     "shell.execute_reply": "2025-05-26T15:38:49.455581Z",
     "shell.execute_reply.started": "2025-05-26T15:38:29.849464Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import json\n",
    "import shutil\n",
    "from IPython.display import FileLink\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    default_data_collator,\n",
    ")\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:38:49.458768Z",
     "iopub.status.busy": "2025-05-26T15:38:49.458095Z",
     "iopub.status.idle": "2025-05-26T15:38:49.596473Z",
     "shell.execute_reply": "2025-05-26T15:38:49.595560Z",
     "shell.execute_reply.started": "2025-05-26T15:38:49.458744Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:38:49.598014Z",
     "iopub.status.busy": "2025-05-26T15:38:49.597645Z",
     "iopub.status.idle": "2025-05-26T15:38:49.601831Z",
     "shell.execute_reply": "2025-05-26T15:38:49.601065Z",
     "shell.execute_reply.started": "2025-05-26T15:38:49.597990Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:38:49.603042Z",
     "iopub.status.busy": "2025-05-26T15:38:49.602742Z",
     "iopub.status.idle": "2025-05-26T15:38:49.622647Z",
     "shell.execute_reply": "2025-05-26T15:38:49.621721Z",
     "shell.execute_reply.started": "2025-05-26T15:38:49.603012Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:38:49.623880Z",
     "iopub.status.busy": "2025-05-26T15:38:49.623590Z",
     "iopub.status.idle": "2025-05-26T15:39:08.067964Z",
     "shell.execute_reply": "2025-05-26T15:39:08.067027Z",
     "shell.execute_reply.started": "2025-05-26T15:38:49.623850Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399231b60ca243fcb9540851f0b7b253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0986c8320d764846ab3deba7e5551a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26081b4132d6419a8aeba576c44c1f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d278207c8941899394c0ee7b795bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d0f09cb80f45db963d8c2697000f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c75f2ff0ef4454a4a317d186269a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# Thiết lập pad_token và padding side\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:39:08.069226Z",
     "iopub.status.busy": "2025-05-26T15:39:08.068903Z",
     "iopub.status.idle": "2025-05-26T15:39:08.074047Z",
     "shell.execute_reply": "2025-05-26T15:39:08.073255Z",
     "shell.execute_reply.started": "2025-05-26T15:39:08.069192Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    attention_masks = [item[\"attention_mask\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_masks,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:39:08.076760Z",
     "iopub.status.busy": "2025-05-26T15:39:08.076536Z",
     "iopub.status.idle": "2025-05-26T15:39:13.321642Z",
     "shell.execute_reply": "2025-05-26T15:39:13.320792Z",
     "shell.execute_reply.started": "2025-05-26T15:39:08.076741Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Continue Pretrain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:02:51.782934Z",
     "iopub.status.busy": "2025-05-25T03:02:51.782613Z",
     "iopub.status.idle": "2025-05-25T03:02:54.889569Z",
     "shell.execute_reply": "2025-05-25T03:02:54.888919Z",
     "shell.execute_reply.started": "2025-05-25T03:02:51.782904Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab96146c2024a1b8855d8c16e9c3b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/635 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d935df233d17414bb79a1183eba11d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/35.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4b1ff0664844eb932b08699f7a635d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/4.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c73f47f1d014b5880278e9d5623f2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/4.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319482b65aa44d1594d7fe4c1e76b86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/12806 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4548d22c245c41919618d4a62cab75f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca939bd27d244c088575c7b80bb8b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"vukhai248/vietnamese_news_16k\", split=\"train\")\n",
    "dataset = dataset.select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:02:54.891173Z",
     "iopub.status.busy": "2025-05-25T03:02:54.890933Z",
     "iopub.status.idle": "2025-05-25T03:02:54.894619Z",
     "shell.execute_reply": "2025-05-25T03:02:54.893717Z",
     "shell.execute_reply.started": "2025-05-25T03:02:54.891150Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    return text.replace(\"\\n\", \" \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:02:54.895648Z",
     "iopub.status.busy": "2025-05-25T03:02:54.895450Z",
     "iopub.status.idle": "2025-05-25T03:03:04.441895Z",
     "shell.execute_reply": "2025-05-25T03:03:04.440976Z",
     "shell.execute_reply.started": "2025-05-25T03:02:54.895632Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_fn(text, max_length=2048):\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "    attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "    labels = input_ids.clone()\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:03:04.443066Z",
     "iopub.status.busy": "2025-05-25T03:03:04.442775Z",
     "iopub.status.idle": "2025-05-25T03:03:10.608088Z",
     "shell.execute_reply": "2025-05-25T03:03:10.607451Z",
     "shell.execute_reply.started": "2025-05-25T03:03:04.443043Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "texts = dataset[\"content\"]\n",
    "tokenized_data = [tokenize_fn(text) for text in texts]\n",
    "train_loader = DataLoader(\n",
    "    tokenized_data,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:03:10.609190Z",
     "iopub.status.busy": "2025-05-25T03:03:10.608887Z",
     "iopub.status.idle": "2025-05-25T10:51:54.609669Z",
     "shell.execute_reply": "2025-05-25T10:51:54.608737Z",
     "shell.execute_reply.started": "2025-05-25T03:03:10.609162Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 1/3: 100%|██████████| 1000/1000 [2:35:22<00:00,  9.32s/it, loss=2.72] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Loss: 2.3339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1000/1000 [2:37:08<00:00,  9.43s/it, loss=2.06] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Loss: 2.2864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1000/1000 [2:36:12<00:00,  9.37s/it, loss=2.25] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Loss: 2.2713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, device, n_epochs=3, lr=1e-5):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\", leave=True)\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            # Forward pass with checkpointing\n",
    "            def forward_with_checkpoint(*args, **kwargs):\n",
    "                return torch.utils.checkpoint.checkpoint(\n",
    "                    lambda *args, **kwargs: model(*args, **kwargs),\n",
    "                    *args,\n",
    "                    **kwargs,\n",
    "                    use_reentrant=False\n",
    "                )\n",
    "\n",
    "            outputs = forward_with_checkpoint(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} - Loss: {avg_loss:.4f}\")\n",
    "train(model, train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T10:51:54.611200Z",
     "iopub.status.busy": "2025-05-25T10:51:54.610857Z",
     "iopub.status.idle": "2025-05-25T10:51:55.038424Z",
     "shell.execute_reply": "2025-05-25T10:51:55.037648Z",
     "shell.execute_reply.started": "2025-05-25T10:51:54.611169Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pretrained model saved at: /kaggle/working/llama3-qlora-continued-pretrain\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-continued-pretrain\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"✅ Pretrained model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T10:51:55.039477Z",
     "iopub.status.busy": "2025-05-25T10:51:55.039197Z",
     "iopub.status.idle": "2025-05-25T10:51:55.626208Z",
     "shell.execute_reply": "2025-05-25T10:51:55.625554Z",
     "shell.execute_reply.started": "2025-05-25T10:51:55.039447Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='llama3-qlora-continued-pretrain.zip' target='_blank'>llama3-qlora-continued-pretrain.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/llama3-qlora-continued-pretrain.zip"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Đường dẫn tới thư mục chứa mô hình\n",
    "model_dir = '/kaggle/working/llama3-qlora-continued-pretrain'\n",
    "\n",
    "# Tạo file nén .zip từ thư mục\n",
    "zip_name = 'llama3-qlora-continued-pretrain'  # Tên file nén (không cần đuôi .zip)\n",
    "shutil.make_archive(zip_name, 'zip', model_dir)\n",
    "\n",
    "# Tạo liên kết tải xuống cho file .zip\n",
    "display(FileLink(f'{zip_name}.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finetune with QA dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:20:37.520991Z",
     "iopub.status.busy": "2025-05-25T17:20:37.520658Z",
     "iopub.status.idle": "2025-05-25T17:20:37.524611Z",
     "shell.execute_reply": "2025-05-25T17:20:37.523676Z",
     "shell.execute_reply.started": "2025-05-25T17:20:37.520964Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"/kaggle/input/llama-continued-pretrain-model/transformers/default/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:06.686944Z",
     "iopub.status.busy": "2025-05-25T11:06:06.686643Z",
     "iopub.status.idle": "2025-05-25T11:06:10.813181Z",
     "shell.execute_reply": "2025-05-25T11:06:10.812250Z",
     "shell.execute_reply.started": "2025-05-25T11:06:06.686922Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/vietnamese-squad/train-v2.0-translated.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    vi_squad_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:10.814439Z",
     "iopub.status.busy": "2025-05-25T11:06:10.814139Z",
     "iopub.status.idle": "2025-05-25T11:06:10.819212Z",
     "shell.execute_reply": "2025-05-25T11:06:10.818379Z",
     "shell.execute_reply.started": "2025-05-25T11:06:10.814410Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Train samples: 10000\n",
      "📦 Validation samples: 2000\n"
     ]
    }
   ],
   "source": [
    "train_data = vi_squad_data[:10000]\n",
    "\n",
    "# Lấy 2k mẫu tiếp theo cho validation\n",
    "val_data = vi_squad_data[10000:12000]\n",
    "\n",
    "print(f\"📦 Train samples: {len(train_data)}\")\n",
    "print(f\"📦 Validation samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:10.820518Z",
     "iopub.status.busy": "2025-05-25T11:06:10.820173Z",
     "iopub.status.idle": "2025-05-25T11:06:15.870562Z",
     "shell.execute_reply": "2025-05-25T11:06:15.869725Z",
     "shell.execute_reply.started": "2025-05-25T11:06:10.820470Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Train samples: ['Beyoncé Giselle Knowles-Carter (/ b i gì ɒ n s eɪ / bee-YON-say) (sinh ngày 04 tháng 9 1981) là một ca sĩ, nhạc sĩ, nhà sản xuất thu âm và nữ diễn viên người Mỹ. Sinh ra và lớn lên ở Houston, Texas, cô đã biểu diễn trong các cuộc thi ca hát và nhảy múa khác nhau khi còn nhỏ, và nổi tiếng vào cuối những năm 1990 với tư cách là ca sĩ chính của nhóm nhạc nữ R & B Destiny\\'s Child. Được quản lý bởi cha cô, Mathew Knowles, nhóm đã trở thành một trong những nhóm nhạc nữ bán chạy nhất thế giới mọi thời đại. Sự gián đoạn của họ đã chứng kiến việc phát hành album đầu tay của Beyoncé, Dangerously in Love (2003), giúp cô trở thành một nghệ sĩ solo trên toàn thế giới, giành được năm giải Grammy và có đĩa đơn quán quân Billboard Hot 100 \"Crazy in Love\" và \"Baby Boy\".', 'Beyonce bắt đầu nổi tiếng từ khi nào?', 'Vào cuối những năm 1990']\n"
     ]
    }
   ],
   "source": [
    "print(f\"📦 Train samples: {train_data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:15.871686Z",
     "iopub.status.busy": "2025-05-25T11:06:15.871441Z",
     "iopub.status.idle": "2025-05-25T11:06:16.056357Z",
     "shell.execute_reply": "2025-05-25T11:06:16.055708Z",
     "shell.execute_reply.started": "2025-05-25T11:06:15.871666Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_list([{\"context\": item[0], \"question\": item[1], \"answer\": item[2]} for item in train_data])\n",
    "val_dataset = Dataset.from_list([{\"context\": item[0], \"question\": item[1], \"answer\": item[2]} for item in val_data])\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": val_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:16.057228Z",
     "iopub.status.busy": "2025-05-25T11:06:16.057027Z",
     "iopub.status.idle": "2025-05-25T11:06:16.067113Z",
     "shell.execute_reply": "2025-05-25T11:06:16.066179Z",
     "shell.execute_reply.started": "2025-05-25T11:06:16.057210Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. {'context': 'Beyoncé Giselle Knowles-Carter (/ b i gì ɒ n s eɪ / bee-YON-say) (sinh ngày 04 tháng 9 1981) là một ca sĩ, nhạc sĩ, nhà sản xuất thu âm và nữ diễn viên người Mỹ. Sinh ra và lớn lên ở Houston, Texas, cô đã biểu diễn trong các cuộc thi ca hát và nhảy múa khác nhau khi còn nhỏ, và nổi tiếng vào cuối những năm 1990 với tư cách là ca sĩ chính của nhóm nhạc nữ R & B Destiny\\'s Child. Được quản lý bởi cha cô, Mathew Knowles, nhóm đã trở thành một trong những nhóm nhạc nữ bán chạy nhất thế giới mọi thời đại. Sự gián đoạn của họ đã chứng kiến việc phát hành album đầu tay của Beyoncé, Dangerously in Love (2003), giúp cô trở thành một nghệ sĩ solo trên toàn thế giới, giành được năm giải Grammy và có đĩa đơn quán quân Billboard Hot 100 \"Crazy in Love\" và \"Baby Boy\".', 'question': 'Beyonce bắt đầu nổi tiếng từ khi nào?', 'answer': 'Vào cuối những năm 1990'}\n",
      "2. Beyoncé Giselle Knowles-Carter (/ b i gì ɒ n s eɪ / bee-YON-say) (sinh ngày 04 tháng 9 1981) là một ca sĩ, nhạc sĩ, nhà sản xuất thu âm và nữ diễn viên người Mỹ. Sinh ra và lớn lên ở Houston, Texas, cô đã biểu diễn trong các cuộc thi ca hát và nhảy múa khác nhau khi còn nhỏ, và nổi tiếng vào cuối những năm 1990 với tư cách là ca sĩ chính của nhóm nhạc nữ R & B Destiny's Child. Được quản lý bởi cha cô, Mathew Knowles, nhóm đã trở thành một trong những nhóm nhạc nữ bán chạy nhất thế giới mọi thời đại. Sự gián đoạn của họ đã chứng kiến việc phát hành album đầu tay của Beyoncé, Dangerously in Love (2003), giúp cô trở thành một nghệ sĩ solo trên toàn thế giới, giành được năm giải Grammy và có đĩa đơn quán quân Billboard Hot 100 \"Crazy in Love\" và \"Baby Boy\".\n",
      "3. Beyonce bắt đầu nổi tiếng từ khi nào?\n",
      "4. Vào cuối những năm 1990\n"
     ]
    }
   ],
   "source": [
    "print(f\"1. {dataset_dict['train'][0]}\")\n",
    "print(f\"2. {dataset_dict['train'][0]['context']}\")\n",
    "print(f\"3. {dataset_dict['train'][0]['question']}\")\n",
    "print(f\"4. {dataset_dict['train'][0]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:16.068330Z",
     "iopub.status.busy": "2025-05-25T11:06:16.068070Z",
     "iopub.status.idle": "2025-05-25T11:06:33.217119Z",
     "shell.execute_reply": "2025-05-25T11:06:33.216196Z",
     "shell.execute_reply.started": "2025-05-25T11:06:16.068298Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27c694acfcc45e48aef08fa794fe98a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742c0882c4a8464787c11b82c639a0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_qa(example, max_length=512):\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"]\n",
    "    prompt = f\"### Đây là dạng câu hỏi và trả lời dựa trên nội dung ###\\n\\nCâu hỏi: {question}\\n\\nNội dung: {context}\\n\\nTrả lời:\"\n",
    "    completion = f\" {answer}\"\n",
    "    \n",
    "    prompt_ids = tokenizer(prompt, add_special_tokens=False).input_ids\n",
    "    completion_ids = tokenizer(completion, add_special_tokens=False).input_ids\n",
    "\n",
    "    input_ids = prompt_ids + completion_ids\n",
    "\n",
    "    labels = [-100] * len(prompt_ids) + completion_ids\n",
    "\n",
    "    if len(input_ids) > max_length:\n",
    "        input_ids = input_ids[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    \n",
    "    input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "    attention_mask = attention_mask + [0] * padding_length\n",
    "    labels = labels + [-100] * padding_length\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids),\n",
    "        \"attention_mask\": torch.tensor(attention_mask),\n",
    "        \"labels\": torch.tensor(labels)\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset_dict.map(preprocess_qa)\n",
    "\n",
    "tokenized_dataset.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:33.218537Z",
     "iopub.status.busy": "2025-05-25T11:06:33.218201Z",
     "iopub.status.idle": "2025-05-25T11:06:33.223084Z",
     "shell.execute_reply": "2025-05-25T11:06:33.222309Z",
     "shell.execute_reply.started": "2025-05-25T11:06:33.218500Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"], batch_size=4, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_dataset[\"test\"], batch_size=4, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-25T17:18:25.136Z",
     "iopub.execute_input": "2025-05-25T11:06:33.224338Z",
     "iopub.status.busy": "2025-05-25T11:06:33.224038Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:   0%|          | 0/2500 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 1/2: 100%|██████████| 2500/2500 [3:22:25<00:00,  4.86s/it, loss=1.42]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1/2 - Train Loss: 1.7102\n",
      "📊 Validation Loss: 1.3921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  25%|██▍       | 621/2500 [50:22<2:32:00,  4.85s/it, loss=2.36] "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "n_epochs = 2\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\", leave=True)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"✅ Epoch {epoch+1}/{n_epochs} - Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"📊 Validation Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-25T17:18:25.136Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-qa-finetune\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"✅ Pretrained model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-25T17:18:25.137Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_dir = '/kaggle/working/llama3-qlora-qa-finetune'\n",
    "\n",
    "# Tạo file nén .zip từ thư mục\n",
    "zip_name = 'llama3-qlora-qa-finetune'  # Tên file nén (không cần đuôi .zip)\n",
    "shutil.make_archive(zip_name, 'zip', model_dir)\n",
    "\n",
    "# Tạo liên kết tải xuống cho file .zip\n",
    "display(FileLink(f'{zip_name}.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finetune with BBC News**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:39:13.323315Z",
     "iopub.status.busy": "2025-05-26T15:39:13.323026Z",
     "iopub.status.idle": "2025-05-26T15:39:13.328052Z",
     "shell.execute_reply": "2025-05-26T15:39:13.327436Z",
     "shell.execute_reply.started": "2025-05-26T15:39:13.323281Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:39:13.328975Z",
     "iopub.status.busy": "2025-05-26T15:39:13.328782Z",
     "iopub.status.idle": "2025-05-26T15:39:13.896387Z",
     "shell.execute_reply": "2025-05-26T15:39:13.895524Z",
     "shell.execute_reply.started": "2025-05-26T15:39:13.328958Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88bb685b1214b19b5cdd681cab3399b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = \"/kaggle/input/data-llama-finetune/bbc_data_llama_finetune.json\"\n",
    "\n",
    "full_dataset = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
    "dataset = full_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:39:13.897458Z",
     "iopub.status.busy": "2025-05-26T15:39:13.897217Z",
     "iopub.status.idle": "2025-05-26T15:39:20.636998Z",
     "shell.execute_reply": "2025-05-26T15:39:20.635982Z",
     "shell.execute_reply.started": "2025-05-26T15:39:13.897438Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115ce413698243d8af1e18312877e268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1914 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333f321300bd4fbeb41a52f57fccef1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/213 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(example, max_length=2048):\n",
    "    prompt = f\"### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Lệnh:\\n{example['prompt']}\\n\\nTóm tắt:\\n\"\n",
    "    summary = example[\"summary\"]\n",
    "    prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    summary_ids = tokenizer.encode(summary, add_special_tokens=False)\n",
    "    \n",
    "    # Kiểm tra tổng số token\n",
    "    total_length = len(prompt_ids) + len(summary_ids)\n",
    "    if total_length > max_length:\n",
    "        overflow = total_length - max_length\n",
    "        if overflow < len(prompt_ids):\n",
    "            prompt_ids = prompt_ids[:-overflow]\n",
    "        else:\n",
    "            prompt_ids = []\n",
    "    input_ids = prompt_ids + summary_ids\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    labels = [-100] * len(prompt_ids) + summary_ids\n",
    "\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "        attention_mask = attention_mask + [0] * padding_length\n",
    "        labels = labels + [-100] * padding_length\n",
    "    else:\n",
    "        input_ids = input_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "tokenized_dataset = dataset.map(tokenize, batched=False, fn_kwargs={\"max_length\": 2048})\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "collator = default_data_collator\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_dataset[\"test\"],\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:39:20.637991Z",
     "iopub.status.busy": "2025-05-26T15:39:20.637755Z",
     "iopub.status.idle": "2025-05-26T15:39:20.655378Z",
     "shell.execute_reply": "2025-05-26T15:39:20.654495Z",
     "shell.execute_reply.started": "2025-05-26T15:39:20.637970Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:39:20.656326Z",
     "iopub.status.busy": "2025-05-26T15:39:20.656123Z",
     "iopub.status.idle": "2025-05-27T00:41:23.755923Z",
     "shell.execute_reply": "2025-05-27T00:41:23.754971Z",
     "shell.execute_reply.started": "2025-05-26T15:39:20.656308Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1281: 100%|██████████| 957/957 [3:00:37<00:00, 11.32s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1 completed — Avg loss: 0.1809\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1309: 100%|██████████| 957/957 [3:00:42<00:00, 11.33s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2 completed — Avg loss: 0.1512\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1048: 100%|██████████| 957/957 [3:00:42<00:00, 11.33s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 3 completed — Avg loss: 0.1442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in pbar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"✅ Epoch {epoch+1} completed — Avg loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T00:41:23.757165Z",
     "iopub.status.busy": "2025-05-27T00:41:23.756908Z",
     "iopub.status.idle": "2025-05-27T00:41:24.288814Z",
     "shell.execute_reply": "2025-05-27T00:41:24.287945Z",
     "shell.execute_reply.started": "2025-05-27T00:41:23.757145Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finetuned model saved at: /kaggle/working/llama3-qlora-bbc-finetuned\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-bbc-finetuned\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"✅ Finetuned model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T00:41:24.289881Z",
     "iopub.status.busy": "2025-05-27T00:41:24.289612Z",
     "iopub.status.idle": "2025-05-27T00:41:24.897396Z",
     "shell.execute_reply": "2025-05-27T00:41:24.896710Z",
     "shell.execute_reply.started": "2025-05-27T00:41:24.289847Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='llama3-qlora-qa-finetune.zip' target='_blank'>llama3-qlora-qa-finetune.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/llama3-qlora-qa-finetune.zip"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_dir = '/kaggle/working/llama3-qlora-bbc-finetuned'\n",
    "\n",
    "# Tạo file nén .zip từ thư mục\n",
    "zip_name = 'llama3-qlora-qa-finetune'  # Tên file nén (không cần đuôi .zip)\n",
    "shutil.make_archive(zip_name, 'zip', model_dir)\n",
    "\n",
    "# Tạo liên kết tải xuống cho file .zip\n",
    "display(FileLink(f'{zip_name}.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finetune with donvanban**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-25T10:52:16.026572Z",
     "iopub.status.idle": "2025-05-25T10:52:16.026934Z",
     "shell.execute_reply": "2025-05-25T10:52:16.026776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#model_id = \"/kaggle/input/llama_finetune_v1.3/transformers/default/1/data_llama_finetune_v1.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T00:41:24.898455Z",
     "iopub.status.busy": "2025-05-27T00:41:24.898168Z",
     "iopub.status.idle": "2025-05-27T00:41:25.201868Z",
     "shell.execute_reply": "2025-05-27T00:41:25.201238Z",
     "shell.execute_reply.started": "2025-05-27T00:41:24.898422Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f687f457454a2caf346ea5c58f9d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = \"/kaggle/input/data-llama-finetune/vi_data_llama_finetune.json\"\n",
    "\n",
    "full_dataset = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
    "dataset = full_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T00:41:25.203012Z",
     "iopub.status.busy": "2025-05-27T00:41:25.202745Z",
     "iopub.status.idle": "2025-05-27T00:41:26.182946Z",
     "shell.execute_reply": "2025-05-27T00:41:26.182110Z",
     "shell.execute_reply.started": "2025-05-27T00:41:25.202991Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e61aaab1f3e4a3da52f3df046a5b275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266478fa6ee545a0a9a7a5f3b26476d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(example, max_length=2048):\n",
    "    prompt = f\"### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Lệnh:\\n{example['prompt']}\\n\\nTóm tắt:\\n\"\n",
    "    summary = example[\"summary\"]\n",
    "    \n",
    "    # Mã hóa riêng prompt và summary (không thêm special tokens)\n",
    "    prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    summary_ids = tokenizer.encode(summary, add_special_tokens=False)\n",
    "    \n",
    "    # Kiểm tra tổng số token\n",
    "    total_length = len(prompt_ids) + len(summary_ids)\n",
    "    if total_length > max_length:\n",
    "        overflow = total_length - max_length\n",
    "        # Ưu tiên giữ lại phần summary; cắt bớt prompt\n",
    "        if overflow < len(prompt_ids):\n",
    "            prompt_ids = prompt_ids[:-overflow]\n",
    "        else:\n",
    "            prompt_ids = []  # Nếu quá tràn, bỏ hết prompt\n",
    "    \n",
    "    # Nối prompt và summary\n",
    "    input_ids = prompt_ids + summary_ids\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    \n",
    "    # Tạo labels: phần prompt được mask bằng -100, phần summary giữ nguyên token IDs\n",
    "    labels = [-100] * len(prompt_ids) + summary_ids\n",
    "    \n",
    "    # Padding tất cả các trường về độ dài max_length\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "        attention_mask = attention_mask + [0] * padding_length\n",
    "        labels = labels + [-100] * padding_length\n",
    "    else:\n",
    "        # Nếu quá dài, cắt bớt (nên không xảy ra nhờ truncation ở trên)\n",
    "        input_ids = input_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "tokenized_dataset = dataset.map(tokenize, batched=False, fn_kwargs={\"max_length\": 2048})\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "collator = default_data_collator\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_dataset[\"test\"],\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T00:41:26.184223Z",
     "iopub.status.busy": "2025-05-27T00:41:26.183906Z",
     "iopub.status.idle": "2025-05-27T00:41:26.198919Z",
     "shell.execute_reply": "2025-05-27T00:41:26.198156Z",
     "shell.execute_reply.started": "2025-05-27T00:41:26.184169Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T00:41:26.199967Z",
     "iopub.status.busy": "2025-05-27T00:41:26.199687Z",
     "iopub.status.idle": "2025-05-27T01:22:51.759528Z",
     "shell.execute_reply": "2025-05-27T01:22:51.758659Z",
     "shell.execute_reply.started": "2025-05-27T00:41:26.199938Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4449: 100%|██████████| 72/72 [13:48<00:00, 11.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1 completed — Avg loss: 0.5622\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.0764: 100%|██████████| 72/72 [13:47<00:00, 11.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2 completed — Avg loss: 0.4837\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1682: 100%|██████████| 72/72 [13:49<00:00, 11.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 3 completed — Avg loss: 0.4461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in pbar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"✅ Epoch {epoch+1} completed — Avg loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T01:22:51.760653Z",
     "iopub.status.busy": "2025-05-27T01:22:51.760401Z",
     "iopub.status.idle": "2025-05-27T01:22:52.209373Z",
     "shell.execute_reply": "2025-05-27T01:22:52.208477Z",
     "shell.execute_reply.started": "2025-05-27T01:22:51.760632Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finetuned model saved at: /kaggle/working/llama3-qlora-finetuned-all\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-finetuned-all\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"✅ Finetuned model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T01:22:52.210577Z",
     "iopub.status.busy": "2025-05-27T01:22:52.210295Z",
     "iopub.status.idle": "2025-05-27T01:22:52.799026Z",
     "shell.execute_reply": "2025-05-27T01:22:52.798245Z",
     "shell.execute_reply.started": "2025-05-27T01:22:52.210546Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='llama3-qlora-donvanban-finetune.zip' target='_blank'>llama3-qlora-donvanban-finetune.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/llama3-qlora-donvanban-finetune.zip"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_dir = '/kaggle/working/llama3-qlora-finetuned-all'\n",
    "\n",
    "# Tạo file nén .zip từ thư mục\n",
    "zip_name = 'llama3-qlora-donvanban-finetune'  # Tên file nén (không cần đuôi .zip)\n",
    "shutil.make_archive(zip_name, 'zip', model_dir)\n",
    "\n",
    "# Tạo liên kết tải xuống cho file .zip\n",
    "display(FileLink(f'{zip_name}.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T01:22:52.800275Z",
     "iopub.status.busy": "2025-05-27T01:22:52.799956Z",
     "iopub.status.idle": "2025-05-27T01:22:52.808886Z",
     "shell.execute_reply": "2025-05-27T01:22:52.807995Z",
     "shell.execute_reply.started": "2025-05-27T01:22:52.800244Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_after_training(model, val_loader, tokenizer, device, num_samples=3):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    samples_to_print = []\n",
    "\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bert_score = evaluate.load(\"bertscore\")\n",
    "\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=128,\n",
    "                num_beams=4,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "\n",
    "            generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            label_texts = []\n",
    "            for label in labels:\n",
    "                label_ids = [token_id for token_id in label.tolist() if token_id != -100]\n",
    "                label_texts.append(tokenizer.decode(label_ids, skip_special_tokens=True))\n",
    "\n",
    "            predictions.extend(generated_texts)\n",
    "            references.extend(label_texts)\n",
    "\n",
    "            if len(samples_to_print) < num_samples:\n",
    "                for src, ref, pred in zip(tokenizer.batch_decode(input_ids, skip_special_tokens=True), label_texts, generated_texts):\n",
    "                    samples_to_print.append((src, ref, pred))\n",
    "                    if len(samples_to_print) >= num_samples:\n",
    "                        break\n",
    "\n",
    "    # ROUGE\n",
    "    rouge_results = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "\n",
    "    # BERTScore (tiếng Việt)\n",
    "    bert_results = bert_score.compute(predictions=predictions, references=references, lang=\"vi\")\n",
    "    bert_f1 = np.mean(bert_results[\"f1\"])\n",
    "\n",
    "    # In kết quả\n",
    "    print(\"\\n📊 ROUGE Scores:\")\n",
    "    for key in rouge_results:\n",
    "        print(f\"{key}: {rouge_results[key]:.4f}\")\n",
    "\n",
    "    print(f\"\\n📈 BERTScore F1: {bert_f1:.4f}\")\n",
    "\n",
    "    print(\"\\n📝 Sample Results:\")\n",
    "    for i, (src, ref, pred) in enumerate(samples_to_print):\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        print(f\"[Prompt]    {src[:300]}...\")  # rút gọn prompt nếu quá dài\n",
    "        print(f\"[Reference] {ref}\")\n",
    "        print(f\"[Generated] {pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T01:22:52.811836Z",
     "iopub.status.busy": "2025-05-27T01:22:52.811637Z",
     "iopub.status.idle": "2025-05-27T01:28:24.534887Z",
     "shell.execute_reply": "2025-05-27T01:28:24.533953Z",
     "shell.execute_reply.started": "2025-05-27T01:22:52.811818Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d20569d36047dcb011afbbe6bb4f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6fcd2dbc2d64009815db6aa2b2d39e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/9 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  11%|█         | 1/9 [00:36<04:55, 36.89s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 2/9 [01:13<04:17, 36.76s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 3/9 [01:50<03:40, 36.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  44%|████▍     | 4/9 [02:26<03:03, 36.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  56%|█████▌    | 5/9 [03:03<02:26, 36.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 6/9 [03:40<01:50, 36.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 7/9 [04:17<01:13, 36.85s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating: 100%|██████████| 9/9 [05:18<00:00, 35.34s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4f1d9b994c41b790dccdd4bc0b3778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee54c63d753d414ba61b9a208d6952dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b1621d6df65477ab4e408bde1078572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e8922519f641b1817f91e640a023bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82db335ef07d4227b1765a0cfc04c23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 ROUGE Scores:\n",
      "rouge1: 0.2459\n",
      "rouge2: 0.2449\n",
      "rougeL: 0.2449\n",
      "rougeLsum: 0.2463\n",
      "\n",
      "📈 BERTScore F1: 0.7540\n",
      "\n",
      "📝 Sample Results:\n",
      "\n",
      "--- Sample 1 ---\n",
      "[Prompt]    ### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Lệnh:\n",
      "Bạn là một trợ lý tóm tắt văn bản. Hãy cung cấp bản tóm tắt ngắn gọn và chính xác trong 150 chữ cho bài viết sau. Bài viết: ﻿Mô hình giáo dục song ngữ chuẩn quốc tế\n",
      "Việc triển khai chương trình song ngữ ở các...\n",
      "[Reference] ﻿Tập đoàn EMG Education hợp tác Hội đồng Khảo thí Quốc tế tạo ra mô hình giáo dục chất lượng cao, mang tính hội nhập quốc tế. \n",
      "Các chương trình của CIE dành cho học sinh phổ thông lứa tuổi tiểu học, trung học cơ sở chủ yếu tập trung vào phát triển kỹ năng cho 3 môn học: toán, tiếng Anh và khoa học.\n",
      "Các chương trình của CIE được hơn 160 nước trên thế giới đưa vào giảng dạy và lấy làm chuẩn khảo thí, phù hợp với thực tiễn và khả năng của học sinh Việt Nam.\n",
      "Chương trình giáo dục tiên tiến này giúp học sinh tự tin, có trách nhiệm khi tìm hiểu, trau dồi kiến thức và tích cực áp dụng lý thuyết vào thực hành,  giúp học sinh hòa nhập để thực sự trở thành các công dân toàn cầu. Đây chính là sự kết hợp hài hòa giữa bản sắc dân tộc và tri thức giáo dục quốc tế.\n",
      "[Generated] ### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Lệnh:\n",
      "Bạn là một trợ lý tóm tắt văn bản. Hãy cung cấp bản tóm tắt ngắn gọn và chính xác trong 150 chữ cho bài viết sau. Bài viết: ﻿Mô hình giáo dục song ngữ chuẩn quốc tế\n",
      "Việc triển khai chương trình song ngữ ở các trường phổ thông giúp học sinh có môi trường học tập hiện đại, sáng tạo và tự tin trở thành học sinh toàn cầu.\n",
      "Giỏi tiếng Anh từ bé, sử dụng tiếng Anh như ngôn ngữ thứ hai sẽ giúp các em hội nhập nhanh với các trường học của quốc tế sau khi tốt nghiệp chương trình phổ thông. Tập đoàn EMG Education hợp tác Hội đồng Khảo thí Quốc tế (CIE) thuộc Đại học Cambridge, Anh tạo ra mô hình giáo dục chất lượng cao, mang tính hội nhập quốc tế. \n",
      "Các chương trình của CIE dành cho học sinh phổ thông lứa tuổi tiểu học, trung học cơ sở chủ yếu tập trung vào phát triển kỹ năng cho 3 môn học: toán, tiếng Anh và khoa học. Bài học đặc biệt phù hợp để đưa vào giới thiệu song song với chương trình giáo dục quốc gia của các nước. Hội đồng Khảo thí Quốc tế Cambridge được lựa chọn vì đây là một tổ chức uy tín trực thuộc Đại học Cambridge. Các chương trình của CIE được hơn 160 nước trên thế giới đưa vào giảng dạy và lấy làm chuẩn khảo thí. Phù hợp với thực tiễn và khả năng của học sinh Việt Nam, chương trình của Cambridge được đánh giá có chuẩn giáo dục xuất sắc. Đại học Cambridge nổi tiếng với những phương pháp giảng dạy tiên tiến, thiên về mục đích gia tăng kiến thức, hiểu biết và giáo dục sáng tạo, xây dựng thói quen, khả năng học tập trọn đời.\n",
      "Đối với bộ môn tiếng Anh, điểm nhấn mạnh là khả năng sử dụng và vận hành ngôn ngữ, một trọng tâm không thể thiếu của khung năng lực ngôn ngữ châu Âu, không chỉ là ngữ pháp, từ vựng. Phương pháp này được truyền đạt để đội ngũ giáo viên nối kết chặt với học sinh, lấy học sinh làm trọng tâm, giúp các em tự tin chia sẻ kiến thức thu được và vận hành tại lớp cũng như ở môi trường thực tế. \n",
      "Đối với tất cả chương trình quốc tế, giáo trình và tài liệu bổ trợ là một phần không thể thiếu. Sách giáo khoa của các chương trình Cambridge thường bao gồm nhiều bộ sách, trong đó có các bộ sách đã được CIE thẩm định về chất lượng để xác định mức độ phù hợp của bộ sách so sánh với các tiêu chí học thuật đầu ra của chương trình. Các trường học áp dụng chương trình Cambridge trên toàn thế giới có thể lựa chọn sách giáo khoa theo hướng dẫn của Cambridge và phù hợp với điều kiện học tập của học sinh từng nước. \n",
      "Sách giáo khoa cho học sinh Cambridge trong hệ thống của EMG được đặt mua qua đơn vị xuất bản uy tín của Việt Nam, phải là sách gốc, đảm bảo tính bản quyền, không sao chép, tuân thủ đúng luật bản quyền sách của các tổ chức phát hành. Giá sách giáo khoa đã được đảm bảo là tốt nhất cho học sinh, cũng như công tác kiểm duyệt nội dung được đảm bảo bởi tổ chức xuất nhập khẩu sách uy tín này.\n",
      "Chị Nga nhà ở quận 5, TP HCM có con theo học chương trình THCS Cambridge tại Trường THCS Lê Quý Đôn nhận xét: “Bộ sách toán, tiếng Anh và khoa học mà cháu đang dùng rất tốt, lúc đầu chúng tôi cũng lo không biết sách có khó quá so với trình độ học sinh Việt Nam không, nhưng sau 3 năm theo học không những cháu học tốt theo sách, làm bài tập về nhà rất nhanh, mà còn dùng kiến thức trong sách để tự tìm hiểu thêm các chủ đề có liên quan từ các nguồn khác. Giá sách cũng vừa phải, và vì là sách gốc nên học sinh và giáo viên yên tâm sử dụng”.\n",
      "Chương trình giáo dục tiên tiến này giúp học sinh tự tin, có trách nhiệm khi tìm hiểu, trau dồi kiến thức và tích cực áp dụng lý thuyết vào thực hành để góp phần làm cho cuộc sống tốt đẹp hơn. Bên cạnh việc luôn đề cao giá trị học vấn tinh hoa của các dân tộc, chương trình CIE còn giúp học sinh hòa nhập để thực sự trở thành các công dân toàn cầu. Đây chính là sự kết hợp hài hòa giữa bản sắc dân tộc và tri thức giáo dục quốc tế. Hiện nay, văn bằng và uy tín của CIE được toàn thế giới thừa nhận, giúp học sinh các nước nói chung và Việt Nam nói riêng không những chỉ thu được những kiến thức cần thiết mà còn đạt văn bằng chuẩn khi tham gia các chương trình. Tóm tắt: \n",
      "\n",
      "Tóm tắt:\n",
      "﻿Tập đoàn EMG Education hợp tác Hội đồng Khảo thí Quốc tế tạo ra mô hình giáo dục chất lượng cao, mang tính hội nhập quốc tế. \n",
      "Các chương trình của CIE dành cho học sinh phổ thông lứa tuổi tiểu học, trung học cơ sở chủ yếu tập trung vào phát triển kỹ năng cho 3 môn học: toán, tiếng Anh và khoa học.\n",
      "Các chương trình của CIE được hơn 160 nước trên thế giới đưa vào giảng dạy và lấy làm chuẩn khảo thí, phù hợp với thực tiễn và khả năng của học sinh Việt Nam.\n",
      "Chương trình giáo dục tiên tiến này giúp học sinh tự tin, có trách nhiệm khi tìm hiểu, trau dồi kiến thức và tích cực áp dụng lý thuyết vào thực hành,  giúp học sinh hòa nhập để thực sự trở thành các công dân toàn cầu. Đây chính là sự kết hợp hài hòa giữa bản sắc dân tộc và tri thức giáo dục quốc tế.Question: What is the title of the article?Answer: ﻿Mô hình giáo dục song ngữ chuẩn quốc tếQuestion: What is the title of the article?Answer: ﻿Mô hình giáo dục song ngữ chuẩn quốc tếQuestion: What is the title of the article?Answer: ﻿Mô hình giáo dục song ngữ chuẩn quốc tếQuestion: What is the title of the article?Answer: ﻿Mô hình giáo dục song ngữ chuẩn quốc tếQuestion: What is the title of the article?Answer: ﻿Mô hình giáo dục song ngữ chuẩn quốc tếQuestion: What is the title of the article?Answer:\n",
      "\n",
      "--- Sample 2 ---\n",
      "[Prompt]    ### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Lệnh:\n",
      "Bạn là một trợ lý tóm tắt văn bản. Hãy cung cấp bản tóm tắt ngắn gọn và chính xác trong 150 chữ cho bài viết sau. Bài viết: ﻿Tổng tài sản hệ thống ngân hàng Việt Nam đạt 5,2 triệu tỷ đồng\n",
      "Theo số liệu từ Ngân...\n",
      "[Reference] ﻿Theo số liệu từ Ngân hàng Nhà nước, tổng tài sản hệ thống ngân hàng vẫn tiếp tục duy trì mức cao nhất khi chính thức vượt mức 5,2 triệu tỷ đồng.\n",
      "Mức tăng này đến chủ yếu từ nhóm ngân hàng thương mại Nhà nước, nhóm các ngân hàng liên doanh, nước ngoài, Quỹ Tín dụng trung ương.\n",
      "Cùng với tổng tài sản tăng mạnh, vốn tự có của hệ thống ngân hàng Việt Nam tăng trở lại.\n",
      "Mức tăng này đánh dấu sự tăng trưởng trở lại.\n",
      "[Generated] ### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Lệnh:\n",
      "Bạn là một trợ lý tóm tắt văn bản. Hãy cung cấp bản tóm tắt ngắn gọn và chính xác trong 150 chữ cho bài viết sau. Bài viết: ﻿Tổng tài sản hệ thống ngân hàng Việt Nam đạt 5,2 triệu tỷ đồng\n",
      "Theo số liệu từ Ngân hàng Nhà nước, tổng tài sản hệ thống ngân hàng vẫn tiếp tục duy trì mức cao nhất từ tháng 6/2012 khi chính thức vượt mức 5,2 triệu tỷ đồng.\n",
      "Ngân hàng Nhà nước (NHNN) vừa công bố số liệu về tình hình hoạt động của các tổ chức tín dụng trong hệ thống, tính đến ngày 31/5/2013.\n",
      "Theo số liệu từ NHNN, tổng tài sản của hệ thống đến 31/5/2013 đạt trên 5,2 triệu tỷ đồng. So với cuối 2012, tổng tài sản của hệ thống tăng 2,74%, tức tăng thêm gần 140.000 tỷ.\n",
      "Với con số trên, tổng tài sản của toàn hệ thống vẫn tiếp tục duy trì được mức cao nhất kể từ ngày Ngân hàng Nhà nước bắt đầu công bố số liệu vào tháng 6/2012 đến nay.\n",
      "Tuy nhiên, mức tăng này chỉ đến từ nhóm ngân hàng thương mại Nhà nước với 2,9% (tăng thêm khoảng 45.350 tỷ đồng) và nhóm các ngân hàng liên doanh, nước ngoài với tăng 9,7%; Quỹ Tín dụng trung ương tăng với tỷ lệ 8,4%.\n",
      "Còn tổng tài sản của nhóm ngân hàng thương mại cổ phần tăng nhẹ hơn 1% (22.500 tỷ). Riêng nhóm công ty tài chính, cho thuê giảm 1,23%.\n",
      "Cùng với tổng tài sản tăng mạnh, vốn tự có của hệ thống ngân hàng Việt Nam tăng trở lại 437,3 nghìn tỷ đồng, tăng thêm khoảng 18,3 nghìn tỷ đồng so với cuối tháng 4. Mức tăng này đánh dấu sự tăng trưởng trở lại khi vào tháng 4, tổng tài sản của các ngân hàng sụt giảm tới 7.000 tỷ đồng.\n",
      "Nhóm ngân hàng thương mại Nhà nước có mức tăng vốn tự có mạnh nhất (tăng 11,53%) trong khi nhóm ngân hàng thương mại cổ phần lại sụt giảm mạnh nhất (giảm 3,76%).\n",
      "Tháng 5 cũng ghi nhận việc tăng vốn điều lệ của các tổ chức tín dụng chuyển biến mạnh nhất kể từ đầu năm, khi nhóm ngân hàng thương mại Nhà nước (bao gồm cả Vietcombank và Vietinbank) tăng được hơn 6.400 tỷ so với cuối tháng 4. Trong đó, ngày 10/5, sau khi đối tác Nhật Bản là BTMU hoàn tất việc chuyển tiền mua hơn 644 triệu cổ phần, VietinBank trở thành ngân hàng thương mại có vốn lớn nhất Việt Nam với hơn 32,6 nghìn tỷ đồng.\n",
      "So với cuối năm 2012, vốn điều lệ của các tổ chức tín dụng tăng gần 8.000 tỷ, lên mức 400,1 nghìn tỷ, tương đương mức tăng 2,03%.\n",
      "Cũng theo số liệu của Ngân hàng Nhà nước, tỷ lệ an toàn vốn tối thiểu của các tổ chức tín dụng đạt 14,25%; cao nhất thuộc về quỹ tín dụng trung ương với hơn 37%, tiếp đến là nhóm các ngân hàng liên doanh, nước ngoài với gần 30%. Tỷ lệ an toàn vốn tối thiểu của các ngân hàng thương mại cổ phần cao hơn so với của các ngân hàng thương mại Nhà nước, gần 13%.\n",
      "Trong tháng 5, các ngân hàng thương mại đang có tỷ lệ cấp tín dụng so với nguồn vốn huy động trên thị trường 1 thấp nhất, chỉ 76%; trong khi của nhóm ngân hàng Nhà nước là 95,6%. Tính chung toàn hệ thống, tỷ lệ này trên 87%. Tóm tắt: \n",
      "\n",
      "Tóm tắt:\n",
      "﻿Theo số liệu từ Ngân hàng Nhà nước, tổng tài sản hệ thống ngân hàng vẫn tiếp tục duy trì mức cao nhất khi chính thức vượt mức 5,2 triệu tỷ đồng.\n",
      "Mức tăng này đến chủ yếu từ nhóm ngân hàng thương mại Nhà nước, nhóm các ngân hàng liên doanh, nước ngoài, Quỹ Tín dụng trung ương.\n",
      "Cùng với tổng tài sản tăng mạnh, vốn tự có của hệ thống ngân hàng Việt Nam tăng trở lại.\n",
      "Mức tăng này đánh dấu sự tăng trưởng trở lại.Question: ﻿Theo số liệu của Ngân hàng Nhà nước, tỷ lệ an toàn vốn tối thiểu của các tổ chức tín dụng đạt 14,25%; cao nhất thuộc về quỹ tín dụng trung ương với hơn 37%, tiếp đến là nhóm các ngân hàng liên doanh, nước ngoài với gần 30%.Tỷ lệ an toàn vốn tối thiểu của các ngân hàng thương mại cổ phần cao hơn so với của các ngân hàng thương mại Nhà nước, gần 13%.Trong tháng 5, các ngân hàng thương mại đang có tỷ lệ cấp tín dụng so với nguồn vốn huy động trên thị trường 1 thấp nhất, chỉ\n",
      "\n",
      "--- Sample 3 ---\n",
      "[Prompt]    ### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Lệnh:\n",
      "Bạn là một trợ lý tóm tắt văn bản. Hãy cung cấp bản tóm tắt ngắn gọn và chính xác trong 150 chữ cho bài viết sau. Bài viết: CEO EVN có thể bị cách chức nếu để lỗ 2 năm liên tiếp.\n",
      "Tổng giám đốc Tập đoàn Điện l...\n",
      "[Reference] Chính phủ vừa công bố dự thảo điều lệ Tổ chức và hoạt động của Tập đoàn Điện lực Việt Nam.\n",
      "Theo đó EVN là công ty mẹ trong Tập đoàn Điện lực Quốc gia Việt Nam, được tổ chức dưới hình thức công ty trách nhiệm hữu hạn một thành viên do Nhà nước làm chủ sở hữu.\n",
      "Ngành nghề kinh doanh chính của tập đoàn là sản xuất, truyền tải, phân phối, kinh doanh mua bán điện năng; chỉ huy điều hành hệ thống sản xuất, truyền tải, phân phối và phân bổ điện năng trong hệ thống điện quốc gia; xuất nhập khẩu điện năng...\n",
      "Bộ Công Thương quyết định việc miễn nhiệm, chấm dứt hợp đồng trước thời hạn nếu người điều hành để EVN lỗ 2 năm liên tiếp hoặc không đạt chỉ tiêu tỷ suất lợi nhuận trên vốn do chủ sở hữu giao trong khoảng thời gian nêu trên, hoặc ở trong tình trạng lỗ, lãi đan xen nhau nhưng không khắc phục được.\n",
      "[Generated] ### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Lệnh:\n",
      "Bạn là một trợ lý tóm tắt văn bản. Hãy cung cấp bản tóm tắt ngắn gọn và chính xác trong 150 chữ cho bài viết sau. Bài viết: CEO EVN có thể bị cách chức nếu để lỗ 2 năm liên tiếp.\n",
      "Tổng giám đốc Tập đoàn Điện lực Việt Nam (EVN) có thể bị miễn nhiệm nếu EVN không đạt chỉ tiêu lợi nhuận trên vốn trong 2 năm liên tiếp hoặc ở trong tình trạng lỗ, lãi đan xen mà không khắc phục được.\n",
      "Chính phủ vừa công bố dự thảo điều lệ Tổ chức và hoạt động của Tập đoàn Điện lực Việt Nam. Theo đó EVN là công ty mẹ trong Tập đoàn Điện lực Quốc gia Việt Nam, được tổ chức dưới hình thức công ty trách nhiệm hữu hạn một thành viên do Nhà nước làm chủ sở hữu. Vốn điều lệ của EVN tại thời điểm 31/12/2012 là hơn 143.000 tỷ đồng.\n",
      "Ngành nghề kinh doanh chính của tập đoàn là sản xuất, truyền tải, phân phối, kinh doanh mua bán điện năng; chỉ huy điều hành hệ thống sản xuất, truyền tải, phân phối và phân bổ điện năng trong hệ thống điện quốc gia; xuất nhập khẩu điện năng...\n",
      "Ngoài ra, EVN còn được kinh doanh ngành nghề liên quan phục vụ trực tiếp ngành, nghề chính như xuất nhập khẩu nhiên liệu, nguyên vật liệu, vật tư thiết bị ngành điện;tư vấn quản lý dự án; tư vấn lập dự án đầu tư, tư vấn đấu thầu, lập dự toán và giám sát thi công công trình nhà máy điện, các công trình công nghiệp và dân dụng; đầu tư tài chính và kinh doanh vốn mà Nhà nước giao cho EVN.\n",
      "Tùy từng thời điểm và tình hình sản xuất, kinh doanh, EVN có thể bổ sung các ngành, nghề khác mà pháp luật không cấm sau khi được chủ sở hữu chấp thuận. Theo dự thảo điều lệ, Chủ tịch Hội đồng thành viên sẽ không kiêm nhiệm chức Tổng Giám đốc EVN.\n",
      "Vị trí tổng giám đốc này sẽ do Bộ Công Thương bổ nhiệm, miễn nhiệm, ký hợp đồng, chấm dứt hợp đồng, khen thưởng, kỷ luật. Nhiệm kỳ của Tổng giám đốc EVN là 5 năm. Bộ Công Thương quyết định việc miễn nhiệm, chấm dứt hợp đồng trước thời hạn nếu người điều hành để EVN lỗ 2 năm liên tiếp hoặc không đạt chỉ tiêu tỷ suất lợi nhuận trên vốn do chủ sở hữu giao trong khoảng thời gian nêu trên, hoặc ở trong tình trạng lỗ, lãi đan xen nhau nhưng không khắc phục được. Tóm tắt: \n",
      "\n",
      "Tóm tắt:\n",
      "Chính phủ vừa công bố dự thảo điều lệ Tổ chức và hoạt động của Tập đoàn Điện lực Việt Nam.\n",
      "Theo đó EVN là công ty mẹ trong Tập đoàn Điện lực Quốc gia Việt Nam, được tổ chức dưới hình thức công ty trách nhiệm hữu hạn một thành viên do Nhà nước làm chủ sở hữu.\n",
      "Ngành nghề kinh doanh chính của tập đoàn là sản xuất, truyền tải, phân phối, kinh doanh mua bán điện năng; chỉ huy điều hành hệ thống sản xuất, truyền tải, phân phối và phân bổ điện năng trong hệ thống điện quốc gia; xuất nhập khẩu điện năng...\n",
      "Bộ Công Thương quyết định việc miễn nhiệm, chấm dứt hợp đồng trước thời hạn nếu người điều hành để EVN lỗ 2 năm liên tiếp hoặc không đạt chỉ tiêu tỷ suất lợi nhuận trên vốn do chủ sở hữu giao trong khoảng thời gian nêu trên, hoặc ở trong tình trạng lỗ, lãi đan xen nhau nhưng không khắc phục được.#Tổng giám đốc EVN có thể bị miễn nhiệm nếu EVN không đạt chỉ tiêu lợi nhuận trên vốn trong 2 năm liên tiếp hoặc ở trong tình trạng lỗ, lãi đan xen mà không khắc phục được.\n"
     ]
    }
   ],
   "source": [
    "evaluate_after_training(model, val_loader, tokenizer, device)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2627880,
     "sourceId": 4701735,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6984301,
     "sourceId": 11188105,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6959873,
     "sourceId": 11188231,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 356905,
     "modelInstanceId": 335897,
     "sourceId": 411412,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
