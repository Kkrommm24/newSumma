{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-26T15:38:19.070778Z",
     "iopub.status.busy": "2025-05-26T15:38:19.070466Z",
     "iopub.status.idle": "2025-05-26T15:38:29.847686Z",
     "shell.execute_reply": "2025-05-26T15:38:29.846553Z",
     "shell.execute_reply.started": "2025-05-26T15:38:19.070753Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge_score) (2024.2.0)\n",
      "Requirement already satisfied: bert_score in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.5.1+cu121)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.2.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.47.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.26.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.5)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2025.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.29.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bert_score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bert_score) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->bert_score) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->bert_score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->bert_score) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate peft datasets bitsandbytes evaluate -q\n",
    "!pip install rouge_score\n",
    "!pip install bert_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import library and load module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:38:29.849490Z",
     "iopub.status.busy": "2025-05-26T15:38:29.849120Z",
     "iopub.status.idle": "2025-05-26T15:38:49.456491Z",
     "shell.execute_reply": "2025-05-26T15:38:49.455581Z",
     "shell.execute_reply.started": "2025-05-26T15:38:29.849464Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import json\n",
    "import shutil\n",
    "from IPython.display import FileLink\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    default_data_collator,\n",
    ")\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:38:49.458768Z",
     "iopub.status.busy": "2025-05-26T15:38:49.458095Z",
     "iopub.status.idle": "2025-05-26T15:38:49.596473Z",
     "shell.execute_reply": "2025-05-26T15:38:49.595560Z",
     "shell.execute_reply.started": "2025-05-26T15:38:49.458744Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:38:49.598014Z",
     "iopub.status.busy": "2025-05-26T15:38:49.597645Z",
     "iopub.status.idle": "2025-05-26T15:38:49.601831Z",
     "shell.execute_reply": "2025-05-26T15:38:49.601065Z",
     "shell.execute_reply.started": "2025-05-26T15:38:49.597990Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:38:49.603042Z",
     "iopub.status.busy": "2025-05-26T15:38:49.602742Z",
     "iopub.status.idle": "2025-05-26T15:38:49.622647Z",
     "shell.execute_reply": "2025-05-26T15:38:49.621721Z",
     "shell.execute_reply.started": "2025-05-26T15:38:49.603012Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:38:49.623880Z",
     "iopub.status.busy": "2025-05-26T15:38:49.623590Z",
     "iopub.status.idle": "2025-05-26T15:39:08.067964Z",
     "shell.execute_reply": "2025-05-26T15:39:08.067027Z",
     "shell.execute_reply.started": "2025-05-26T15:38:49.623850Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399231b60ca243fcb9540851f0b7b253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0986c8320d764846ab3deba7e5551a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26081b4132d6419a8aeba576c44c1f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d278207c8941899394c0ee7b795bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d0f09cb80f45db963d8c2697000f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c75f2ff0ef4454a4a317d186269a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# Thi·∫øt l·∫≠p pad_token v√† padding side\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:39:08.069226Z",
     "iopub.status.busy": "2025-05-26T15:39:08.068903Z",
     "iopub.status.idle": "2025-05-26T15:39:08.074047Z",
     "shell.execute_reply": "2025-05-26T15:39:08.073255Z",
     "shell.execute_reply.started": "2025-05-26T15:39:08.069192Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    attention_masks = [item[\"attention_mask\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_masks,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:39:08.076760Z",
     "iopub.status.busy": "2025-05-26T15:39:08.076536Z",
     "iopub.status.idle": "2025-05-26T15:39:13.321642Z",
     "shell.execute_reply": "2025-05-26T15:39:13.320792Z",
     "shell.execute_reply.started": "2025-05-26T15:39:08.076741Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Continue Pretrain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:02:51.782934Z",
     "iopub.status.busy": "2025-05-25T03:02:51.782613Z",
     "iopub.status.idle": "2025-05-25T03:02:54.889569Z",
     "shell.execute_reply": "2025-05-25T03:02:54.888919Z",
     "shell.execute_reply.started": "2025-05-25T03:02:51.782904Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab96146c2024a1b8855d8c16e9c3b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/635 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d935df233d17414bb79a1183eba11d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/35.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4b1ff0664844eb932b08699f7a635d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/4.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c73f47f1d014b5880278e9d5623f2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/4.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319482b65aa44d1594d7fe4c1e76b86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/12806 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4548d22c245c41919618d4a62cab75f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca939bd27d244c088575c7b80bb8b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"vukhai248/vietnamese_news_16k\", split=\"train\")\n",
    "dataset = dataset.select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:02:54.891173Z",
     "iopub.status.busy": "2025-05-25T03:02:54.890933Z",
     "iopub.status.idle": "2025-05-25T03:02:54.894619Z",
     "shell.execute_reply": "2025-05-25T03:02:54.893717Z",
     "shell.execute_reply.started": "2025-05-25T03:02:54.891150Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    return text.replace(\"\\n\", \" \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:02:54.895648Z",
     "iopub.status.busy": "2025-05-25T03:02:54.895450Z",
     "iopub.status.idle": "2025-05-25T03:03:04.441895Z",
     "shell.execute_reply": "2025-05-25T03:03:04.440976Z",
     "shell.execute_reply.started": "2025-05-25T03:02:54.895632Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_fn(text, max_length=2048):\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "    attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "    labels = input_ids.clone()\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:03:04.443066Z",
     "iopub.status.busy": "2025-05-25T03:03:04.442775Z",
     "iopub.status.idle": "2025-05-25T03:03:10.608088Z",
     "shell.execute_reply": "2025-05-25T03:03:10.607451Z",
     "shell.execute_reply.started": "2025-05-25T03:03:04.443043Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "texts = dataset[\"content\"]\n",
    "tokenized_data = [tokenize_fn(text) for text in texts]\n",
    "train_loader = DataLoader(\n",
    "    tokenized_data,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:03:10.609190Z",
     "iopub.status.busy": "2025-05-25T03:03:10.608887Z",
     "iopub.status.idle": "2025-05-25T10:51:54.609669Z",
     "shell.execute_reply": "2025-05-25T10:51:54.608737Z",
     "shell.execute_reply.started": "2025-05-25T03:03:10.609162Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 1/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [2:35:22<00:00,  9.32s/it, loss=2.72] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Loss: 2.3339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [2:37:08<00:00,  9.43s/it, loss=2.06] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Loss: 2.2864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [2:36:12<00:00,  9.37s/it, loss=2.25] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Loss: 2.2713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, device, n_epochs=3, lr=1e-5):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\", leave=True)\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            # Forward pass with checkpointing\n",
    "            def forward_with_checkpoint(*args, **kwargs):\n",
    "                return torch.utils.checkpoint.checkpoint(\n",
    "                    lambda *args, **kwargs: model(*args, **kwargs),\n",
    "                    *args,\n",
    "                    **kwargs,\n",
    "                    use_reentrant=False\n",
    "                )\n",
    "\n",
    "            outputs = forward_with_checkpoint(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} - Loss: {avg_loss:.4f}\")\n",
    "train(model, train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T10:51:54.611200Z",
     "iopub.status.busy": "2025-05-25T10:51:54.610857Z",
     "iopub.status.idle": "2025-05-25T10:51:55.038424Z",
     "shell.execute_reply": "2025-05-25T10:51:55.037648Z",
     "shell.execute_reply.started": "2025-05-25T10:51:54.611169Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pretrained model saved at: /kaggle/working/llama3-qlora-continued-pretrain\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-continued-pretrain\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"‚úÖ Pretrained model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T10:51:55.039477Z",
     "iopub.status.busy": "2025-05-25T10:51:55.039197Z",
     "iopub.status.idle": "2025-05-25T10:51:55.626208Z",
     "shell.execute_reply": "2025-05-25T10:51:55.625554Z",
     "shell.execute_reply.started": "2025-05-25T10:51:55.039447Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='llama3-qlora-continued-pretrain.zip' target='_blank'>llama3-qlora-continued-pretrain.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/llama3-qlora-continued-pretrain.zip"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ƒê∆∞·ªùng d·∫´n t·ªõi th∆∞ m·ª•c ch·ª©a m√¥ h√¨nh\n",
    "model_dir = '/kaggle/working/llama3-qlora-continued-pretrain'\n",
    "\n",
    "# T·∫°o file n√©n .zip t·ª´ th∆∞ m·ª•c\n",
    "zip_name = 'llama3-qlora-continued-pretrain'  # T√™n file n√©n (kh√¥ng c·∫ßn ƒëu√¥i .zip)\n",
    "shutil.make_archive(zip_name, 'zip', model_dir)\n",
    "\n",
    "# T·∫°o li√™n k·∫øt t·∫£i xu·ªëng cho file .zip\n",
    "display(FileLink(f'{zip_name}.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finetune with QA dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T17:20:37.520991Z",
     "iopub.status.busy": "2025-05-25T17:20:37.520658Z",
     "iopub.status.idle": "2025-05-25T17:20:37.524611Z",
     "shell.execute_reply": "2025-05-25T17:20:37.523676Z",
     "shell.execute_reply.started": "2025-05-25T17:20:37.520964Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"/kaggle/input/llama-continued-pretrain-model/transformers/default/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:06.686944Z",
     "iopub.status.busy": "2025-05-25T11:06:06.686643Z",
     "iopub.status.idle": "2025-05-25T11:06:10.813181Z",
     "shell.execute_reply": "2025-05-25T11:06:10.812250Z",
     "shell.execute_reply.started": "2025-05-25T11:06:06.686922Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/vietnamese-squad/train-v2.0-translated.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    vi_squad_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:10.814439Z",
     "iopub.status.busy": "2025-05-25T11:06:10.814139Z",
     "iopub.status.idle": "2025-05-25T11:06:10.819212Z",
     "shell.execute_reply": "2025-05-25T11:06:10.818379Z",
     "shell.execute_reply.started": "2025-05-25T11:06:10.814410Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Train samples: 10000\n",
      "üì¶ Validation samples: 2000\n"
     ]
    }
   ],
   "source": [
    "train_data = vi_squad_data[:10000]\n",
    "\n",
    "# L·∫•y 2k m·∫´u ti·∫øp theo cho validation\n",
    "val_data = vi_squad_data[10000:12000]\n",
    "\n",
    "print(f\"üì¶ Train samples: {len(train_data)}\")\n",
    "print(f\"üì¶ Validation samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:10.820518Z",
     "iopub.status.busy": "2025-05-25T11:06:10.820173Z",
     "iopub.status.idle": "2025-05-25T11:06:15.870562Z",
     "shell.execute_reply": "2025-05-25T11:06:15.869725Z",
     "shell.execute_reply.started": "2025-05-25T11:06:10.820470Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Train samples: ['Beyonc√© Giselle Knowles-Carter (/ b i g√¨ …í n s e…™ / bee-YON-say) (sinh ng√†y 04 th√°ng 9 1981) l√† m·ªôt ca sƒ©, nh·∫°c sƒ©, nh√† s·∫£n xu·∫•t thu √¢m v√† n·ªØ di·ªÖn vi√™n ng∆∞·ªùi M·ªπ. Sinh ra v√† l·ªõn l√™n ·ªü Houston, Texas, c√¥ ƒë√£ bi·ªÉu di·ªÖn trong c√°c cu·ªôc thi ca h√°t v√† nh·∫£y m√∫a kh√°c nhau khi c√≤n nh·ªè, v√† n·ªïi ti·∫øng v√†o cu·ªëi nh·ªØng nƒÉm 1990 v·ªõi t∆∞ c√°ch l√† ca sƒ© ch√≠nh c·ªßa nh√≥m nh·∫°c n·ªØ R & B Destiny\\'s Child. ƒê∆∞·ª£c qu·∫£n l√Ω b·ªüi cha c√¥, Mathew Knowles, nh√≥m ƒë√£ tr·ªü th√†nh m·ªôt trong nh·ªØng nh√≥m nh·∫°c n·ªØ b√°n ch·∫°y nh·∫•t th·∫ø gi·ªõi m·ªçi th·ªùi ƒë·∫°i. S·ª± gi√°n ƒëo·∫°n c·ªßa h·ªç ƒë√£ ch·ª©ng ki·∫øn vi·ªác ph√°t h√†nh album ƒë·∫ßu tay c·ªßa Beyonc√©, Dangerously in Love (2003), gi√∫p c√¥ tr·ªü th√†nh m·ªôt ngh·ªá sƒ© solo tr√™n to√†n th·∫ø gi·ªõi, gi√†nh ƒë∆∞·ª£c nƒÉm gi·∫£i Grammy v√† c√≥ ƒëƒ©a ƒë∆°n qu√°n qu√¢n Billboard Hot 100 \"Crazy in Love\" v√† \"Baby Boy\".', 'Beyonce b·∫Øt ƒë·∫ßu n·ªïi ti·∫øng t·ª´ khi n√†o?', 'V√†o cu·ªëi nh·ªØng nƒÉm 1990']\n"
     ]
    }
   ],
   "source": [
    "print(f\"üì¶ Train samples: {train_data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:15.871686Z",
     "iopub.status.busy": "2025-05-25T11:06:15.871441Z",
     "iopub.status.idle": "2025-05-25T11:06:16.056357Z",
     "shell.execute_reply": "2025-05-25T11:06:16.055708Z",
     "shell.execute_reply.started": "2025-05-25T11:06:15.871666Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_list([{\"context\": item[0], \"question\": item[1], \"answer\": item[2]} for item in train_data])\n",
    "val_dataset = Dataset.from_list([{\"context\": item[0], \"question\": item[1], \"answer\": item[2]} for item in val_data])\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": val_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:16.057228Z",
     "iopub.status.busy": "2025-05-25T11:06:16.057027Z",
     "iopub.status.idle": "2025-05-25T11:06:16.067113Z",
     "shell.execute_reply": "2025-05-25T11:06:16.066179Z",
     "shell.execute_reply.started": "2025-05-25T11:06:16.057210Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. {'context': 'Beyonc√© Giselle Knowles-Carter (/ b i g√¨ …í n s e…™ / bee-YON-say) (sinh ng√†y 04 th√°ng 9 1981) l√† m·ªôt ca sƒ©, nh·∫°c sƒ©, nh√† s·∫£n xu·∫•t thu √¢m v√† n·ªØ di·ªÖn vi√™n ng∆∞·ªùi M·ªπ. Sinh ra v√† l·ªõn l√™n ·ªü Houston, Texas, c√¥ ƒë√£ bi·ªÉu di·ªÖn trong c√°c cu·ªôc thi ca h√°t v√† nh·∫£y m√∫a kh√°c nhau khi c√≤n nh·ªè, v√† n·ªïi ti·∫øng v√†o cu·ªëi nh·ªØng nƒÉm 1990 v·ªõi t∆∞ c√°ch l√† ca sƒ© ch√≠nh c·ªßa nh√≥m nh·∫°c n·ªØ R & B Destiny\\'s Child. ƒê∆∞·ª£c qu·∫£n l√Ω b·ªüi cha c√¥, Mathew Knowles, nh√≥m ƒë√£ tr·ªü th√†nh m·ªôt trong nh·ªØng nh√≥m nh·∫°c n·ªØ b√°n ch·∫°y nh·∫•t th·∫ø gi·ªõi m·ªçi th·ªùi ƒë·∫°i. S·ª± gi√°n ƒëo·∫°n c·ªßa h·ªç ƒë√£ ch·ª©ng ki·∫øn vi·ªác ph√°t h√†nh album ƒë·∫ßu tay c·ªßa Beyonc√©, Dangerously in Love (2003), gi√∫p c√¥ tr·ªü th√†nh m·ªôt ngh·ªá sƒ© solo tr√™n to√†n th·∫ø gi·ªõi, gi√†nh ƒë∆∞·ª£c nƒÉm gi·∫£i Grammy v√† c√≥ ƒëƒ©a ƒë∆°n qu√°n qu√¢n Billboard Hot 100 \"Crazy in Love\" v√† \"Baby Boy\".', 'question': 'Beyonce b·∫Øt ƒë·∫ßu n·ªïi ti·∫øng t·ª´ khi n√†o?', 'answer': 'V√†o cu·ªëi nh·ªØng nƒÉm 1990'}\n",
      "2. Beyonc√© Giselle Knowles-Carter (/ b i g√¨ …í n s e…™ / bee-YON-say) (sinh ng√†y 04 th√°ng 9 1981) l√† m·ªôt ca sƒ©, nh·∫°c sƒ©, nh√† s·∫£n xu·∫•t thu √¢m v√† n·ªØ di·ªÖn vi√™n ng∆∞·ªùi M·ªπ. Sinh ra v√† l·ªõn l√™n ·ªü Houston, Texas, c√¥ ƒë√£ bi·ªÉu di·ªÖn trong c√°c cu·ªôc thi ca h√°t v√† nh·∫£y m√∫a kh√°c nhau khi c√≤n nh·ªè, v√† n·ªïi ti·∫øng v√†o cu·ªëi nh·ªØng nƒÉm 1990 v·ªõi t∆∞ c√°ch l√† ca sƒ© ch√≠nh c·ªßa nh√≥m nh·∫°c n·ªØ R & B Destiny's Child. ƒê∆∞·ª£c qu·∫£n l√Ω b·ªüi cha c√¥, Mathew Knowles, nh√≥m ƒë√£ tr·ªü th√†nh m·ªôt trong nh·ªØng nh√≥m nh·∫°c n·ªØ b√°n ch·∫°y nh·∫•t th·∫ø gi·ªõi m·ªçi th·ªùi ƒë·∫°i. S·ª± gi√°n ƒëo·∫°n c·ªßa h·ªç ƒë√£ ch·ª©ng ki·∫øn vi·ªác ph√°t h√†nh album ƒë·∫ßu tay c·ªßa Beyonc√©, Dangerously in Love (2003), gi√∫p c√¥ tr·ªü th√†nh m·ªôt ngh·ªá sƒ© solo tr√™n to√†n th·∫ø gi·ªõi, gi√†nh ƒë∆∞·ª£c nƒÉm gi·∫£i Grammy v√† c√≥ ƒëƒ©a ƒë∆°n qu√°n qu√¢n Billboard Hot 100 \"Crazy in Love\" v√† \"Baby Boy\".\n",
      "3. Beyonce b·∫Øt ƒë·∫ßu n·ªïi ti·∫øng t·ª´ khi n√†o?\n",
      "4. V√†o cu·ªëi nh·ªØng nƒÉm 1990\n"
     ]
    }
   ],
   "source": [
    "print(f\"1. {dataset_dict['train'][0]}\")\n",
    "print(f\"2. {dataset_dict['train'][0]['context']}\")\n",
    "print(f\"3. {dataset_dict['train'][0]['question']}\")\n",
    "print(f\"4. {dataset_dict['train'][0]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:16.068330Z",
     "iopub.status.busy": "2025-05-25T11:06:16.068070Z",
     "iopub.status.idle": "2025-05-25T11:06:33.217119Z",
     "shell.execute_reply": "2025-05-25T11:06:33.216196Z",
     "shell.execute_reply.started": "2025-05-25T11:06:16.068298Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27c694acfcc45e48aef08fa794fe98a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742c0882c4a8464787c11b82c639a0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_qa(example, max_length=512):\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"]\n",
    "    prompt = f\"### ƒê√¢y l√† d·∫°ng c√¢u h·ªèi v√† tr·∫£ l·ªùi d·ª±a tr√™n n·ªôi dung ###\\n\\nC√¢u h·ªèi: {question}\\n\\nN·ªôi dung: {context}\\n\\nTr·∫£ l·ªùi:\"\n",
    "    completion = f\" {answer}\"\n",
    "    \n",
    "    prompt_ids = tokenizer(prompt, add_special_tokens=False).input_ids\n",
    "    completion_ids = tokenizer(completion, add_special_tokens=False).input_ids\n",
    "\n",
    "    input_ids = prompt_ids + completion_ids\n",
    "\n",
    "    labels = [-100] * len(prompt_ids) + completion_ids\n",
    "\n",
    "    if len(input_ids) > max_length:\n",
    "        input_ids = input_ids[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    \n",
    "    input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "    attention_mask = attention_mask + [0] * padding_length\n",
    "    labels = labels + [-100] * padding_length\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids),\n",
    "        \"attention_mask\": torch.tensor(attention_mask),\n",
    "        \"labels\": torch.tensor(labels)\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset_dict.map(preprocess_qa)\n",
    "\n",
    "tokenized_dataset.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T11:06:33.218537Z",
     "iopub.status.busy": "2025-05-25T11:06:33.218201Z",
     "iopub.status.idle": "2025-05-25T11:06:33.223084Z",
     "shell.execute_reply": "2025-05-25T11:06:33.222309Z",
     "shell.execute_reply.started": "2025-05-25T11:06:33.218500Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"], batch_size=4, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_dataset[\"test\"], batch_size=4, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-25T17:18:25.136Z",
     "iopub.execute_input": "2025-05-25T11:06:33.224338Z",
     "iopub.status.busy": "2025-05-25T11:06:33.224038Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:   0%|          | 0/2500 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 1/2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2500/2500 [3:22:25<00:00,  4.86s/it, loss=1.42]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1/2 - Train Loss: 1.7102\n",
      "üìä Validation Loss: 1.3921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  25%|‚ñà‚ñà‚ñç       | 621/2500 [50:22<2:32:00,  4.85s/it, loss=2.36] "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "n_epochs = 2\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\", leave=True)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"‚úÖ Epoch {epoch+1}/{n_epochs} - Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"üìä Validation Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-25T17:18:25.136Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-qa-finetune\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"‚úÖ Pretrained model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-25T17:18:25.137Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_dir = '/kaggle/working/llama3-qlora-qa-finetune'\n",
    "\n",
    "# T·∫°o file n√©n .zip t·ª´ th∆∞ m·ª•c\n",
    "zip_name = 'llama3-qlora-qa-finetune'  # T√™n file n√©n (kh√¥ng c·∫ßn ƒëu√¥i .zip)\n",
    "shutil.make_archive(zip_name, 'zip', model_dir)\n",
    "\n",
    "# T·∫°o li√™n k·∫øt t·∫£i xu·ªëng cho file .zip\n",
    "display(FileLink(f'{zip_name}.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finetune with BBC News**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:39:13.323315Z",
     "iopub.status.busy": "2025-05-26T15:39:13.323026Z",
     "iopub.status.idle": "2025-05-26T15:39:13.328052Z",
     "shell.execute_reply": "2025-05-26T15:39:13.327436Z",
     "shell.execute_reply.started": "2025-05-26T15:39:13.323281Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:39:13.328975Z",
     "iopub.status.busy": "2025-05-26T15:39:13.328782Z",
     "iopub.status.idle": "2025-05-26T15:39:13.896387Z",
     "shell.execute_reply": "2025-05-26T15:39:13.895524Z",
     "shell.execute_reply.started": "2025-05-26T15:39:13.328958Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88bb685b1214b19b5cdd681cab3399b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = \"/kaggle/input/data-llama-finetune/bbc_data_llama_finetune.json\"\n",
    "\n",
    "full_dataset = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
    "dataset = full_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:39:13.897458Z",
     "iopub.status.busy": "2025-05-26T15:39:13.897217Z",
     "iopub.status.idle": "2025-05-26T15:39:20.636998Z",
     "shell.execute_reply": "2025-05-26T15:39:20.635982Z",
     "shell.execute_reply.started": "2025-05-26T15:39:13.897438Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115ce413698243d8af1e18312877e268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1914 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333f321300bd4fbeb41a52f57fccef1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/213 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(example, max_length=2048):\n",
    "    prompt = f\"### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:  ### L·ªánh:\\n{example['prompt']}\\n\\nT√≥m t·∫Øt:\\n\"\n",
    "    summary = example[\"summary\"]\n",
    "    prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    summary_ids = tokenizer.encode(summary, add_special_tokens=False)\n",
    "    \n",
    "    # Ki·ªÉm tra t·ªïng s·ªë token\n",
    "    total_length = len(prompt_ids) + len(summary_ids)\n",
    "    if total_length > max_length:\n",
    "        overflow = total_length - max_length\n",
    "        if overflow < len(prompt_ids):\n",
    "            prompt_ids = prompt_ids[:-overflow]\n",
    "        else:\n",
    "            prompt_ids = []\n",
    "    input_ids = prompt_ids + summary_ids\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    labels = [-100] * len(prompt_ids) + summary_ids\n",
    "\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "        attention_mask = attention_mask + [0] * padding_length\n",
    "        labels = labels + [-100] * padding_length\n",
    "    else:\n",
    "        input_ids = input_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "tokenized_dataset = dataset.map(tokenize, batched=False, fn_kwargs={\"max_length\": 2048})\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "collator = default_data_collator\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_dataset[\"test\"],\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:39:20.637991Z",
     "iopub.status.busy": "2025-05-26T15:39:20.637755Z",
     "iopub.status.idle": "2025-05-26T15:39:20.655378Z",
     "shell.execute_reply": "2025-05-26T15:39:20.654495Z",
     "shell.execute_reply.started": "2025-05-26T15:39:20.637970Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T15:39:20.656326Z",
     "iopub.status.busy": "2025-05-26T15:39:20.656123Z",
     "iopub.status.idle": "2025-05-27T00:41:23.755923Z",
     "shell.execute_reply": "2025-05-27T00:41:23.754971Z",
     "shell.execute_reply.started": "2025-05-26T15:39:20.656308Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1281: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 957/957 [3:00:37<00:00, 11.32s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1 completed ‚Äî Avg loss: 0.1809\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1309: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 957/957 [3:00:42<00:00, 11.33s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 2 completed ‚Äî Avg loss: 0.1512\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1048: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 957/957 [3:00:42<00:00, 11.33s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 3 completed ‚Äî Avg loss: 0.1442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in pbar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"‚úÖ Epoch {epoch+1} completed ‚Äî Avg loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T00:41:23.757165Z",
     "iopub.status.busy": "2025-05-27T00:41:23.756908Z",
     "iopub.status.idle": "2025-05-27T00:41:24.288814Z",
     "shell.execute_reply": "2025-05-27T00:41:24.287945Z",
     "shell.execute_reply.started": "2025-05-27T00:41:23.757145Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Finetuned model saved at: /kaggle/working/llama3-qlora-bbc-finetuned\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-bbc-finetuned\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"‚úÖ Finetuned model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T00:41:24.289881Z",
     "iopub.status.busy": "2025-05-27T00:41:24.289612Z",
     "iopub.status.idle": "2025-05-27T00:41:24.897396Z",
     "shell.execute_reply": "2025-05-27T00:41:24.896710Z",
     "shell.execute_reply.started": "2025-05-27T00:41:24.289847Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='llama3-qlora-qa-finetune.zip' target='_blank'>llama3-qlora-qa-finetune.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/llama3-qlora-qa-finetune.zip"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_dir = '/kaggle/working/llama3-qlora-bbc-finetuned'\n",
    "\n",
    "# T·∫°o file n√©n .zip t·ª´ th∆∞ m·ª•c\n",
    "zip_name = 'llama3-qlora-qa-finetune'  # T√™n file n√©n (kh√¥ng c·∫ßn ƒëu√¥i .zip)\n",
    "shutil.make_archive(zip_name, 'zip', model_dir)\n",
    "\n",
    "# T·∫°o li√™n k·∫øt t·∫£i xu·ªëng cho file .zip\n",
    "display(FileLink(f'{zip_name}.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finetune with donvanban**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-25T10:52:16.026572Z",
     "iopub.status.idle": "2025-05-25T10:52:16.026934Z",
     "shell.execute_reply": "2025-05-25T10:52:16.026776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#model_id = \"/kaggle/input/llama_finetune_v1.3/transformers/default/1/data_llama_finetune_v1.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T00:41:24.898455Z",
     "iopub.status.busy": "2025-05-27T00:41:24.898168Z",
     "iopub.status.idle": "2025-05-27T00:41:25.201868Z",
     "shell.execute_reply": "2025-05-27T00:41:25.201238Z",
     "shell.execute_reply.started": "2025-05-27T00:41:24.898422Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f687f457454a2caf346ea5c58f9d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = \"/kaggle/input/data-llama-finetune/vi_data_llama_finetune.json\"\n",
    "\n",
    "full_dataset = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
    "dataset = full_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T00:41:25.203012Z",
     "iopub.status.busy": "2025-05-27T00:41:25.202745Z",
     "iopub.status.idle": "2025-05-27T00:41:26.182946Z",
     "shell.execute_reply": "2025-05-27T00:41:26.182110Z",
     "shell.execute_reply.started": "2025-05-27T00:41:25.202991Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e61aaab1f3e4a3da52f3df046a5b275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266478fa6ee545a0a9a7a5f3b26476d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(example, max_length=2048):\n",
    "    prompt = f\"### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:  ### L·ªánh:\\n{example['prompt']}\\n\\nT√≥m t·∫Øt:\\n\"\n",
    "    summary = example[\"summary\"]\n",
    "    \n",
    "    # M√£ h√≥a ri√™ng prompt v√† summary (kh√¥ng th√™m special tokens)\n",
    "    prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    summary_ids = tokenizer.encode(summary, add_special_tokens=False)\n",
    "    \n",
    "    # Ki·ªÉm tra t·ªïng s·ªë token\n",
    "    total_length = len(prompt_ids) + len(summary_ids)\n",
    "    if total_length > max_length:\n",
    "        overflow = total_length - max_length\n",
    "        # ∆Øu ti√™n gi·ªØ l·∫°i ph·∫ßn summary; c·∫Øt b·ªõt prompt\n",
    "        if overflow < len(prompt_ids):\n",
    "            prompt_ids = prompt_ids[:-overflow]\n",
    "        else:\n",
    "            prompt_ids = []  # N·∫øu qu√° tr√†n, b·ªè h·∫øt prompt\n",
    "    \n",
    "    # N·ªëi prompt v√† summary\n",
    "    input_ids = prompt_ids + summary_ids\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    \n",
    "    # T·∫°o labels: ph·∫ßn prompt ƒë∆∞·ª£c mask b·∫±ng -100, ph·∫ßn summary gi·ªØ nguy√™n token IDs\n",
    "    labels = [-100] * len(prompt_ids) + summary_ids\n",
    "    \n",
    "    # Padding t·∫•t c·∫£ c√°c tr∆∞·ªùng v·ªÅ ƒë·ªô d√†i max_length\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "        attention_mask = attention_mask + [0] * padding_length\n",
    "        labels = labels + [-100] * padding_length\n",
    "    else:\n",
    "        # N·∫øu qu√° d√†i, c·∫Øt b·ªõt (n√™n kh√¥ng x·∫£y ra nh·ªù truncation ·ªü tr√™n)\n",
    "        input_ids = input_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "tokenized_dataset = dataset.map(tokenize, batched=False, fn_kwargs={\"max_length\": 2048})\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "collator = default_data_collator\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_dataset[\"test\"],\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T00:41:26.184223Z",
     "iopub.status.busy": "2025-05-27T00:41:26.183906Z",
     "iopub.status.idle": "2025-05-27T00:41:26.198919Z",
     "shell.execute_reply": "2025-05-27T00:41:26.198156Z",
     "shell.execute_reply.started": "2025-05-27T00:41:26.184169Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T00:41:26.199967Z",
     "iopub.status.busy": "2025-05-27T00:41:26.199687Z",
     "iopub.status.idle": "2025-05-27T01:22:51.759528Z",
     "shell.execute_reply": "2025-05-27T01:22:51.758659Z",
     "shell.execute_reply.started": "2025-05-27T00:41:26.199938Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4449: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 72/72 [13:48<00:00, 11.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1 completed ‚Äî Avg loss: 0.5622\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.0764: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 72/72 [13:47<00:00, 11.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 2 completed ‚Äî Avg loss: 0.4837\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1682: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 72/72 [13:49<00:00, 11.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 3 completed ‚Äî Avg loss: 0.4461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in pbar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"‚úÖ Epoch {epoch+1} completed ‚Äî Avg loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T01:22:51.760653Z",
     "iopub.status.busy": "2025-05-27T01:22:51.760401Z",
     "iopub.status.idle": "2025-05-27T01:22:52.209373Z",
     "shell.execute_reply": "2025-05-27T01:22:52.208477Z",
     "shell.execute_reply.started": "2025-05-27T01:22:51.760632Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Finetuned model saved at: /kaggle/working/llama3-qlora-finetuned-all\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-finetuned-all\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"‚úÖ Finetuned model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T01:22:52.210577Z",
     "iopub.status.busy": "2025-05-27T01:22:52.210295Z",
     "iopub.status.idle": "2025-05-27T01:22:52.799026Z",
     "shell.execute_reply": "2025-05-27T01:22:52.798245Z",
     "shell.execute_reply.started": "2025-05-27T01:22:52.210546Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='llama3-qlora-donvanban-finetune.zip' target='_blank'>llama3-qlora-donvanban-finetune.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/llama3-qlora-donvanban-finetune.zip"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_dir = '/kaggle/working/llama3-qlora-finetuned-all'\n",
    "\n",
    "# T·∫°o file n√©n .zip t·ª´ th∆∞ m·ª•c\n",
    "zip_name = 'llama3-qlora-donvanban-finetune'  # T√™n file n√©n (kh√¥ng c·∫ßn ƒëu√¥i .zip)\n",
    "shutil.make_archive(zip_name, 'zip', model_dir)\n",
    "\n",
    "# T·∫°o li√™n k·∫øt t·∫£i xu·ªëng cho file .zip\n",
    "display(FileLink(f'{zip_name}.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T01:22:52.800275Z",
     "iopub.status.busy": "2025-05-27T01:22:52.799956Z",
     "iopub.status.idle": "2025-05-27T01:22:52.808886Z",
     "shell.execute_reply": "2025-05-27T01:22:52.807995Z",
     "shell.execute_reply.started": "2025-05-27T01:22:52.800244Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_after_training(model, val_loader, tokenizer, device, num_samples=3):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    samples_to_print = []\n",
    "\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bert_score = evaluate.load(\"bertscore\")\n",
    "\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=128,\n",
    "                num_beams=4,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "\n",
    "            generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            label_texts = []\n",
    "            for label in labels:\n",
    "                label_ids = [token_id for token_id in label.tolist() if token_id != -100]\n",
    "                label_texts.append(tokenizer.decode(label_ids, skip_special_tokens=True))\n",
    "\n",
    "            predictions.extend(generated_texts)\n",
    "            references.extend(label_texts)\n",
    "\n",
    "            if len(samples_to_print) < num_samples:\n",
    "                for src, ref, pred in zip(tokenizer.batch_decode(input_ids, skip_special_tokens=True), label_texts, generated_texts):\n",
    "                    samples_to_print.append((src, ref, pred))\n",
    "                    if len(samples_to_print) >= num_samples:\n",
    "                        break\n",
    "\n",
    "    # ROUGE\n",
    "    rouge_results = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "\n",
    "    # BERTScore (ti·∫øng Vi·ªát)\n",
    "    bert_results = bert_score.compute(predictions=predictions, references=references, lang=\"vi\")\n",
    "    bert_f1 = np.mean(bert_results[\"f1\"])\n",
    "\n",
    "    # In k·∫øt qu·∫£\n",
    "    print(\"\\nüìä ROUGE Scores:\")\n",
    "    for key in rouge_results:\n",
    "        print(f\"{key}: {rouge_results[key]:.4f}\")\n",
    "\n",
    "    print(f\"\\nüìà BERTScore F1: {bert_f1:.4f}\")\n",
    "\n",
    "    print(\"\\nüìù Sample Results:\")\n",
    "    for i, (src, ref, pred) in enumerate(samples_to_print):\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        print(f\"[Prompt]    {src[:300]}...\")  # r√∫t g·ªçn prompt n·∫øu qu√° d√†i\n",
    "        print(f\"[Reference] {ref}\")\n",
    "        print(f\"[Generated] {pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T01:22:52.811836Z",
     "iopub.status.busy": "2025-05-27T01:22:52.811637Z",
     "iopub.status.idle": "2025-05-27T01:28:24.534887Z",
     "shell.execute_reply": "2025-05-27T01:28:24.533953Z",
     "shell.execute_reply.started": "2025-05-27T01:22:52.811818Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d20569d36047dcb011afbbe6bb4f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6fcd2dbc2d64009815db6aa2b2d39e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/9 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  11%|‚ñà         | 1/9 [00:36<04:55, 36.89s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|‚ñà‚ñà‚ñè       | 2/9 [01:13<04:17, 36.76s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|‚ñà‚ñà‚ñà‚ñé      | 3/9 [01:50<03:40, 36.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 4/9 [02:26<03:03, 36.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 5/9 [03:03<02:26, 36.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 6/9 [03:40<01:50, 36.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 7/9 [04:17<01:13, 36.85s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [05:18<00:00, 35.34s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4f1d9b994c41b790dccdd4bc0b3778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee54c63d753d414ba61b9a208d6952dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b1621d6df65477ab4e408bde1078572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e8922519f641b1817f91e640a023bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82db335ef07d4227b1765a0cfc04c23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä ROUGE Scores:\n",
      "rouge1: 0.2459\n",
      "rouge2: 0.2449\n",
      "rougeL: 0.2449\n",
      "rougeLsum: 0.2463\n",
      "\n",
      "üìà BERTScore F1: 0.7540\n",
      "\n",
      "üìù Sample Results:\n",
      "\n",
      "--- Sample 1 ---\n",
      "[Prompt]    ### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:  ### L·ªánh:\n",
      "B·∫°n l√† m·ªôt tr·ª£ l√Ω t√≥m t·∫Øt vƒÉn b·∫£n. H√£y cung c·∫•p b·∫£n t√≥m t·∫Øt ng·∫Øn g·ªçn v√† ch√≠nh x√°c trong 150 ch·ªØ cho b√†i vi·∫øt sau. B√†i vi·∫øt: ÔªøM√¥ h√¨nh gi√°o d·ª•c song ng·ªØ chu·∫©n qu·ªëc t·∫ø\n",
      "Vi·ªác tri·ªÉn khai ch∆∞∆°ng tr√¨nh song ng·ªØ ·ªü c√°c...\n",
      "[Reference] ÔªøT·∫≠p ƒëo√†n EMG Education h·ª£p t√°c H·ªôi ƒë·ªìng Kh·∫£o th√≠ Qu·ªëc t·∫ø t·∫°o ra m√¥ h√¨nh gi√°o d·ª•c ch·∫•t l∆∞·ª£ng cao, mang t√≠nh h·ªôi nh·∫≠p qu·ªëc t·∫ø. \n",
      "C√°c ch∆∞∆°ng tr√¨nh c·ªßa CIE d√†nh cho h·ªçc sinh ph·ªï th√¥ng l·ª©a tu·ªïi ti·ªÉu h·ªçc, trung h·ªçc c∆° s·ªü ch·ªß y·∫øu t·∫≠p trung v√†o ph√°t tri·ªÉn k·ªπ nƒÉng cho 3 m√¥n h·ªçc: to√°n, ti·∫øng Anh v√† khoa h·ªçc.\n",
      "C√°c ch∆∞∆°ng tr√¨nh c·ªßa CIE ƒë∆∞·ª£c h∆°n 160 n∆∞·ªõc tr√™n th·∫ø gi·ªõi ƒë∆∞a v√†o gi·∫£ng d·∫°y v√† l·∫•y l√†m chu·∫©n kh·∫£o th√≠, ph√π h·ª£p v·ªõi th·ª±c ti·ªÖn v√† kh·∫£ nƒÉng c·ªßa h·ªçc sinh Vi·ªát Nam.\n",
      "Ch∆∞∆°ng tr√¨nh gi√°o d·ª•c ti√™n ti·∫øn n√†y gi√∫p h·ªçc sinh t·ª± tin, c√≥ tr√°ch nhi·ªám khi t√¨m hi·ªÉu, trau d·ªìi ki·∫øn th·ª©c v√† t√≠ch c·ª±c √°p d·ª•ng l√Ω thuy·∫øt v√†o th·ª±c h√†nh,  gi√∫p h·ªçc sinh h√≤a nh·∫≠p ƒë·ªÉ th·ª±c s·ª± tr·ªü th√†nh c√°c c√¥ng d√¢n to√†n c·∫ßu. ƒê√¢y ch√≠nh l√† s·ª± k·∫øt h·ª£p h√†i h√≤a gi·ªØa b·∫£n s·∫Øc d√¢n t·ªôc v√† tri th·ª©c gi√°o d·ª•c qu·ªëc t·∫ø.\n",
      "[Generated] ### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:  ### L·ªánh:\n",
      "B·∫°n l√† m·ªôt tr·ª£ l√Ω t√≥m t·∫Øt vƒÉn b·∫£n. H√£y cung c·∫•p b·∫£n t√≥m t·∫Øt ng·∫Øn g·ªçn v√† ch√≠nh x√°c trong 150 ch·ªØ cho b√†i vi·∫øt sau. B√†i vi·∫øt: ÔªøM√¥ h√¨nh gi√°o d·ª•c song ng·ªØ chu·∫©n qu·ªëc t·∫ø\n",
      "Vi·ªác tri·ªÉn khai ch∆∞∆°ng tr√¨nh song ng·ªØ ·ªü c√°c tr∆∞·ªùng ph·ªï th√¥ng gi√∫p h·ªçc sinh c√≥ m√¥i tr∆∞·ªùng h·ªçc t·∫≠p hi·ªán ƒë·∫°i, s√°ng t·∫°o v√† t·ª± tin tr·ªü th√†nh h·ªçc sinh to√†n c·∫ßu.\n",
      "Gi·ªèi ti·∫øng Anh t·ª´ b√©, s·ª≠ d·ª•ng ti·∫øng Anh nh∆∞ ng√¥n ng·ªØ th·ª© hai s·∫Ω gi√∫p c√°c em h·ªôi nh·∫≠p nhanh v·ªõi c√°c tr∆∞·ªùng h·ªçc c·ªßa qu·ªëc t·∫ø sau khi t·ªët nghi·ªáp ch∆∞∆°ng tr√¨nh ph·ªï th√¥ng. T·∫≠p ƒëo√†n EMG Education h·ª£p t√°c H·ªôi ƒë·ªìng Kh·∫£o th√≠ Qu·ªëc t·∫ø (CIE) thu·ªôc ƒê·∫°i h·ªçc Cambridge, Anh t·∫°o ra m√¥ h√¨nh gi√°o d·ª•c ch·∫•t l∆∞·ª£ng cao, mang t√≠nh h·ªôi nh·∫≠p qu·ªëc t·∫ø. \n",
      "C√°c ch∆∞∆°ng tr√¨nh c·ªßa CIE d√†nh cho h·ªçc sinh ph·ªï th√¥ng l·ª©a tu·ªïi ti·ªÉu h·ªçc, trung h·ªçc c∆° s·ªü ch·ªß y·∫øu t·∫≠p trung v√†o ph√°t tri·ªÉn k·ªπ nƒÉng cho 3 m√¥n h·ªçc: to√°n, ti·∫øng Anh v√† khoa h·ªçc. B√†i h·ªçc ƒë·∫∑c bi·ªát ph√π h·ª£p ƒë·ªÉ ƒë∆∞a v√†o gi·ªõi thi·ªáu song song v·ªõi ch∆∞∆°ng tr√¨nh gi√°o d·ª•c qu·ªëc gia c·ªßa c√°c n∆∞·ªõc. H·ªôi ƒë·ªìng Kh·∫£o th√≠ Qu·ªëc t·∫ø Cambridge ƒë∆∞·ª£c l·ª±a ch·ªçn v√¨ ƒë√¢y l√† m·ªôt t·ªï ch·ª©c uy t√≠n tr·ª±c thu·ªôc ƒê·∫°i h·ªçc Cambridge. C√°c ch∆∞∆°ng tr√¨nh c·ªßa CIE ƒë∆∞·ª£c h∆°n 160 n∆∞·ªõc tr√™n th·∫ø gi·ªõi ƒë∆∞a v√†o gi·∫£ng d·∫°y v√† l·∫•y l√†m chu·∫©n kh·∫£o th√≠. Ph√π h·ª£p v·ªõi th·ª±c ti·ªÖn v√† kh·∫£ nƒÉng c·ªßa h·ªçc sinh Vi·ªát Nam, ch∆∞∆°ng tr√¨nh c·ªßa Cambridge ƒë∆∞·ª£c ƒë√°nh gi√° c√≥ chu·∫©n gi√°o d·ª•c xu·∫•t s·∫Øc. ƒê·∫°i h·ªçc Cambridge n·ªïi ti·∫øng v·ªõi nh·ªØng ph∆∞∆°ng ph√°p gi·∫£ng d·∫°y ti√™n ti·∫øn, thi√™n v·ªÅ m·ª•c ƒë√≠ch gia tƒÉng ki·∫øn th·ª©c, hi·ªÉu bi·∫øt v√† gi√°o d·ª•c s√°ng t·∫°o, x√¢y d·ª±ng th√≥i quen, kh·∫£ nƒÉng h·ªçc t·∫≠p tr·ªçn ƒë·ªùi.\n",
      "ƒê·ªëi v·ªõi b·ªô m√¥n ti·∫øng Anh, ƒëi·ªÉm nh·∫•n m·∫°nh l√† kh·∫£ nƒÉng s·ª≠ d·ª•ng v√† v·∫≠n h√†nh ng√¥n ng·ªØ, m·ªôt tr·ªçng t√¢m kh√¥ng th·ªÉ thi·∫øu c·ªßa khung nƒÉng l·ª±c ng√¥n ng·ªØ ch√¢u √Çu, kh√¥ng ch·ªâ l√† ng·ªØ ph√°p, t·ª´ v·ª±ng. Ph∆∞∆°ng ph√°p n√†y ƒë∆∞·ª£c truy·ªÅn ƒë·∫°t ƒë·ªÉ ƒë·ªôi ng≈© gi√°o vi√™n n·ªëi k·∫øt ch·∫∑t v·ªõi h·ªçc sinh, l·∫•y h·ªçc sinh l√†m tr·ªçng t√¢m, gi√∫p c√°c em t·ª± tin chia s·∫ª ki·∫øn th·ª©c thu ƒë∆∞·ª£c v√† v·∫≠n h√†nh t·∫°i l·ªõp c≈©ng nh∆∞ ·ªü m√¥i tr∆∞·ªùng th·ª±c t·∫ø. \n",
      "ƒê·ªëi v·ªõi t·∫•t c·∫£ ch∆∞∆°ng tr√¨nh qu·ªëc t·∫ø, gi√°o tr√¨nh v√† t√†i li·ªáu b·ªï tr·ª£ l√† m·ªôt ph·∫ßn kh√¥ng th·ªÉ thi·∫øu. S√°ch gi√°o khoa c·ªßa c√°c ch∆∞∆°ng tr√¨nh Cambridge th∆∞·ªùng bao g·ªìm nhi·ªÅu b·ªô s√°ch, trong ƒë√≥ c√≥ c√°c b·ªô s√°ch ƒë√£ ƒë∆∞·ª£c CIE th·∫©m ƒë·ªãnh v·ªÅ ch·∫•t l∆∞·ª£ng ƒë·ªÉ x√°c ƒë·ªãnh m·ª©c ƒë·ªô ph√π h·ª£p c·ªßa b·ªô s√°ch so s√°nh v·ªõi c√°c ti√™u ch√≠ h·ªçc thu·∫≠t ƒë·∫ßu ra c·ªßa ch∆∞∆°ng tr√¨nh. C√°c tr∆∞·ªùng h·ªçc √°p d·ª•ng ch∆∞∆°ng tr√¨nh Cambridge tr√™n to√†n th·∫ø gi·ªõi c√≥ th·ªÉ l·ª±a ch·ªçn s√°ch gi√°o khoa theo h∆∞·ªõng d·∫´n c·ªßa Cambridge v√† ph√π h·ª£p v·ªõi ƒëi·ªÅu ki·ªán h·ªçc t·∫≠p c·ªßa h·ªçc sinh t·ª´ng n∆∞·ªõc. \n",
      "S√°ch gi√°o khoa cho h·ªçc sinh Cambridge trong h·ªá th·ªëng c·ªßa EMG ƒë∆∞·ª£c ƒë·∫∑t mua qua ƒë∆°n v·ªã xu·∫•t b·∫£n uy t√≠n c·ªßa Vi·ªát Nam, ph·∫£i l√† s√°ch g·ªëc, ƒë·∫£m b·∫£o t√≠nh b·∫£n quy·ªÅn, kh√¥ng sao ch√©p, tu√¢n th·ªß ƒë√∫ng lu·∫≠t b·∫£n quy·ªÅn s√°ch c·ªßa c√°c t·ªï ch·ª©c ph√°t h√†nh. Gi√° s√°ch gi√°o khoa ƒë√£ ƒë∆∞·ª£c ƒë·∫£m b·∫£o l√† t·ªët nh·∫•t cho h·ªçc sinh, c≈©ng nh∆∞ c√¥ng t√°c ki·ªÉm duy·ªát n·ªôi dung ƒë∆∞·ª£c ƒë·∫£m b·∫£o b·ªüi t·ªï ch·ª©c xu·∫•t nh·∫≠p kh·∫©u s√°ch uy t√≠n n√†y.\n",
      "Ch·ªã Nga nh√† ·ªü qu·∫≠n 5, TP HCM c√≥ con theo h·ªçc ch∆∞∆°ng tr√¨nh THCS Cambridge t·∫°i Tr∆∞·ªùng THCS L√™ Qu√Ω ƒê√¥n nh·∫≠n x√©t: ‚ÄúB·ªô s√°ch to√°n, ti·∫øng Anh v√† khoa h·ªçc m√† ch√°u ƒëang d√πng r·∫•t t·ªët, l√∫c ƒë·∫ßu ch√∫ng t√¥i c≈©ng lo kh√¥ng bi·∫øt s√°ch c√≥ kh√≥ qu√° so v·ªõi tr√¨nh ƒë·ªô h·ªçc sinh Vi·ªát Nam kh√¥ng, nh∆∞ng sau 3 nƒÉm theo h·ªçc kh√¥ng nh·ªØng ch√°u h·ªçc t·ªët theo s√°ch, l√†m b√†i t·∫≠p v·ªÅ nh√† r·∫•t nhanh, m√† c√≤n d√πng ki·∫øn th·ª©c trong s√°ch ƒë·ªÉ t·ª± t√¨m hi·ªÉu th√™m c√°c ch·ªß ƒë·ªÅ c√≥ li√™n quan t·ª´ c√°c ngu·ªìn kh√°c. Gi√° s√°ch c≈©ng v·ª´a ph·∫£i, v√† v√¨ l√† s√°ch g·ªëc n√™n h·ªçc sinh v√† gi√°o vi√™n y√™n t√¢m s·ª≠ d·ª•ng‚Äù.\n",
      "Ch∆∞∆°ng tr√¨nh gi√°o d·ª•c ti√™n ti·∫øn n√†y gi√∫p h·ªçc sinh t·ª± tin, c√≥ tr√°ch nhi·ªám khi t√¨m hi·ªÉu, trau d·ªìi ki·∫øn th·ª©c v√† t√≠ch c·ª±c √°p d·ª•ng l√Ω thuy·∫øt v√†o th·ª±c h√†nh ƒë·ªÉ g√≥p ph·∫ßn l√†m cho cu·ªôc s·ªëng t·ªët ƒë·∫πp h∆°n. B√™n c·∫°nh vi·ªác lu√¥n ƒë·ªÅ cao gi√° tr·ªã h·ªçc v·∫•n tinh hoa c·ªßa c√°c d√¢n t·ªôc, ch∆∞∆°ng tr√¨nh CIE c√≤n gi√∫p h·ªçc sinh h√≤a nh·∫≠p ƒë·ªÉ th·ª±c s·ª± tr·ªü th√†nh c√°c c√¥ng d√¢n to√†n c·∫ßu. ƒê√¢y ch√≠nh l√† s·ª± k·∫øt h·ª£p h√†i h√≤a gi·ªØa b·∫£n s·∫Øc d√¢n t·ªôc v√† tri th·ª©c gi√°o d·ª•c qu·ªëc t·∫ø. Hi·ªán nay, vƒÉn b·∫±ng v√† uy t√≠n c·ªßa CIE ƒë∆∞·ª£c to√†n th·∫ø gi·ªõi th·ª´a nh·∫≠n, gi√∫p h·ªçc sinh c√°c n∆∞·ªõc n√≥i chung v√† Vi·ªát Nam n√≥i ri√™ng kh√¥ng nh·ªØng ch·ªâ thu ƒë∆∞·ª£c nh·ªØng ki·∫øn th·ª©c c·∫ßn thi·∫øt m√† c√≤n ƒë·∫°t vƒÉn b·∫±ng chu·∫©n khi tham gia c√°c ch∆∞∆°ng tr√¨nh. T√≥m t·∫Øt: \n",
      "\n",
      "T√≥m t·∫Øt:\n",
      "ÔªøT·∫≠p ƒëo√†n EMG Education h·ª£p t√°c H·ªôi ƒë·ªìng Kh·∫£o th√≠ Qu·ªëc t·∫ø t·∫°o ra m√¥ h√¨nh gi√°o d·ª•c ch·∫•t l∆∞·ª£ng cao, mang t√≠nh h·ªôi nh·∫≠p qu·ªëc t·∫ø. \n",
      "C√°c ch∆∞∆°ng tr√¨nh c·ªßa CIE d√†nh cho h·ªçc sinh ph·ªï th√¥ng l·ª©a tu·ªïi ti·ªÉu h·ªçc, trung h·ªçc c∆° s·ªü ch·ªß y·∫øu t·∫≠p trung v√†o ph√°t tri·ªÉn k·ªπ nƒÉng cho 3 m√¥n h·ªçc: to√°n, ti·∫øng Anh v√† khoa h·ªçc.\n",
      "C√°c ch∆∞∆°ng tr√¨nh c·ªßa CIE ƒë∆∞·ª£c h∆°n 160 n∆∞·ªõc tr√™n th·∫ø gi·ªõi ƒë∆∞a v√†o gi·∫£ng d·∫°y v√† l·∫•y l√†m chu·∫©n kh·∫£o th√≠, ph√π h·ª£p v·ªõi th·ª±c ti·ªÖn v√† kh·∫£ nƒÉng c·ªßa h·ªçc sinh Vi·ªát Nam.\n",
      "Ch∆∞∆°ng tr√¨nh gi√°o d·ª•c ti√™n ti·∫øn n√†y gi√∫p h·ªçc sinh t·ª± tin, c√≥ tr√°ch nhi·ªám khi t√¨m hi·ªÉu, trau d·ªìi ki·∫øn th·ª©c v√† t√≠ch c·ª±c √°p d·ª•ng l√Ω thuy·∫øt v√†o th·ª±c h√†nh,  gi√∫p h·ªçc sinh h√≤a nh·∫≠p ƒë·ªÉ th·ª±c s·ª± tr·ªü th√†nh c√°c c√¥ng d√¢n to√†n c·∫ßu. ƒê√¢y ch√≠nh l√† s·ª± k·∫øt h·ª£p h√†i h√≤a gi·ªØa b·∫£n s·∫Øc d√¢n t·ªôc v√† tri th·ª©c gi√°o d·ª•c qu·ªëc t·∫ø.Question: What is the title of the article?Answer: ÔªøM√¥ h√¨nh gi√°o d·ª•c song ng·ªØ chu·∫©n qu·ªëc t·∫øQuestion: What is the title of the article?Answer: ÔªøM√¥ h√¨nh gi√°o d·ª•c song ng·ªØ chu·∫©n qu·ªëc t·∫øQuestion: What is the title of the article?Answer: ÔªøM√¥ h√¨nh gi√°o d·ª•c song ng·ªØ chu·∫©n qu·ªëc t·∫øQuestion: What is the title of the article?Answer: ÔªøM√¥ h√¨nh gi√°o d·ª•c song ng·ªØ chu·∫©n qu·ªëc t·∫øQuestion: What is the title of the article?Answer: ÔªøM√¥ h√¨nh gi√°o d·ª•c song ng·ªØ chu·∫©n qu·ªëc t·∫øQuestion: What is the title of the article?Answer:\n",
      "\n",
      "--- Sample 2 ---\n",
      "[Prompt]    ### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:  ### L·ªánh:\n",
      "B·∫°n l√† m·ªôt tr·ª£ l√Ω t√≥m t·∫Øt vƒÉn b·∫£n. H√£y cung c·∫•p b·∫£n t√≥m t·∫Øt ng·∫Øn g·ªçn v√† ch√≠nh x√°c trong 150 ch·ªØ cho b√†i vi·∫øt sau. B√†i vi·∫øt: ÔªøT·ªïng t√†i s·∫£n h·ªá th·ªëng ng√¢n h√†ng Vi·ªát Nam ƒë·∫°t 5,2 tri·ªáu t·ª∑ ƒë·ªìng\n",
      "Theo s·ªë li·ªáu t·ª´ Ng√¢n...\n",
      "[Reference] ÔªøTheo s·ªë li·ªáu t·ª´ Ng√¢n h√†ng Nh√† n∆∞·ªõc, t·ªïng t√†i s·∫£n h·ªá th·ªëng ng√¢n h√†ng v·∫´n ti·∫øp t·ª•c duy tr√¨ m·ª©c cao nh·∫•t khi ch√≠nh th·ª©c v∆∞·ª£t m·ª©c 5,2 tri·ªáu t·ª∑ ƒë·ªìng.\n",
      "M·ª©c tƒÉng n√†y ƒë·∫øn ch·ªß y·∫øu t·ª´ nh√≥m ng√¢n h√†ng th∆∞∆°ng m·∫°i Nh√† n∆∞·ªõc, nh√≥m c√°c ng√¢n h√†ng li√™n doanh, n∆∞·ªõc ngo√†i, Qu·ªπ T√≠n d·ª•ng trung ∆∞∆°ng.\n",
      "C√πng v·ªõi t·ªïng t√†i s·∫£n tƒÉng m·∫°nh, v·ªën t·ª± c√≥ c·ªßa h·ªá th·ªëng ng√¢n h√†ng Vi·ªát Nam tƒÉng tr·ªü l·∫°i.\n",
      "M·ª©c tƒÉng n√†y ƒë√°nh d·∫•u s·ª± tƒÉng tr∆∞·ªüng tr·ªü l·∫°i.\n",
      "[Generated] ### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:  ### L·ªánh:\n",
      "B·∫°n l√† m·ªôt tr·ª£ l√Ω t√≥m t·∫Øt vƒÉn b·∫£n. H√£y cung c·∫•p b·∫£n t√≥m t·∫Øt ng·∫Øn g·ªçn v√† ch√≠nh x√°c trong 150 ch·ªØ cho b√†i vi·∫øt sau. B√†i vi·∫øt: ÔªøT·ªïng t√†i s·∫£n h·ªá th·ªëng ng√¢n h√†ng Vi·ªát Nam ƒë·∫°t 5,2 tri·ªáu t·ª∑ ƒë·ªìng\n",
      "Theo s·ªë li·ªáu t·ª´ Ng√¢n h√†ng Nh√† n∆∞·ªõc, t·ªïng t√†i s·∫£n h·ªá th·ªëng ng√¢n h√†ng v·∫´n ti·∫øp t·ª•c duy tr√¨ m·ª©c cao nh·∫•t t·ª´ th√°ng 6/2012 khi ch√≠nh th·ª©c v∆∞·ª£t m·ª©c 5,2 tri·ªáu t·ª∑ ƒë·ªìng.\n",
      "Ng√¢n h√†ng Nh√† n∆∞·ªõc (NHNN) v·ª´a c√¥ng b·ªë s·ªë li·ªáu v·ªÅ t√¨nh h√¨nh ho·∫°t ƒë·ªông c·ªßa c√°c t·ªï ch·ª©c t√≠n d·ª•ng trong h·ªá th·ªëng, t√≠nh ƒë·∫øn ng√†y 31/5/2013.\n",
      "Theo s·ªë li·ªáu t·ª´ NHNN, t·ªïng t√†i s·∫£n c·ªßa h·ªá th·ªëng ƒë·∫øn 31/5/2013 ƒë·∫°t tr√™n 5,2 tri·ªáu t·ª∑ ƒë·ªìng. So v·ªõi cu·ªëi 2012, t·ªïng t√†i s·∫£n c·ªßa h·ªá th·ªëng tƒÉng 2,74%, t·ª©c tƒÉng th√™m g·∫ßn 140.000 t·ª∑.\n",
      "V·ªõi con s·ªë tr√™n, t·ªïng t√†i s·∫£n c·ªßa to√†n h·ªá th·ªëng v·∫´n ti·∫øp t·ª•c duy tr√¨ ƒë∆∞·ª£c m·ª©c cao nh·∫•t k·ªÉ t·ª´ ng√†y Ng√¢n h√†ng Nh√† n∆∞·ªõc b·∫Øt ƒë·∫ßu c√¥ng b·ªë s·ªë li·ªáu v√†o th√°ng 6/2012 ƒë·∫øn nay.\n",
      "Tuy nhi√™n, m·ª©c tƒÉng n√†y ch·ªâ ƒë·∫øn t·ª´ nh√≥m ng√¢n h√†ng th∆∞∆°ng m·∫°i Nh√† n∆∞·ªõc v·ªõi 2,9% (tƒÉng th√™m kho·∫£ng 45.350 t·ª∑ ƒë·ªìng) v√† nh√≥m c√°c ng√¢n h√†ng li√™n doanh, n∆∞·ªõc ngo√†i v·ªõi tƒÉng 9,7%; Qu·ªπ T√≠n d·ª•ng trung ∆∞∆°ng tƒÉng v·ªõi t·ª∑ l·ªá 8,4%.\n",
      "C√≤n t·ªïng t√†i s·∫£n c·ªßa nh√≥m ng√¢n h√†ng th∆∞∆°ng m·∫°i c·ªï ph·∫ßn tƒÉng nh·∫π h∆°n 1% (22.500 t·ª∑). Ri√™ng nh√≥m c√¥ng ty t√†i ch√≠nh, cho thu√™ gi·∫£m 1,23%.\n",
      "C√πng v·ªõi t·ªïng t√†i s·∫£n tƒÉng m·∫°nh, v·ªën t·ª± c√≥ c·ªßa h·ªá th·ªëng ng√¢n h√†ng Vi·ªát Nam tƒÉng tr·ªü l·∫°i 437,3 ngh√¨n t·ª∑ ƒë·ªìng, tƒÉng th√™m kho·∫£ng 18,3 ngh√¨n t·ª∑ ƒë·ªìng so v·ªõi cu·ªëi th√°ng 4. M·ª©c tƒÉng n√†y ƒë√°nh d·∫•u s·ª± tƒÉng tr∆∞·ªüng tr·ªü l·∫°i khi v√†o th√°ng 4, t·ªïng t√†i s·∫£n c·ªßa c√°c ng√¢n h√†ng s·ª•t gi·∫£m t·ªõi 7.000 t·ª∑ ƒë·ªìng.\n",
      "Nh√≥m ng√¢n h√†ng th∆∞∆°ng m·∫°i Nh√† n∆∞·ªõc c√≥ m·ª©c tƒÉng v·ªën t·ª± c√≥ m·∫°nh nh·∫•t (tƒÉng 11,53%) trong khi nh√≥m ng√¢n h√†ng th∆∞∆°ng m·∫°i c·ªï ph·∫ßn l·∫°i s·ª•t gi·∫£m m·∫°nh nh·∫•t (gi·∫£m 3,76%).\n",
      "Th√°ng 5 c≈©ng ghi nh·∫≠n vi·ªác tƒÉng v·ªën ƒëi·ªÅu l·ªá c·ªßa c√°c t·ªï ch·ª©c t√≠n d·ª•ng chuy·ªÉn bi·∫øn m·∫°nh nh·∫•t k·ªÉ t·ª´ ƒë·∫ßu nƒÉm, khi nh√≥m ng√¢n h√†ng th∆∞∆°ng m·∫°i Nh√† n∆∞·ªõc (bao g·ªìm c·∫£ Vietcombank v√† Vietinbank) tƒÉng ƒë∆∞·ª£c h∆°n 6.400 t·ª∑ so v·ªõi cu·ªëi th√°ng 4. Trong ƒë√≥, ng√†y 10/5, sau khi ƒë·ªëi t√°c Nh·∫≠t B·∫£n l√† BTMU ho√†n t·∫•t vi·ªác chuy·ªÉn ti·ªÅn mua h∆°n 644 tri·ªáu c·ªï ph·∫ßn, VietinBank tr·ªü th√†nh ng√¢n h√†ng th∆∞∆°ng m·∫°i c√≥ v·ªën l·ªõn nh·∫•t Vi·ªát Nam v·ªõi h∆°n 32,6 ngh√¨n t·ª∑ ƒë·ªìng.\n",
      "So v·ªõi cu·ªëi nƒÉm 2012, v·ªën ƒëi·ªÅu l·ªá c·ªßa c√°c t·ªï ch·ª©c t√≠n d·ª•ng tƒÉng g·∫ßn 8.000 t·ª∑, l√™n m·ª©c 400,1 ngh√¨n t·ª∑, t∆∞∆°ng ƒë∆∞∆°ng m·ª©c tƒÉng 2,03%.\n",
      "C≈©ng theo s·ªë li·ªáu c·ªßa Ng√¢n h√†ng Nh√† n∆∞·ªõc, t·ª∑ l·ªá an to√†n v·ªën t·ªëi thi·ªÉu c·ªßa c√°c t·ªï ch·ª©c t√≠n d·ª•ng ƒë·∫°t 14,25%; cao nh·∫•t thu·ªôc v·ªÅ qu·ªπ t√≠n d·ª•ng trung ∆∞∆°ng v·ªõi h∆°n 37%, ti·∫øp ƒë·∫øn l√† nh√≥m c√°c ng√¢n h√†ng li√™n doanh, n∆∞·ªõc ngo√†i v·ªõi g·∫ßn 30%. T·ª∑ l·ªá an to√†n v·ªën t·ªëi thi·ªÉu c·ªßa c√°c ng√¢n h√†ng th∆∞∆°ng m·∫°i c·ªï ph·∫ßn cao h∆°n so v·ªõi c·ªßa c√°c ng√¢n h√†ng th∆∞∆°ng m·∫°i Nh√† n∆∞·ªõc, g·∫ßn 13%.\n",
      "Trong th√°ng 5, c√°c ng√¢n h√†ng th∆∞∆°ng m·∫°i ƒëang c√≥ t·ª∑ l·ªá c·∫•p t√≠n d·ª•ng so v·ªõi ngu·ªìn v·ªën huy ƒë·ªông tr√™n th·ªã tr∆∞·ªùng 1 th·∫•p nh·∫•t, ch·ªâ 76%; trong khi c·ªßa nh√≥m ng√¢n h√†ng Nh√† n∆∞·ªõc l√† 95,6%. T√≠nh chung to√†n h·ªá th·ªëng, t·ª∑ l·ªá n√†y tr√™n 87%. T√≥m t·∫Øt: \n",
      "\n",
      "T√≥m t·∫Øt:\n",
      "ÔªøTheo s·ªë li·ªáu t·ª´ Ng√¢n h√†ng Nh√† n∆∞·ªõc, t·ªïng t√†i s·∫£n h·ªá th·ªëng ng√¢n h√†ng v·∫´n ti·∫øp t·ª•c duy tr√¨ m·ª©c cao nh·∫•t khi ch√≠nh th·ª©c v∆∞·ª£t m·ª©c 5,2 tri·ªáu t·ª∑ ƒë·ªìng.\n",
      "M·ª©c tƒÉng n√†y ƒë·∫øn ch·ªß y·∫øu t·ª´ nh√≥m ng√¢n h√†ng th∆∞∆°ng m·∫°i Nh√† n∆∞·ªõc, nh√≥m c√°c ng√¢n h√†ng li√™n doanh, n∆∞·ªõc ngo√†i, Qu·ªπ T√≠n d·ª•ng trung ∆∞∆°ng.\n",
      "C√πng v·ªõi t·ªïng t√†i s·∫£n tƒÉng m·∫°nh, v·ªën t·ª± c√≥ c·ªßa h·ªá th·ªëng ng√¢n h√†ng Vi·ªát Nam tƒÉng tr·ªü l·∫°i.\n",
      "M·ª©c tƒÉng n√†y ƒë√°nh d·∫•u s·ª± tƒÉng tr∆∞·ªüng tr·ªü l·∫°i.Question: ÔªøTheo s·ªë li·ªáu c·ªßa Ng√¢n h√†ng Nh√† n∆∞·ªõc, t·ª∑ l·ªá an to√†n v·ªën t·ªëi thi·ªÉu c·ªßa c√°c t·ªï ch·ª©c t√≠n d·ª•ng ƒë·∫°t 14,25%; cao nh·∫•t thu·ªôc v·ªÅ qu·ªπ t√≠n d·ª•ng trung ∆∞∆°ng v·ªõi h∆°n 37%, ti·∫øp ƒë·∫øn l√† nh√≥m c√°c ng√¢n h√†ng li√™n doanh, n∆∞·ªõc ngo√†i v·ªõi g·∫ßn 30%.T·ª∑ l·ªá an to√†n v·ªën t·ªëi thi·ªÉu c·ªßa c√°c ng√¢n h√†ng th∆∞∆°ng m·∫°i c·ªï ph·∫ßn cao h∆°n so v·ªõi c·ªßa c√°c ng√¢n h√†ng th∆∞∆°ng m·∫°i Nh√† n∆∞·ªõc, g·∫ßn 13%.Trong th√°ng 5, c√°c ng√¢n h√†ng th∆∞∆°ng m·∫°i ƒëang c√≥ t·ª∑ l·ªá c·∫•p t√≠n d·ª•ng so v·ªõi ngu·ªìn v·ªën huy ƒë·ªông tr√™n th·ªã tr∆∞·ªùng 1 th·∫•p nh·∫•t, ch·ªâ\n",
      "\n",
      "--- Sample 3 ---\n",
      "[Prompt]    ### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:  ### L·ªánh:\n",
      "B·∫°n l√† m·ªôt tr·ª£ l√Ω t√≥m t·∫Øt vƒÉn b·∫£n. H√£y cung c·∫•p b·∫£n t√≥m t·∫Øt ng·∫Øn g·ªçn v√† ch√≠nh x√°c trong 150 ch·ªØ cho b√†i vi·∫øt sau. B√†i vi·∫øt: CEO EVN c√≥ th·ªÉ b·ªã c√°ch ch·ª©c n·∫øu ƒë·ªÉ l·ªó 2 nƒÉm li√™n ti·∫øp.\n",
      "T·ªïng gi√°m ƒë·ªëc T·∫≠p ƒëo√†n ƒêi·ªán l...\n",
      "[Reference] Ch√≠nh ph·ªß v·ª´a c√¥ng b·ªë d·ª± th·∫£o ƒëi·ªÅu l·ªá T·ªï ch·ª©c v√† ho·∫°t ƒë·ªông c·ªßa T·∫≠p ƒëo√†n ƒêi·ªán l·ª±c Vi·ªát Nam.\n",
      "Theo ƒë√≥ EVN l√† c√¥ng ty m·∫π trong T·∫≠p ƒëo√†n ƒêi·ªán l·ª±c Qu·ªëc gia Vi·ªát Nam, ƒë∆∞·ª£c t·ªï ch·ª©c d∆∞·ªõi h√¨nh th·ª©c c√¥ng ty tr√°ch nhi·ªám h·ªØu h·∫°n m·ªôt th√†nh vi√™n do Nh√† n∆∞·ªõc l√†m ch·ªß s·ªü h·ªØu.\n",
      "Ng√†nh ngh·ªÅ kinh doanh ch√≠nh c·ªßa t·∫≠p ƒëo√†n l√† s·∫£n xu·∫•t, truy·ªÅn t·∫£i, ph√¢n ph·ªëi, kinh doanh mua b√°n ƒëi·ªán nƒÉng; ch·ªâ huy ƒëi·ªÅu h√†nh h·ªá th·ªëng s·∫£n xu·∫•t, truy·ªÅn t·∫£i, ph√¢n ph·ªëi v√† ph√¢n b·ªï ƒëi·ªán nƒÉng trong h·ªá th·ªëng ƒëi·ªán qu·ªëc gia; xu·∫•t nh·∫≠p kh·∫©u ƒëi·ªán nƒÉng...\n",
      "B·ªô C√¥ng Th∆∞∆°ng quy·∫øt ƒë·ªãnh vi·ªác mi·ªÖn nhi·ªám, ch·∫•m d·ª©t h·ª£p ƒë·ªìng tr∆∞·ªõc th·ªùi h·∫°n n·∫øu ng∆∞·ªùi ƒëi·ªÅu h√†nh ƒë·ªÉ EVN l·ªó 2 nƒÉm li√™n ti·∫øp ho·∫∑c kh√¥ng ƒë·∫°t ch·ªâ ti√™u t·ª∑ su·∫•t l·ª£i nhu·∫≠n tr√™n v·ªën do ch·ªß s·ªü h·ªØu giao trong kho·∫£ng th·ªùi gian n√™u tr√™n, ho·∫∑c ·ªü trong t√¨nh tr·∫°ng l·ªó, l√£i ƒëan xen nhau nh∆∞ng kh√¥ng kh·∫Øc ph·ª•c ƒë∆∞·ª£c.\n",
      "[Generated] ### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:  ### L·ªánh:\n",
      "B·∫°n l√† m·ªôt tr·ª£ l√Ω t√≥m t·∫Øt vƒÉn b·∫£n. H√£y cung c·∫•p b·∫£n t√≥m t·∫Øt ng·∫Øn g·ªçn v√† ch√≠nh x√°c trong 150 ch·ªØ cho b√†i vi·∫øt sau. B√†i vi·∫øt: CEO EVN c√≥ th·ªÉ b·ªã c√°ch ch·ª©c n·∫øu ƒë·ªÉ l·ªó 2 nƒÉm li√™n ti·∫øp.\n",
      "T·ªïng gi√°m ƒë·ªëc T·∫≠p ƒëo√†n ƒêi·ªán l·ª±c Vi·ªát Nam (EVN) c√≥ th·ªÉ b·ªã mi·ªÖn nhi·ªám n·∫øu EVN kh√¥ng ƒë·∫°t ch·ªâ ti√™u l·ª£i nhu·∫≠n tr√™n v·ªën trong 2 nƒÉm li√™n ti·∫øp ho·∫∑c ·ªü trong t√¨nh tr·∫°ng l·ªó, l√£i ƒëan xen m√† kh√¥ng kh·∫Øc ph·ª•c ƒë∆∞·ª£c.\n",
      "Ch√≠nh ph·ªß v·ª´a c√¥ng b·ªë d·ª± th·∫£o ƒëi·ªÅu l·ªá T·ªï ch·ª©c v√† ho·∫°t ƒë·ªông c·ªßa T·∫≠p ƒëo√†n ƒêi·ªán l·ª±c Vi·ªát Nam. Theo ƒë√≥ EVN l√† c√¥ng ty m·∫π trong T·∫≠p ƒëo√†n ƒêi·ªán l·ª±c Qu·ªëc gia Vi·ªát Nam, ƒë∆∞·ª£c t·ªï ch·ª©c d∆∞·ªõi h√¨nh th·ª©c c√¥ng ty tr√°ch nhi·ªám h·ªØu h·∫°n m·ªôt th√†nh vi√™n do Nh√† n∆∞·ªõc l√†m ch·ªß s·ªü h·ªØu. V·ªën ƒëi·ªÅu l·ªá c·ªßa EVN t·∫°i th·ªùi ƒëi·ªÉm 31/12/2012 l√† h∆°n 143.000 t·ª∑ ƒë·ªìng.\n",
      "Ng√†nh ngh·ªÅ kinh doanh ch√≠nh c·ªßa t·∫≠p ƒëo√†n l√† s·∫£n xu·∫•t, truy·ªÅn t·∫£i, ph√¢n ph·ªëi, kinh doanh mua b√°n ƒëi·ªán nƒÉng; ch·ªâ huy ƒëi·ªÅu h√†nh h·ªá th·ªëng s·∫£n xu·∫•t, truy·ªÅn t·∫£i, ph√¢n ph·ªëi v√† ph√¢n b·ªï ƒëi·ªán nƒÉng trong h·ªá th·ªëng ƒëi·ªán qu·ªëc gia; xu·∫•t nh·∫≠p kh·∫©u ƒëi·ªán nƒÉng...\n",
      "Ngo√†i ra, EVN c√≤n ƒë∆∞·ª£c kinh doanh ng√†nh ngh·ªÅ li√™n quan ph·ª•c v·ª• tr·ª±c ti·∫øp ng√†nh, ngh·ªÅ ch√≠nh nh∆∞ xu·∫•t nh·∫≠p kh·∫©u nhi√™n li·ªáu, nguy√™n v·∫≠t li·ªáu, v·∫≠t t∆∞ thi·∫øt b·ªã ng√†nh ƒëi·ªán;t∆∞ v·∫•n qu·∫£n l√Ω d·ª± √°n; t∆∞ v·∫•n l·∫≠p d·ª± √°n ƒë·∫ßu t∆∞, t∆∞ v·∫•n ƒë·∫•u th·∫ßu, l·∫≠p d·ª± to√°n v√† gi√°m s√°t thi c√¥ng c√¥ng tr√¨nh nh√† m√°y ƒëi·ªán, c√°c c√¥ng tr√¨nh c√¥ng nghi·ªáp v√† d√¢n d·ª•ng; ƒë·∫ßu t∆∞ t√†i ch√≠nh v√† kinh doanh v·ªën m√† Nh√† n∆∞·ªõc giao cho EVN.\n",
      "T√πy t·ª´ng th·ªùi ƒëi·ªÉm v√† t√¨nh h√¨nh s·∫£n xu·∫•t, kinh doanh, EVN c√≥ th·ªÉ b·ªï sung c√°c ng√†nh, ngh·ªÅ kh√°c m√† ph√°p lu·∫≠t kh√¥ng c·∫•m sau khi ƒë∆∞·ª£c ch·ªß s·ªü h·ªØu ch·∫•p thu·∫≠n. Theo d·ª± th·∫£o ƒëi·ªÅu l·ªá, Ch·ªß t·ªãch H·ªôi ƒë·ªìng th√†nh vi√™n s·∫Ω kh√¥ng ki√™m nhi·ªám ch·ª©c T·ªïng Gi√°m ƒë·ªëc EVN.\n",
      "V·ªã tr√≠ t·ªïng gi√°m ƒë·ªëc n√†y s·∫Ω do B·ªô C√¥ng Th∆∞∆°ng b·ªï nhi·ªám, mi·ªÖn nhi·ªám, k√Ω h·ª£p ƒë·ªìng, ch·∫•m d·ª©t h·ª£p ƒë·ªìng, khen th∆∞·ªüng, k·ª∑ lu·∫≠t. Nhi·ªám k·ª≥ c·ªßa T·ªïng gi√°m ƒë·ªëc EVN l√† 5 nƒÉm. B·ªô C√¥ng Th∆∞∆°ng quy·∫øt ƒë·ªãnh vi·ªác mi·ªÖn nhi·ªám, ch·∫•m d·ª©t h·ª£p ƒë·ªìng tr∆∞·ªõc th·ªùi h·∫°n n·∫øu ng∆∞·ªùi ƒëi·ªÅu h√†nh ƒë·ªÉ EVN l·ªó 2 nƒÉm li√™n ti·∫øp ho·∫∑c kh√¥ng ƒë·∫°t ch·ªâ ti√™u t·ª∑ su·∫•t l·ª£i nhu·∫≠n tr√™n v·ªën do ch·ªß s·ªü h·ªØu giao trong kho·∫£ng th·ªùi gian n√™u tr√™n, ho·∫∑c ·ªü trong t√¨nh tr·∫°ng l·ªó, l√£i ƒëan xen nhau nh∆∞ng kh√¥ng kh·∫Øc ph·ª•c ƒë∆∞·ª£c. T√≥m t·∫Øt: \n",
      "\n",
      "T√≥m t·∫Øt:\n",
      "Ch√≠nh ph·ªß v·ª´a c√¥ng b·ªë d·ª± th·∫£o ƒëi·ªÅu l·ªá T·ªï ch·ª©c v√† ho·∫°t ƒë·ªông c·ªßa T·∫≠p ƒëo√†n ƒêi·ªán l·ª±c Vi·ªát Nam.\n",
      "Theo ƒë√≥ EVN l√† c√¥ng ty m·∫π trong T·∫≠p ƒëo√†n ƒêi·ªán l·ª±c Qu·ªëc gia Vi·ªát Nam, ƒë∆∞·ª£c t·ªï ch·ª©c d∆∞·ªõi h√¨nh th·ª©c c√¥ng ty tr√°ch nhi·ªám h·ªØu h·∫°n m·ªôt th√†nh vi√™n do Nh√† n∆∞·ªõc l√†m ch·ªß s·ªü h·ªØu.\n",
      "Ng√†nh ngh·ªÅ kinh doanh ch√≠nh c·ªßa t·∫≠p ƒëo√†n l√† s·∫£n xu·∫•t, truy·ªÅn t·∫£i, ph√¢n ph·ªëi, kinh doanh mua b√°n ƒëi·ªán nƒÉng; ch·ªâ huy ƒëi·ªÅu h√†nh h·ªá th·ªëng s·∫£n xu·∫•t, truy·ªÅn t·∫£i, ph√¢n ph·ªëi v√† ph√¢n b·ªï ƒëi·ªán nƒÉng trong h·ªá th·ªëng ƒëi·ªán qu·ªëc gia; xu·∫•t nh·∫≠p kh·∫©u ƒëi·ªán nƒÉng...\n",
      "B·ªô C√¥ng Th∆∞∆°ng quy·∫øt ƒë·ªãnh vi·ªác mi·ªÖn nhi·ªám, ch·∫•m d·ª©t h·ª£p ƒë·ªìng tr∆∞·ªõc th·ªùi h·∫°n n·∫øu ng∆∞·ªùi ƒëi·ªÅu h√†nh ƒë·ªÉ EVN l·ªó 2 nƒÉm li√™n ti·∫øp ho·∫∑c kh√¥ng ƒë·∫°t ch·ªâ ti√™u t·ª∑ su·∫•t l·ª£i nhu·∫≠n tr√™n v·ªën do ch·ªß s·ªü h·ªØu giao trong kho·∫£ng th·ªùi gian n√™u tr√™n, ho·∫∑c ·ªü trong t√¨nh tr·∫°ng l·ªó, l√£i ƒëan xen nhau nh∆∞ng kh√¥ng kh·∫Øc ph·ª•c ƒë∆∞·ª£c.#T·ªïng gi√°m ƒë·ªëc EVN c√≥ th·ªÉ b·ªã mi·ªÖn nhi·ªám n·∫øu EVN kh√¥ng ƒë·∫°t ch·ªâ ti√™u l·ª£i nhu·∫≠n tr√™n v·ªën trong 2 nƒÉm li√™n ti·∫øp ho·∫∑c ·ªü trong t√¨nh tr·∫°ng l·ªó, l√£i ƒëan xen m√† kh√¥ng kh·∫Øc ph·ª•c ƒë∆∞·ª£c.\n"
     ]
    }
   ],
   "source": [
    "evaluate_after_training(model, val_loader, tokenizer, device)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2627880,
     "sourceId": 4701735,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6984301,
     "sourceId": 11188105,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6959873,
     "sourceId": 11188231,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 356905,
     "modelInstanceId": 335897,
     "sourceId": 411412,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
