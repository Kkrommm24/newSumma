{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-07T01:47:37.710863Z",
     "iopub.status.busy": "2025-04-07T01:47:37.710564Z",
     "iopub.status.idle": "2025-04-07T01:47:45.862339Z",
     "shell.execute_reply": "2025-04-07T01:47:45.861435Z",
     "shell.execute_reply.started": "2025-04-07T01:47:37.710838Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install accelerate peft datasets bitsandbytes evaluate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:47:45.863722Z",
     "iopub.status.busy": "2025-04-07T01:47:45.863383Z",
     "iopub.status.idle": "2025-04-07T01:47:51.381050Z",
     "shell.execute_reply": "2025-04-07T01:47:51.380062Z",
     "shell.execute_reply.started": "2025-04-07T01:47:45.863668Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge_score) (2024.2.0)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=4e1d18159e8eb1d871bf02edc15d8fc3dd9b7d9a87f7f84b330785d633431c9a\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import library and load module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:53:24.675556Z",
     "iopub.status.busy": "2025-04-07T01:53:24.675215Z",
     "iopub.status.idle": "2025-04-07T01:53:24.679650Z",
     "shell.execute_reply": "2025-04-07T01:53:24.678947Z",
     "shell.execute_reply.started": "2025-04-07T01:53:24.675525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    default_data_collator,\n",
    ")\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:53:27.033922Z",
     "iopub.status.busy": "2025-04-07T01:53:27.033584Z",
     "iopub.status.idle": "2025-04-07T01:53:27.197763Z",
     "shell.execute_reply": "2025-04-07T01:53:27.197104Z",
     "shell.execute_reply.started": "2025-04-07T01:53:27.033896Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:53:28.729419Z",
     "iopub.status.busy": "2025-04-07T01:53:28.729088Z",
     "iopub.status.idle": "2025-04-07T01:53:28.733427Z",
     "shell.execute_reply": "2025-04-07T01:53:28.732464Z",
     "shell.execute_reply.started": "2025-04-07T01:53:28.729391Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Continue Pretrain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T18:27:05.922419Z",
     "iopub.status.busy": "2025-04-04T18:27:05.922114Z",
     "iopub.status.idle": "2025-04-04T18:27:22.302199Z",
     "shell.execute_reply": "2025-04-04T18:27:22.301258Z",
     "shell.execute_reply.started": "2025-04-04T18:27:05.922390Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3ddc438fbd40e3a9d74db36486215e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b5f23770704a91a064f8617cd758b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d894150b33f14f8fb8cb8fcf7780bb7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b1771c6a5e43fcba1abb049fd2d691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6cf8e6e25084c04b5d5d76ed4b36cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632f920fa05245cf82eb417861059de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "# 4. QLoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"khai2k48/vietnamese_news_17k\", split=\"train\")\n",
    "dataset = dataset.select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(examples[\"content\"], padding=\"max_length\", truncation=True, max_length=2048)\n",
    "    labels = tokenized[\"input_ids\"].copy()\n",
    "    labels = [-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "train_loader = DataLoader(tokenized_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T18:27:35.812567Z",
     "iopub.status.busy": "2025-04-04T18:27:35.812346Z",
     "iopub.status.idle": "2025-04-05T03:19:54.181088Z",
     "shell.execute_reply": "2025-04-05T03:19:54.180125Z",
     "shell.execute_reply.started": "2025-04-04T18:27:35.812547Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/1000 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 1/3: 100%|██████████| 1000/1000 [2:57:23<00:00, 10.64s/it, loss=0.828] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Loss: 1.2615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1000/1000 [2:57:27<00:00, 10.65s/it, loss=1.33]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Loss: 1.1460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1000/1000 [2:57:27<00:00, 10.65s/it, loss=0.825] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Loss: 1.1380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Số epoch\n",
    "n_epochs = 3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Thêm tqdm để hiển thị tiến trình training\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\", leave=True)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        # Đưa batch vào GPU\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backpropagation và cập nhật trọng số\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient Clipping (tránh exploding gradients)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())  # Hiển thị loss từng batch trên tqdm\n",
    "\n",
    "    # Tính loss trung bình của epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} - Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T03:19:54.182368Z",
     "iopub.status.busy": "2025-04-05T03:19:54.182073Z",
     "iopub.status.idle": "2025-04-05T03:19:54.673721Z",
     "shell.execute_reply": "2025-04-05T03:19:54.672683Z",
     "shell.execute_reply.started": "2025-04-05T03:19:54.182345Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pretrained model saved at: /kaggle/working/llama3-qlora-continue-pretrained\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-continued-pretrain\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"✅ Pretrained model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finetune with QA dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T06:02:26.516684Z",
     "iopub.status.busy": "2025-04-06T06:02:26.516365Z",
     "iopub.status.idle": "2025-04-06T06:02:26.520509Z",
     "shell.execute_reply": "2025-04-06T06:02:26.519554Z",
     "shell.execute_reply.started": "2025-04-06T06:02:26.516662Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"/kaggle/input/llama-qa-partially-finetune/transformers/default/1/llama-qa-finetune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T06:02:28.250286Z",
     "iopub.status.busy": "2025-04-06T06:02:28.249853Z",
     "iopub.status.idle": "2025-04-06T06:02:44.587657Z",
     "shell.execute_reply": "2025-04-06T06:02:44.586642Z",
     "shell.execute_reply.started": "2025-04-06T06:02:28.250219Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a38bd19c774c52b3437b347f9226a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc3c85c421648b2a035cf956d3f066f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c915b8f11d3d4d0688a8a637a0b17816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "# 4. QLoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T06:02:46.561139Z",
     "iopub.status.busy": "2025-04-06T06:02:46.560808Z",
     "iopub.status.idle": "2025-04-06T06:02:48.525658Z",
     "shell.execute_reply": "2025-04-06T06:02:48.524879Z",
     "shell.execute_reply.started": "2025-04-06T06:02:46.561110Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/vietnamese-squad/train-v2.0-translated.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    vi_squad_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T06:02:50.555155Z",
     "iopub.status.busy": "2025-04-06T06:02:50.554786Z",
     "iopub.status.idle": "2025-04-06T06:02:52.287125Z",
     "shell.execute_reply": "2025-04-06T06:02:52.286055Z",
     "shell.execute_reply.started": "2025-04-06T06:02:50.555123Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Train samples: 10000\n",
      "📦 Validation samples: 2000\n"
     ]
    }
   ],
   "source": [
    "train_data = vi_squad_data[:10000]\n",
    "\n",
    "# Lấy 2k mẫu tiếp theo cho validation\n",
    "val_data = vi_squad_data[10000:12000]\n",
    "\n",
    "print(f\"📦 Train samples: {len(train_data)}\")\n",
    "print(f\"📦 Validation samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T06:02:54.174874Z",
     "iopub.status.busy": "2025-04-06T06:02:54.174549Z",
     "iopub.status.idle": "2025-04-06T06:02:54.179759Z",
     "shell.execute_reply": "2025-04-06T06:02:54.178756Z",
     "shell.execute_reply.started": "2025-04-06T06:02:54.174848Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Train samples: ['Beyoncé Giselle Knowles-Carter (/ b i gì ɒ n s eɪ / bee-YON-say) (sinh ngày 04 tháng 9 1981) là một ca sĩ, nhạc sĩ, nhà sản xuất thu âm và nữ diễn viên người Mỹ. Sinh ra và lớn lên ở Houston, Texas, cô đã biểu diễn trong các cuộc thi ca hát và nhảy múa khác nhau khi còn nhỏ, và nổi tiếng vào cuối những năm 1990 với tư cách là ca sĩ chính của nhóm nhạc nữ R & B Destiny\\'s Child. Được quản lý bởi cha cô, Mathew Knowles, nhóm đã trở thành một trong những nhóm nhạc nữ bán chạy nhất thế giới mọi thời đại. Sự gián đoạn của họ đã chứng kiến việc phát hành album đầu tay của Beyoncé, Dangerously in Love (2003), giúp cô trở thành một nghệ sĩ solo trên toàn thế giới, giành được năm giải Grammy và có đĩa đơn quán quân Billboard Hot 100 \"Crazy in Love\" và \"Baby Boy\".', 'Beyonce bắt đầu nổi tiếng từ khi nào?', 'Vào cuối những năm 1990']\n"
     ]
    }
   ],
   "source": [
    "print(f\"📦 Train samples: {train_data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T06:02:58.824477Z",
     "iopub.status.busy": "2025-04-06T06:02:58.824111Z",
     "iopub.status.idle": "2025-04-06T06:02:59.031274Z",
     "shell.execute_reply": "2025-04-06T06:02:59.030348Z",
     "shell.execute_reply.started": "2025-04-06T06:02:58.824447Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_list([{\"context\": item[0], \"question\": item[1], \"answer\": item[2]} for item in train_data])\n",
    "val_dataset = Dataset.from_list([{\"context\": item[0], \"question\": item[1], \"answer\": item[2]} for item in val_data])\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": val_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T06:03:00.894526Z",
     "iopub.status.busy": "2025-04-06T06:03:00.894068Z",
     "iopub.status.idle": "2025-04-06T06:03:00.906447Z",
     "shell.execute_reply": "2025-04-06T06:03:00.905652Z",
     "shell.execute_reply.started": "2025-04-06T06:03:00.894486Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. {'context': 'Beyoncé Giselle Knowles-Carter (/ b i gì ɒ n s eɪ / bee-YON-say) (sinh ngày 04 tháng 9 1981) là một ca sĩ, nhạc sĩ, nhà sản xuất thu âm và nữ diễn viên người Mỹ. Sinh ra và lớn lên ở Houston, Texas, cô đã biểu diễn trong các cuộc thi ca hát và nhảy múa khác nhau khi còn nhỏ, và nổi tiếng vào cuối những năm 1990 với tư cách là ca sĩ chính của nhóm nhạc nữ R & B Destiny\\'s Child. Được quản lý bởi cha cô, Mathew Knowles, nhóm đã trở thành một trong những nhóm nhạc nữ bán chạy nhất thế giới mọi thời đại. Sự gián đoạn của họ đã chứng kiến việc phát hành album đầu tay của Beyoncé, Dangerously in Love (2003), giúp cô trở thành một nghệ sĩ solo trên toàn thế giới, giành được năm giải Grammy và có đĩa đơn quán quân Billboard Hot 100 \"Crazy in Love\" và \"Baby Boy\".', 'question': 'Beyonce bắt đầu nổi tiếng từ khi nào?', 'answer': 'Vào cuối những năm 1990'}\n",
      "2. Beyoncé Giselle Knowles-Carter (/ b i gì ɒ n s eɪ / bee-YON-say) (sinh ngày 04 tháng 9 1981) là một ca sĩ, nhạc sĩ, nhà sản xuất thu âm và nữ diễn viên người Mỹ. Sinh ra và lớn lên ở Houston, Texas, cô đã biểu diễn trong các cuộc thi ca hát và nhảy múa khác nhau khi còn nhỏ, và nổi tiếng vào cuối những năm 1990 với tư cách là ca sĩ chính của nhóm nhạc nữ R & B Destiny's Child. Được quản lý bởi cha cô, Mathew Knowles, nhóm đã trở thành một trong những nhóm nhạc nữ bán chạy nhất thế giới mọi thời đại. Sự gián đoạn của họ đã chứng kiến việc phát hành album đầu tay của Beyoncé, Dangerously in Love (2003), giúp cô trở thành một nghệ sĩ solo trên toàn thế giới, giành được năm giải Grammy và có đĩa đơn quán quân Billboard Hot 100 \"Crazy in Love\" và \"Baby Boy\".\n",
      "3. Beyonce bắt đầu nổi tiếng từ khi nào?\n",
      "4. Vào cuối những năm 1990\n"
     ]
    }
   ],
   "source": [
    "print(f\"1. {dataset_dict['train'][0]}\")\n",
    "print(f\"2. {dataset_dict['train'][0]['context']}\")\n",
    "print(f\"3. {dataset_dict['train'][0]['question']}\")\n",
    "print(f\"4. {dataset_dict['train'][0]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T06:03:02.800024Z",
     "iopub.status.busy": "2025-04-06T06:03:02.799737Z",
     "iopub.status.idle": "2025-04-06T06:03:22.119544Z",
     "shell.execute_reply": "2025-04-06T06:03:22.118626Z",
     "shell.execute_reply.started": "2025-04-06T06:03:02.800004Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f76e2bec13b4197a9930549d1273aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110c5c7c141249d58d8a25166107de50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_qa(example, max_length=512):\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"]\n",
    "    prompt = f\"### Đây là dạng câu hỏi và trả lời dựa trên nội dung ###\\n\\nCâu hỏi: {question}\\n\\nNội dung: {context}\\n\\nTrả lời:\"\n",
    "    completion = f\" {answer}\"\n",
    "    \n",
    "    prompt_ids = tokenizer(prompt, add_special_tokens=False).input_ids\n",
    "    completion_ids = tokenizer(completion, add_special_tokens=False).input_ids\n",
    "\n",
    "    input_ids = prompt_ids + completion_ids\n",
    "\n",
    "    labels = [-100] * len(prompt_ids) + completion_ids\n",
    "\n",
    "    if len(input_ids) > max_length:\n",
    "        input_ids = input_ids[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    \n",
    "    input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "    attention_mask = attention_mask + [0] * padding_length\n",
    "    labels = labels + [-100] * padding_length\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids),\n",
    "        \"attention_mask\": torch.tensor(attention_mask),\n",
    "        \"labels\": torch.tensor(labels)\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset_dict.map(preprocess_qa)\n",
    "\n",
    "tokenized_dataset.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T06:03:24.476077Z",
     "iopub.status.busy": "2025-04-06T06:03:24.475786Z",
     "iopub.status.idle": "2025-04-06T06:03:24.480534Z",
     "shell.execute_reply": "2025-04-06T06:03:24.479729Z",
     "shell.execute_reply.started": "2025-04-06T06:03:24.476056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "collator = default_data_collator\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"], batch_size=4, shuffle=True, collate_fn=collator\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_dataset[\"test\"], batch_size=4, shuffle=False, collate_fn=collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T06:03:33.414082Z",
     "iopub.status.busy": "2025-04-06T06:03:33.413767Z",
     "iopub.status.idle": "2025-04-06T13:02:16.682858Z",
     "shell.execute_reply": "2025-04-06T13:02:16.681821Z",
     "shell.execute_reply.started": "2025-04-06T06:03:33.414057Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:   0%|          | 0/2500 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 1/2: 100%|██████████| 2500/2500 [3:15:37<00:00,  4.69s/it, loss=0.853]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1/2 - Train Loss: 1.6862\n",
      "📊 Validation Loss: 1.4126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 2500/2500 [3:15:50<00:00,  4.70s/it, loss=0.749]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2/2 - Train Loss: 1.3846\n",
      "📊 Validation Loss: 1.2764\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "n_epochs = 2\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\", leave=True)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"✅ Epoch {epoch+1}/{n_epochs} - Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"📊 Validation Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T13:12:33.793626Z",
     "iopub.status.busy": "2025-04-06T13:12:33.793300Z",
     "iopub.status.idle": "2025-04-06T13:12:34.387391Z",
     "shell.execute_reply": "2025-04-06T13:12:34.386338Z",
     "shell.execute_reply.started": "2025-04-06T13:12:33.793600Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pretrained model saved at: /kaggle/working/llama3-qlora-qa-finetune\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-qa-finetune\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"✅ Pretrained model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T13:20:00.418325Z",
     "iopub.status.busy": "2025-04-06T13:20:00.417933Z",
     "iopub.status.idle": "2025-04-06T13:20:01.193743Z",
     "shell.execute_reply": "2025-04-06T13:20:01.192933Z",
     "shell.execute_reply.started": "2025-04-06T13:20:00.418294Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='llama3-qlora-qa-finetune.zip' target='_blank'>llama3-qlora-qa-finetune.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/llama3-qlora-qa-finetune.zip"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import shutil\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# Đường dẫn tới thư mục chứa mô hình\n",
    "model_dir = '/kaggle/working/llama3-qlora-qa-finetune'\n",
    "\n",
    "# Tạo file nén .zip từ thư mục\n",
    "zip_name = 'llama3-qlora-qa-finetune'  # Tên file nén (không cần đuôi .zip)\n",
    "shutil.make_archive(zip_name, 'zip', model_dir)\n",
    "\n",
    "# Tạo liên kết tải xuống cho file .zip\n",
    "display(FileLink(f'{zip_name}.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finetune with BBC News**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:53:12.711554Z",
     "iopub.status.busy": "2025-04-07T01:53:12.711217Z",
     "iopub.status.idle": "2025-04-07T01:53:12.715189Z",
     "shell.execute_reply": "2025-04-07T01:53:12.714226Z",
     "shell.execute_reply.started": "2025-04-07T01:53:12.711532Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"/kaggle/input/llama-qa-finetune/transformers/default/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:53:47.862407Z",
     "iopub.status.busy": "2025-04-07T01:53:47.862108Z",
     "iopub.status.idle": "2025-04-07T01:54:02.110257Z",
     "shell.execute_reply": "2025-04-07T01:54:02.109201Z",
     "shell.execute_reply.started": "2025-04-07T01:53:47.862383Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ffe6cba8cf34f67887fa2783e3a15b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78322ab504124d75b4e7186ef1bb5688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe8060324fe4274b9b5db516661ee6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "# 4. QLoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:54:06.073087Z",
     "iopub.status.busy": "2025-04-07T01:54:06.072743Z",
     "iopub.status.idle": "2025-04-07T01:54:06.078998Z",
     "shell.execute_reply": "2025-04-07T01:54:06.078190Z",
     "shell.execute_reply.started": "2025-04-07T01:54:06.073062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:54:56.559889Z",
     "iopub.status.busy": "2025-04-07T01:54:56.559521Z",
     "iopub.status.idle": "2025-04-07T01:54:57.085726Z",
     "shell.execute_reply": "2025-04-07T01:54:57.084876Z",
     "shell.execute_reply.started": "2025-04-07T01:54:56.559857Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428546799fbc4339b807623dc3909b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = \"/kaggle/input/data-llama-finetune/bbc_data_llama_finetune.json\"\n",
    "\n",
    "full_dataset = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
    "dataset = full_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:55:00.656526Z",
     "iopub.status.busy": "2025-04-07T01:55:00.656189Z",
     "iopub.status.idle": "2025-04-07T01:55:07.826029Z",
     "shell.execute_reply": "2025-04-07T01:55:07.825016Z",
     "shell.execute_reply.started": "2025-04-07T01:55:00.656498Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19802598833f476c84da88a5a8e4a8c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1914 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26779e63ff524362ad6c61cc71403372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/213 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(example, max_length=2048):\n",
    "    prompt = f\"### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Prompt:\\n{example['prompt']}\\n\\n### Summary:\\n\"\n",
    "    summary = example[\"summary\"]\n",
    "    prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    summary_ids = tokenizer.encode(summary, add_special_tokens=False)\n",
    "    \n",
    "    # Kiểm tra tổng số token\n",
    "    total_length = len(prompt_ids) + len(summary_ids)\n",
    "    if total_length > max_length:\n",
    "        overflow = total_length - max_length\n",
    "        if overflow < len(prompt_ids):\n",
    "            prompt_ids = prompt_ids[:-overflow]\n",
    "        else:\n",
    "            prompt_ids = []\n",
    "    input_ids = prompt_ids + summary_ids\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    labels = [-100] * len(prompt_ids) + summary_ids\n",
    "\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "        attention_mask = attention_mask + [0] * padding_length\n",
    "        labels = labels + [-100] * padding_length\n",
    "    else:\n",
    "        input_ids = input_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "tokenized_dataset = dataset.map(tokenize, batched=False, fn_kwargs={\"max_length\": 2048})\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "collator = default_data_collator\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_dataset[\"test\"],\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:55:09.106339Z",
     "iopub.status.busy": "2025-04-07T01:55:09.106017Z",
     "iopub.status.idle": "2025-04-07T01:55:09.124606Z",
     "shell.execute_reply": "2025-04-07T01:55:09.123631Z",
     "shell.execute_reply.started": "2025-04-07T01:55:09.106316Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:55:11.722519Z",
     "iopub.status.busy": "2025-04-07T01:55:11.722159Z",
     "iopub.status.idle": "2025-04-07T10:22:39.183765Z",
     "shell.execute_reply": "2025-04-07T10:22:39.182869Z",
     "shell.execute_reply.started": "2025-04-07T01:55:11.722488Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1446: 100%|██████████| 957/957 [2:49:01<00:00, 10.60s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1 completed — Avg loss: 0.1809\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1074: 100%|██████████| 957/957 [2:49:16<00:00, 10.61s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2 completed — Avg loss: 0.1497\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1099: 100%|██████████| 957/957 [2:49:09<00:00, 10.61s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 3 completed — Avg loss: 0.1431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in pbar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"✅ Epoch {epoch+1} completed — Avg loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T10:22:39.185425Z",
     "iopub.status.busy": "2025-04-07T10:22:39.185130Z",
     "iopub.status.idle": "2025-04-07T10:22:39.827664Z",
     "shell.execute_reply": "2025-04-07T10:22:39.826947Z",
     "shell.execute_reply.started": "2025-04-07T10:22:39.185402Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finetuned model saved at: /kaggle/working/llama3-qlora-finetuned\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-bbc-news-finetuned\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"✅ Finetuned model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T10:22:39.829665Z",
     "iopub.status.busy": "2025-04-07T10:22:39.829375Z",
     "iopub.status.idle": "2025-04-07T10:22:39.840440Z",
     "shell.execute_reply": "2025-04-07T10:22:39.839482Z",
     "shell.execute_reply.started": "2025-04-07T10:22:39.829641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_after_training(model, val_loader, tokenizer, device, num_samples=3):\n",
    "    import evaluate\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    samples_to_print = []\n",
    "\n",
    "    # Load ROUGE\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    def left_pad(inputs, pad_token_id):\n",
    "        \"\"\"Chuyển batch input thành left-padded\"\"\"\n",
    "        max_len = max(len(seq) for seq in inputs)\n",
    "        return torch.stack([\n",
    "            torch.cat([torch.full((max_len - len(seq),), pad_token_id, dtype=torch.long), seq])\n",
    "            for seq in inputs\n",
    "        ])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            # Chuyển sang left padding thủ công\n",
    "            input_ids = left_pad([x[x != tokenizer.pad_token_id] for x in input_ids], tokenizer.pad_token_id).to(device)\n",
    "            attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "\n",
    "            # Generate\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=128,\n",
    "                num_beams=4,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            # Decode\n",
    "            generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "            label_texts = []\n",
    "            for label in labels:\n",
    "                label_ids = [token_id for token_id in label.tolist() if token_id != -100]\n",
    "                label_texts.append(tokenizer.decode(label_ids, skip_special_tokens=True))\n",
    "\n",
    "            predictions.extend(generated_texts)\n",
    "            references.extend(label_texts)\n",
    "\n",
    "            if len(samples_to_print) < num_samples:\n",
    "                src_texts = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "                for src, ref, pred in zip(src_texts, label_texts, generated_texts):\n",
    "                    samples_to_print.append((src, ref, pred))\n",
    "                    if len(samples_to_print) >= num_samples:\n",
    "                        break\n",
    "\n",
    "    # Tính ROUGE\n",
    "    results = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "\n",
    "    print(\"\\n📊 ROUGE Scores:\")\n",
    "    for key in results:\n",
    "        print(f\"{key}: {results[key]:.4f}\")\n",
    "\n",
    "    print(\"\\n📝 Sample Results:\")\n",
    "    for i, (src, ref, pred) in enumerate(samples_to_print):\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        print(f\"[Prompt]    {src}\")\n",
    "        print(f\"[Reference] {ref}\")\n",
    "        print(f\"[Generated] {pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T10:22:39.841789Z",
     "iopub.status.busy": "2025-04-07T10:22:39.841550Z",
     "iopub.status.idle": "2025-04-07T10:53:27.514816Z",
     "shell.execute_reply": "2025-04-07T10:53:27.513776Z",
     "shell.execute_reply.started": "2025-04-07T10:22:39.841769Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d5ba3335144246bf6cc852229f0217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/107 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Evaluating: 100%|██████████| 107/107 [30:23<00:00, 17.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 ROUGE Scores:\n",
      "rouge1: 0.4069\n",
      "rouge2: 0.4053\n",
      "rougeL: 0.4071\n",
      "rougeLsum: 0.4072\n",
      "\n",
      "📝 Sample Results:\n",
      "\n",
      "--- Sample 1 ---\n",
      "[Prompt]    ### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Prompt:\n",
      "You are a helpful summarization assistant. Summarize the following article in 150 words: Campaign 'cold calls' questioned\n",
      "\n",
      "Labour and the Conservatives are still telephoning the millions of people who have signed up to make sure they do not get marketing \"cold calls\".\n",
      "\n",
      "The parties say they can stick to the rules by ensuring that their calls are not marketing - for instance by asking about people's voting intentions. The Lib Dems are asking the watchdog overseeing the rules to stop the calls. The information commissioner's office says surveys are allowed but people had to be told if personal data was kept. Telephone call centres are expected to be used as never before by all the three major parties in the run-up to the general election.\n",
      "\n",
      "But seven million telephone numbers are on the Telephone Preference Service (TPS) lists, which ban unsolicited sales and marketing calls. Both schemes are run by the Direct Marketing Association and backed by EU directives on privacy and electronic communications.\n",
      "\n",
      "The rules on marketing calls apply as much to politicians as to private sector companies. But that does not mean Labour and the Tories are not calling people signed up to the TPS. A Labour Party spokesman told the BBC News website the party avoided those on TPS lists when telephoning people about membership or fundraising. But that did not happen for \"voter identification\" calls. \"When we ask which party they will vote for, that is not marketing and we have very clear legal advice that it is not,\" he said. \"So it is not covered by the Telephone Preference Service.\"\n",
      "\n",
      "He said the party always asked people if they would be happy to be contacted again and if they said no, they were not rung again. A Conservative spokeswoman said the party stuck to the rules when it rang TPS subscribers. She said: \"We do apply TPS but in line with the law. We would not do things that are not allowed in the law.\" Assistant information commissioner Phil Jones said it was classed as marketing if political parties telephoned people to encourage them to vote for them. But \"classic market research\", such as a poll of voter intentions, did not constitute direct marketing, he said. \"If a party is calling someone who is registered on TPS and records their voting intention with a view to using this information in the future, this should be clear to the voter concerned,\" said Mr Jones. \"If a party rings a person who is registered on TPS to ask about their voting intention and goes on to encourage that voter to support them, the party may well be in breach of the regulations. \"In summary, whether a party calling TPS registered voters to check their voting intentions will breach regulations will depend on the script used and whether the script is followed.\" Mr Jones said the watchdog received \"very few complaints\" on the issue.\n",
      "\n",
      "Earlier, Lib Dem chairman Matthew Taylor wrote to the watchdog saying: \"The advice we have received on several previous occasions is that such phone calls are illegal.\" He says evidence from local Lib Dem parties around the country suggests there are \"significant\" numbers of such calls. \"I hope you can therefore take swift and efficient action to ensure that this ceases,\" he tells the commissioner. Mr Taylor argues there should be new guidelines so all parties can act in the same way if the watchdog believes the rules allow parties to ring TPS numbers about voting intentions and later urge those people to vote for them. Summary: \n",
      "\n",
      "### Summary:\n",
      "Assistant information commissioner Phil Jones said it was classed as marketing if political parties telephoned people to encourage them to vote for them.Mr Taylor argues there should be new guidelines so all parties can act in the same way if the watchdog believes the rules allow parties to ring TPS numbers about voting intentions and later urge those people to vote for them.\"If a party is calling someone who is registered on TPS and records their voting intention with a view to using this information in the future, this should be clear to the voter concerned,\" said Mr Jones.\"When we ask which party they will vote for, that is not marketing and we have very clear legal advice that it is not,\" he said.A Conservative spokeswoman said the party stuck to the rules when it rang TPS subscribers.He said the party always asked people if they would be happy to be contacted again and if they said no, they were not rung again.\"If a party rings a person who is registered on TPS to ask about their voting intention and goes on to encourage that voter to support them, the party may well be in breach of the regulations.The parties say they can stick to the rules by ensuring that their calls are not marketing - for instance by asking about people's voting intentions.A Labour Party spokesman told the BBC News website the party avoided those on TPS lists when telephoning people about membership or fundraising.But seven million telephone numbers are on the Telephone Preference Service (TPS) lists, which ban unsolicited sales and marketing calls.She said: \"We do apply TPS but in line with the law.\n",
      "[Reference] Assistant information commissioner Phil Jones said it was classed as marketing if political parties telephoned people to encourage them to vote for them.Mr Taylor argues there should be new guidelines so all parties can act in the same way if the watchdog believes the rules allow parties to ring TPS numbers about voting intentions and later urge those people to vote for them.\"If a party is calling someone who is registered on TPS and records their voting intention with a view to using this information in the future, this should be clear to the voter concerned,\" said Mr Jones.\"When we ask which party they will vote for, that is not marketing and we have very clear legal advice that it is not,\" he said.A Conservative spokeswoman said the party stuck to the rules when it rang TPS subscribers.He said the party always asked people if they would be happy to be contacted again and if they said no, they were not rung again.\"If a party rings a person who is registered on TPS to ask about their voting intention and goes on to encourage that voter to support them, the party may well be in breach of the regulations.The parties say they can stick to the rules by ensuring that their calls are not marketing - for instance by asking about people's voting intentions.A Labour Party spokesman told the BBC News website the party avoided those on TPS lists when telephoning people about membership or fundraising.But seven million telephone numbers are on the Telephone Preference Service (TPS) lists, which ban unsolicited sales and marketing calls.She said: \"We do apply TPS but in line with the law.\n",
      "[Generated] ### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Prompt:\n",
      "You are a helpful summarization assistant. Summarize the following article in 150 words: Campaign 'cold calls' questioned\n",
      "\n",
      "Labour and the Conservatives are still telephoning the millions of people who have signed up to make sure they do not get marketing \"cold calls\".\n",
      "\n",
      "The parties say they can stick to the rules by ensuring that their calls are not marketing - for instance by asking about people's voting intentions. The Lib Dems are asking the watchdog overseeing the rules to stop the calls. The information commissioner's office says surveys are allowed but people had to be told if personal data was kept. Telephone call centres are expected to be used as never before by all the three major parties in the run-up to the general election.\n",
      "\n",
      "But seven million telephone numbers are on the Telephone Preference Service (TPS) lists, which ban unsolicited sales and marketing calls. Both schemes are run by the Direct Marketing Association and backed by EU directives on privacy and electronic communications.\n",
      "\n",
      "The rules on marketing calls apply as much to politicians as to private sector companies. But that does not mean Labour and the Tories are not calling people signed up to the TPS. A Labour Party spokesman told the BBC News website the party avoided those on TPS lists when telephoning people about membership or fundraising. But that did not happen for \"voter identification\" calls. \"When we ask which party they will vote for, that is not marketing and we have very clear legal advice that it is not,\" he said. \"So it is not covered by the Telephone Preference Service.\"\n",
      "\n",
      "He said the party always asked people if they would be happy to be contacted again and if they said no, they were not rung again. A Conservative spokeswoman said the party stuck to the rules when it rang TPS subscribers. She said: \"We do apply TPS but in line with the law. We would not do things that are not allowed in the law.\" Assistant information commissioner Phil Jones said it was classed as marketing if political parties telephoned people to encourage them to vote for them. But \"classic market research\", such as a poll of voter intentions, did not constitute direct marketing, he said. \"If a party is calling someone who is registered on TPS and records their voting intention with a view to using this information in the future, this should be clear to the voter concerned,\" said Mr Jones. \"If a party rings a person who is registered on TPS to ask about their voting intention and goes on to encourage that voter to support them, the party may well be in breach of the regulations. \"In summary, whether a party calling TPS registered voters to check their voting intentions will breach regulations will depend on the script used and whether the script is followed.\" Mr Jones said the watchdog received \"very few complaints\" on the issue.\n",
      "\n",
      "Earlier, Lib Dem chairman Matthew Taylor wrote to the watchdog saying: \"The advice we have received on several previous occasions is that such phone calls are illegal.\" He says evidence from local Lib Dem parties around the country suggests there are \"significant\" numbers of such calls. \"I hope you can therefore take swift and efficient action to ensure that this ceases,\" he tells the commissioner. Mr Taylor argues there should be new guidelines so all parties can act in the same way if the watchdog believes the rules allow parties to ring TPS numbers about voting intentions and later urge those people to vote for them. Summary: \n",
      "\n",
      "### Summary:\n",
      "Assistant information commissioner Phil Jones said it was classed as marketing if political parties telephoned people to encourage them to vote for them.Mr Taylor argues there should be new guidelines so all parties can act in the same way if the watchdog believes the rules allow parties to ring TPS numbers about voting intentions and later urge those people to vote for them.\"If a party is calling someone who is registered on TPS and records their voting intention with a view to using this information in the future, this should be clear to the voter concerned,\" said Mr Jones.\"When we ask which party they will vote for, that is not marketing and we have very clear legal advice that it is not,\" he said.A Conservative spokeswoman said the party stuck to the rules when it rang TPS subscribers.He said the party always asked people if they would be happy to be contacted again and if they said no, they were not rung again.\"If a party rings a person who is registered on TPS to ask about their voting intention and goes on to encourage that voter to support them, the party may well be in breach of the regulations.The parties say they can stick to the rules by ensuring that their calls are not marketing - for instance by asking about people's voting intentions.A Labour Party spokesman told the BBC News website the party avoided those on TPS lists when telephoning people about membership or fundraising.But seven million telephone numbers are on the Telephone Preference Service (TPS) lists, which ban unsolicited sales and marketing calls.She said: \"We do apply TPS but in line with the law.Both schemes are run by the Direct Marketing Association and backed by EU directives on privacy and electronic communications.Both schemes are run by the Direct Marketing Association and backed by EU directives on privacy and electronic communications.\"In summary, whether a party calling TPS registered voters to check their voting intentions will breach regulations will depend on the script used and whether the script is followed.\"Mr Jones said the watchdog received \"very few complaints\" on the issue.\n",
      "\n",
      "--- Sample 2 ---\n",
      "[Prompt]    ### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Prompt:\n",
      "You are a helpful summarization assistant. Summarize the following article in 150 words: Brown hits back in Blair rift row\n",
      "\n",
      "Gordon Brown has criticised a union leader who said conflict between himself and Tony Blair was harming the workings of government.\n",
      "\n",
      "Jonathan Baume, of the top civil servants' union, spoke of \"competing agendas\" between Mr Brown and Mr Blair. But the chancellor said Mr Baume was never at meetings between himself and the prime minister so could not judge. He said the union leader was trying to block civil service reform which threatened his members' jobs. It suited the purpose of Mr Baume's union, the First Division Association, to suggest there were two agendas battling against each other because the union was trying to resist the planned reforms, Mr Brown told BBC Radio 4's Today programme.\n",
      "\n",
      "Under the plans, unveiled in the Gershon report, some 84,000 civil servants jobs will be axed or changed and the savings ploughed back into frontline services. Mr Brown said: \"To be honest I don't think you can rely on his [Mr Baume's] judgement on this matter when it comes to the decisions that the government are making. \"Mr Blair and I are making exactly the same decisions on civil service reforms. We are determined to go on with the Gershon reforms.\" He also said that as Mr Baume was never present at meetings between himself and the prime minister, he was not in a position to judge. On Wednesday, ahead of the Chancellor's pre-Budget report, Mr Baume told BBC News there were sometimes \"conflicting and competing agendas for government\" between Number 10 and the Treasury.\n",
      "\n",
      "What the chancellor wanted was \"not by any means what Alan Milburn and the prime minister want to see\", Mr Baume said. \"Government departments get their money from the Treasury on the basis of public service agreements they sign up to, but at the same time the prime minister also has an agenda and that's not necessarily the same as the Treasury's and the prime minister is of course a very powerful figure in any government. \"He also sends instructions and messages and directions to departments about how he would like each secretary of state and each department to implement a policy agenda. \"The problem is that on many occasions these two don't add up and individual cabinet ministers as well as departments have to make sense of this battle.\" Number 10 said ministers were interested in governing and not a \"soap opera\" about Mr Blair and Mr Brown. Tory shadow chancellor Oliver Letwin said: \"The battle Royal that the top civil servants are now reporting on between the chancellor and Tony Blair is preventing them both from getting on with the business of getting taxpayers value for money.\" Summary: \n",
      "\n",
      "### Summary:\n",
      "But the chancellor said Mr Baume was never at meetings between himself and the prime minister so could not judge.He also said that as Mr Baume was never present at meetings between himself and the prime minister, he was not in a position to judge.Number 10 said ministers were interested in governing and not a \"soap opera\" about Mr Blair and Mr Brown.Jonathan Baume, of the top civil servants' union, spoke of \"competing agendas\" between Mr Brown and Mr Blair.What the chancellor wanted was \"not by any means what Alan Milburn and the prime minister want to see\", Mr Baume said.Mr Brown said: \"To be honest I don't think you can rely on his [Mr Baume's] judgement on this matter when it comes to the decisions that the government are making.\"Mr Blair and I are making exactly the same decisions on civil service reforms.\n",
      "[Reference] But the chancellor said Mr Baume was never at meetings between himself and the prime minister so could not judge.He also said that as Mr Baume was never present at meetings between himself and the prime minister, he was not in a position to judge.Number 10 said ministers were interested in governing and not a \"soap opera\" about Mr Blair and Mr Brown.Jonathan Baume, of the top civil servants' union, spoke of \"competing agendas\" between Mr Brown and Mr Blair.What the chancellor wanted was \"not by any means what Alan Milburn and the prime minister want to see\", Mr Baume said.Mr Brown said: \"To be honest I don't think you can rely on his [Mr Baume's] judgement on this matter when it comes to the decisions that the government are making.\"Mr Blair and I are making exactly the same decisions on civil service reforms.\n",
      "[Generated] ### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Prompt:\n",
      "You are a helpful summarization assistant. Summarize the following article in 150 words: Brown hits back in Blair rift row\n",
      "\n",
      "Gordon Brown has criticised a union leader who said conflict between himself and Tony Blair was harming the workings of government.\n",
      "\n",
      "Jonathan Baume, of the top civil servants' union, spoke of \"competing agendas\" between Mr Brown and Mr Blair. But the chancellor said Mr Baume was never at meetings between himself and the prime minister so could not judge. He said the union leader was trying to block civil service reform which threatened his members' jobs. It suited the purpose of Mr Baume's union, the First Division Association, to suggest there were two agendas battling against each other because the union was trying to resist the planned reforms, Mr Brown told BBC Radio 4's Today programme.\n",
      "\n",
      "Under the plans, unveiled in the Gershon report, some 84,000 civil servants jobs will be axed or changed and the savings ploughed back into frontline services. Mr Brown said: \"To be honest I don't think you can rely on his [Mr Baume's] judgement on this matter when it comes to the decisions that the government are making. \"Mr Blair and I are making exactly the same decisions on civil service reforms. We are determined to go on with the Gershon reforms.\" He also said that as Mr Baume was never present at meetings between himself and the prime minister, he was not in a position to judge. On Wednesday, ahead of the Chancellor's pre-Budget report, Mr Baume told BBC News there were sometimes \"conflicting and competing agendas for government\" between Number 10 and the Treasury.\n",
      "\n",
      "What the chancellor wanted was \"not by any means what Alan Milburn and the prime minister want to see\", Mr Baume said. \"Government departments get their money from the Treasury on the basis of public service agreements they sign up to, but at the same time the prime minister also has an agenda and that's not necessarily the same as the Treasury's and the prime minister is of course a very powerful figure in any government. \"He also sends instructions and messages and directions to departments about how he would like each secretary of state and each department to implement a policy agenda. \"The problem is that on many occasions these two don't add up and individual cabinet ministers as well as departments have to make sense of this battle.\" Number 10 said ministers were interested in governing and not a \"soap opera\" about Mr Blair and Mr Brown. Tory shadow chancellor Oliver Letwin said: \"The battle Royal that the top civil servants are now reporting on between the chancellor and Tony Blair is preventing them both from getting on with the business of getting taxpayers value for money.\" Summary: \n",
      "\n",
      "### Summary:\n",
      "But the chancellor said Mr Baume was never at meetings between himself and the prime minister so could not judge.He also said that as Mr Baume was never present at meetings between himself and the prime minister, he was not in a position to judge.Number 10 said ministers were interested in governing and not a \"soap opera\" about Mr Blair and Mr Brown.Jonathan Baume, of the top civil servants' union, spoke of \"competing agendas\" between Mr Brown and Mr Blair.What the chancellor wanted was \"not by any means what Alan Milburn and the prime minister want to see\", Mr Baume said.Mr Brown said: \"To be honest I don't think you can rely on his [Mr Baume's] judgement on this matter when it comes to the decisions that the government are making.\"Mr Blair and I are making exactly the same decisions on civil service reforms. \"Government departments get their money from the Treasury on the basis of public service agreements they sign up to, but at the same time the prime minister also has an agenda and that's not necessarily the same as the Treasury's and the prime minister is of course a very powerful figure in any government.Tory shadow chancellor Oliver Letwin said: \"The battle Royal that the top civil servants are now reporting on between the chancellor and Tony Blair is preventing them both from getting on with the business of getting taxpayers value for money.\"Under the plans, unveiled in the Gershon report, some 84,000 civil servants jobs will be axed or\n",
      "\n",
      "--- Sample 3 ---\n",
      "[Prompt]    ### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Prompt:\n",
      "You are a helpful summarization assistant. Summarize the following article in 150 words: Mixed reaction to Man Utd offer\n",
      "\n",
      "Shares in Manchester United were up over 5% by noon on Monday following a new offer from Malcolm Glazer.\n",
      "\n",
      "The board of Man Utd is expected to meet early this week to discuss the latest proposal from the US tycoon that values the club at £800m ($1.5bn). Manchester United revealed on Sunday that it had received a detailed proposal from Mr Glazer. A senior source at the club told the BBC: \"This time it's different\". The board is obliged to consider this deal. But the Man Utd supporters club urged the club to reject the new deal. Manchester United past and present footballers Eric Cantona and Ole Gunnar Solskjaer, and club manager Sir Alex Ferguson, have lent their backing to the supporters' group, Shareholders United. They have all spoken out against the bid.\n",
      "\n",
      "A spokesman for the supporters club said: \"I can't see any difference (compared to Mr Glazer's previous proposals) other than £200m less debt. \"He isn't bringing any money into the club; he'll use our money to buy it.\"\n",
      "\n",
      "Mr Glazer's latest move is being led by Mr Glazer's two sons, Avi and Joel, according to the Financial Times. A proposal was received by David Gill, United's chief executive, at the end of last week, pitched at about 300p a share. David Cummings, head of UK equities for Standard Life Investments, said he believed a \"well funded\" 300p a share bid would be enough for Mr Glazer to take control of the club. \"I do not think there is anything that Manchester United fans can do about it,\" he told the BBC. \"They can complain about it but it is curtains for them. They may not want him but they are going to get him.\" The US tycoon, who has been wooing the club for the last 12 months, has approached the United board with \"detailed proposals\", it has confirmed.\n",
      "\n",
      "Mr Glazer, who owns the Tampa Bay Buccaneers team, hopes this will lead to a formal bid being accepted. He is believed to have increased the amount of equity in the new proposal, though it is not clear by how much. For his proposal to succeed, he needs the support of United's largest shareholders, the Irish horseracing tycoons JP McManus and John Magnier. They own 29% of United through their Cubic Expression investment vehicle. Mr Glazer and his family hold a stake of 28.1%. But it is not yet known whether Mr McManus and Mr Magnier would support a Glazer bid. NM Rothschild, the investment bank, is advising Mr Glazer, according to the Financial Times. His previous adviser, JPMorgan, quit last year when Mr Glazer went ahead and voted against the appointment of three United directors to the board, against its advice. But the FT said it thought JP Morgan may still have had some role in financing Mr Glazer's latest financial proposal. Summary: \n",
      "\n",
      "### Summary:\n",
      "Manchester United revealed on Sunday that it had received a detailed proposal from Mr Glazer.But it is not yet known whether Mr McManus and Mr Magnier would support a Glazer bid.David Cummings, head of UK equities for Standard Life Investments, said he believed a \"well funded\" 300p a share bid would be enough for Mr Glazer to take control of the club.But the FT said it thought JP Morgan may still have had some role in financing Mr Glazer's latest financial proposal.His previous adviser, JPMorgan, quit last year when Mr Glazer went ahead and voted against the appointment of three United directors to the board, against its advice.Mr Glazer and his family hold a stake of 28.1%.NM Rothschild, the investment bank, is advising Mr Glazer, according to the Financial Times.Mr Glazer's latest move is being led by Mr Glazer's two sons, Avi and Joel, according to the Financial Times.A spokesman for the supporters club said: \"I can't see any difference (compared to Mr Glazer's previous proposals) other than £200m less debt.The board of Man Utd is expected to meet early this week to discuss the latest proposal from the US tycoon that values the club at £800m ($1.5bn).But the Man Utd supporters club urged the club to reject the new deal.\n",
      "[Reference] Manchester United revealed on Sunday that it had received a detailed proposal from Mr Glazer.But it is not yet known whether Mr McManus and Mr Magnier would support a Glazer bid.David Cummings, head of UK equities for Standard Life Investments, said he believed a \"well funded\" 300p a share bid would be enough for Mr Glazer to take control of the club.But the FT said it thought JP Morgan may still have had some role in financing Mr Glazer's latest financial proposal.His previous adviser, JPMorgan, quit last year when Mr Glazer went ahead and voted against the appointment of three United directors to the board, against its advice.Mr Glazer and his family hold a stake of 28.1%.NM Rothschild, the investment bank, is advising Mr Glazer, according to the Financial Times.Mr Glazer's latest move is being led by Mr Glazer's two sons, Avi and Joel, according to the Financial Times.A spokesman for the supporters club said: \"I can't see any difference (compared to Mr Glazer's previous proposals) other than £200m less debt.The board of Man Utd is expected to meet early this week to discuss the latest proposal from the US tycoon that values the club at £800m ($1.5bn).But the Man Utd supporters club urged the club to reject the new deal.\n",
      "[Generated] ### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Prompt:\n",
      "You are a helpful summarization assistant. Summarize the following article in 150 words: Mixed reaction to Man Utd offer\n",
      "\n",
      "Shares in Manchester United were up over 5% by noon on Monday following a new offer from Malcolm Glazer.\n",
      "\n",
      "The board of Man Utd is expected to meet early this week to discuss the latest proposal from the US tycoon that values the club at £800m ($1.5bn). Manchester United revealed on Sunday that it had received a detailed proposal from Mr Glazer. A senior source at the club told the BBC: \"This time it's different\". The board is obliged to consider this deal. But the Man Utd supporters club urged the club to reject the new deal. Manchester United past and present footballers Eric Cantona and Ole Gunnar Solskjaer, and club manager Sir Alex Ferguson, have lent their backing to the supporters' group, Shareholders United. They have all spoken out against the bid.\n",
      "\n",
      "A spokesman for the supporters club said: \"I can't see any difference (compared to Mr Glazer's previous proposals) other than £200m less debt. \"He isn't bringing any money into the club; he'll use our money to buy it.\"\n",
      "\n",
      "Mr Glazer's latest move is being led by Mr Glazer's two sons, Avi and Joel, according to the Financial Times. A proposal was received by David Gill, United's chief executive, at the end of last week, pitched at about 300p a share. David Cummings, head of UK equities for Standard Life Investments, said he believed a \"well funded\" 300p a share bid would be enough for Mr Glazer to take control of the club. \"I do not think there is anything that Manchester United fans can do about it,\" he told the BBC. \"They can complain about it but it is curtains for them. They may not want him but they are going to get him.\" The US tycoon, who has been wooing the club for the last 12 months, has approached the United board with \"detailed proposals\", it has confirmed.\n",
      "\n",
      "Mr Glazer, who owns the Tampa Bay Buccaneers team, hopes this will lead to a formal bid being accepted. He is believed to have increased the amount of equity in the new proposal, though it is not clear by how much. For his proposal to succeed, he needs the support of United's largest shareholders, the Irish horseracing tycoons JP McManus and John Magnier. They own 29% of United through their Cubic Expression investment vehicle. Mr Glazer and his family hold a stake of 28.1%. But it is not yet known whether Mr McManus and Mr Magnier would support a Glazer bid. NM Rothschild, the investment bank, is advising Mr Glazer, according to the Financial Times. His previous adviser, JPMorgan, quit last year when Mr Glazer went ahead and voted against the appointment of three United directors to the board, against its advice. But the FT said it thought JP Morgan may still have had some role in financing Mr Glazer's latest financial proposal. Summary: \n",
      "\n",
      "### Summary:\n",
      "Manchester United revealed on Sunday that it had received a detailed proposal from Mr Glazer.But it is not yet known whether Mr McManus and Mr Magnier would support a Glazer bid.David Cummings, head of UK equities for Standard Life Investments, said he believed a \"well funded\" 300p a share bid would be enough for Mr Glazer to take control of the club.But the FT said it thought JP Morgan may still have had some role in financing Mr Glazer's latest financial proposal.His previous adviser, JPMorgan, quit last year when Mr Glazer went ahead and voted against the appointment of three United directors to the board, against its advice.Mr Glazer and his family hold a stake of 28.1%.NM Rothschild, the investment bank, is advising Mr Glazer, according to the Financial Times.Mr Glazer's latest move is being led by Mr Glazer's two sons, Avi and Joel, according to the Financial Times.A spokesman for the supporters club said: \"I can't see any difference (compared to Mr Glazer's previous proposals) other than £200m less debt.The board of Man Utd is expected to meet early this week to discuss the latest proposal from the US tycoon that values the club at £800m ($1.5bn).But the Man Utd supporters club urged the club to reject the new deal. \"I do not think there is anything that Manchester United fans can do about it,\" he told the BBC.A senior source at the club told the BBC: \"This time it's different\".A proposal was received by David Gill, United's chief executive, at the end of last week, pitched at about 300p a share.\n"
     ]
    }
   ],
   "source": [
    "evaluate_after_training(model, val_loader, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finetune with donvanban**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T11:40:33.935551Z",
     "iopub.status.busy": "2025-04-07T11:40:33.935187Z",
     "iopub.status.idle": "2025-04-07T11:40:33.939346Z",
     "shell.execute_reply": "2025-04-07T11:40:33.938465Z",
     "shell.execute_reply.started": "2025-04-07T11:40:33.935523Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"/kaggle/working/llama3-qlora-finetuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T11:40:36.835699Z",
     "iopub.status.busy": "2025-04-07T11:40:36.835379Z",
     "iopub.status.idle": "2025-04-07T11:40:41.626707Z",
     "shell.execute_reply": "2025-04-07T11:40:41.626008Z",
     "shell.execute_reply.started": "2025-04-07T11:40:36.835652Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "# 4. QLoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T11:40:41.628209Z",
     "iopub.status.busy": "2025-04-07T11:40:41.627906Z",
     "iopub.status.idle": "2025-04-07T11:40:41.867622Z",
     "shell.execute_reply": "2025-04-07T11:40:41.866760Z",
     "shell.execute_reply.started": "2025-04-07T11:40:41.628186Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/kaggle/input/data-llama-finetune/vi_data_llama_finetune.json\"\n",
    "\n",
    "full_dataset = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
    "dataset = full_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T11:40:41.869383Z",
     "iopub.status.busy": "2025-04-07T11:40:41.869143Z",
     "iopub.status.idle": "2025-04-07T11:40:43.073794Z",
     "shell.execute_reply": "2025-04-07T11:40:43.072670Z",
     "shell.execute_reply.started": "2025-04-07T11:40:41.869362Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce1cf43234143f28baaf98cb230fe6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe7f7a600d045f1b1d33788cce2d272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(example, max_length=2048):\n",
    "    prompt = f\"### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Lệnh:\\n{example['prompt']}\\n\\n### Tóm tắt:\\n\"\n",
    "    summary = example[\"summary\"]\n",
    "    \n",
    "    # Mã hóa riêng prompt và summary (không thêm special tokens)\n",
    "    prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    summary_ids = tokenizer.encode(summary, add_special_tokens=False)\n",
    "    \n",
    "    # Kiểm tra tổng số token\n",
    "    total_length = len(prompt_ids) + len(summary_ids)\n",
    "    if total_length > max_length:\n",
    "        overflow = total_length - max_length\n",
    "        # Ưu tiên giữ lại phần summary; cắt bớt prompt\n",
    "        if overflow < len(prompt_ids):\n",
    "            prompt_ids = prompt_ids[:-overflow]\n",
    "        else:\n",
    "            prompt_ids = []  # Nếu quá tràn, bỏ hết prompt\n",
    "    \n",
    "    # Nối prompt và summary\n",
    "    input_ids = prompt_ids + summary_ids\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    \n",
    "    # Tạo labels: phần prompt được mask bằng -100, phần summary giữ nguyên token IDs\n",
    "    labels = [-100] * len(prompt_ids) + summary_ids\n",
    "    \n",
    "    # Padding tất cả các trường về độ dài max_length\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "        attention_mask = attention_mask + [0] * padding_length\n",
    "        labels = labels + [-100] * padding_length\n",
    "    else:\n",
    "        # Nếu quá dài, cắt bớt (nên không xảy ra nhờ truncation ở trên)\n",
    "        input_ids = input_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "tokenized_dataset = dataset.map(tokenize, batched=False, fn_kwargs={\"max_length\": 2048})\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "collator = default_data_collator\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_dataset[\"test\"],\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T11:40:48.914314Z",
     "iopub.status.busy": "2025-04-07T11:40:48.913997Z",
     "iopub.status.idle": "2025-04-07T11:40:48.930785Z",
     "shell.execute_reply": "2025-04-07T11:40:48.930103Z",
     "shell.execute_reply.started": "2025-04-07T11:40:48.914289Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T11:41:15.929654Z",
     "iopub.status.busy": "2025-04-07T11:41:15.929343Z",
     "iopub.status.idle": "2025-04-07T12:45:29.102807Z",
     "shell.execute_reply": "2025-04-07T12:45:29.101844Z",
     "shell.execute_reply.started": "2025-04-07T11:41:15.929626Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3921: 100%|██████████| 72/72 [12:51<00:00, 10.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1 completed — Avg loss: 0.5270\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4752: 100%|██████████| 72/72 [12:48<00:00, 10.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2 completed — Avg loss: 0.4871\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1806: 100%|██████████| 72/72 [12:50<00:00, 10.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 3 completed — Avg loss: 0.4527\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2761: 100%|██████████| 72/72 [12:51<00:00, 10.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 4 completed — Avg loss: 0.4381\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3524: 100%|██████████| 72/72 [12:50<00:00, 10.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 5 completed — Avg loss: 0.4144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in pbar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"✅ Epoch {epoch+1} completed — Avg loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T12:45:29.104503Z",
     "iopub.status.busy": "2025-04-07T12:45:29.104207Z",
     "iopub.status.idle": "2025-04-07T12:45:29.695624Z",
     "shell.execute_reply": "2025-04-07T12:45:29.694800Z",
     "shell.execute_reply.started": "2025-04-07T12:45:29.104481Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finetuned model saved at: /kaggle/working/llama3-qlora-finetuned-all-1\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-finetuned-all-1\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"✅ Finetuned model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T12:47:47.844662Z",
     "iopub.status.busy": "2025-04-07T12:47:47.844334Z",
     "iopub.status.idle": "2025-04-07T12:47:47.855642Z",
     "shell.execute_reply": "2025-04-07T12:47:47.854781Z",
     "shell.execute_reply.started": "2025-04-07T12:47:47.844633Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_after_training(model, val_loader, tokenizer, device, num_samples=3):\n",
    "    import evaluate\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    samples_to_print = []\n",
    "\n",
    "    # Load ROUGE\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    def left_pad(inputs, pad_token_id):\n",
    "        \"\"\"Chuyển batch input thành left-padded\"\"\"\n",
    "        max_len = max(len(seq) for seq in inputs)\n",
    "        return torch.stack([\n",
    "            torch.cat([torch.full((max_len - len(seq),), pad_token_id, dtype=torch.long), seq])\n",
    "            for seq in inputs\n",
    "        ])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            # Chuyển sang left padding thủ công\n",
    "            input_ids = left_pad([x[x != tokenizer.pad_token_id] for x in input_ids], tokenizer.pad_token_id).to(device)\n",
    "            attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "\n",
    "            # Generate\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=128,\n",
    "                num_beams=4,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            # Decode\n",
    "            generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "            label_texts = []\n",
    "            for label in labels:\n",
    "                label_ids = [token_id for token_id in label.tolist() if token_id != -100]\n",
    "                label_texts.append(tokenizer.decode(label_ids, skip_special_tokens=True))\n",
    "\n",
    "            predictions.extend(generated_texts)\n",
    "            references.extend(label_texts)\n",
    "\n",
    "            if len(samples_to_print) < num_samples:\n",
    "                src_texts = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "                for src, ref, pred in zip(src_texts, label_texts, generated_texts):\n",
    "                    samples_to_print.append((src, ref, pred))\n",
    "                    if len(samples_to_print) >= num_samples:\n",
    "                        break\n",
    "\n",
    "    # Tính ROUGE\n",
    "    results = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "\n",
    "    print(\"\\n📊 ROUGE Scores:\")\n",
    "    for key in results:\n",
    "        print(f\"{key}: {results[key]:.4f}\")\n",
    "\n",
    "    print(\"\\n📝 Sample Results:\")\n",
    "    for i, (src, ref, pred) in enumerate(samples_to_print):\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        print(f\"[Prompt]    {src}\")\n",
    "        print(f\"[Reference] {ref}\")\n",
    "        print(f\"[Generated] {pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T12:47:56.995645Z",
     "iopub.status.busy": "2025-04-07T12:47:56.995358Z",
     "iopub.status.idle": "2025-04-07T12:51:36.623267Z",
     "shell.execute_reply": "2025-04-07T12:51:36.622399Z",
     "shell.execute_reply.started": "2025-04-07T12:47:56.995623Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 9/9 [03:34<00:00, 23.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 ROUGE Scores:\n",
      "rouge1: 0.2992\n",
      "rouge2: 0.2967\n",
      "rougeL: 0.2985\n",
      "rougeLsum: 0.2977\n",
      "\n",
      "📝 Sample Results:\n",
      "\n",
      "--- Sample 1 ---\n",
      "[Prompt]    ### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Lệnh:\n",
      "Bạn là một trợ lý tóm tắt văn bản. Hãy cung cấp bản tóm tắt ngắn gọn và chính xác trong 150 chữ cho bài viết sau. Bài viết: ﻿Tạp chí Mỹ bị lên án vì chụp ảnh \"thời trang tự tử\"\n",
      " Tạp chí Vice của Mỹ vừa bị lên án vì thực hiện bộ ảnh \"thời trang tự tử\", tái hiện lại giây phút cuối đời của những nữ văn sĩ nổi tiếng. Trong những năm gần đây, để cạnh tranh với những báo khác, các tờ tạp chí thường mạnh dạn đưa ra những ý tưởng mới lạ, nhiều khi trở thành quá đà.  Đối với ý tưởng sáng tạo \"thảm họa\" lần này của tạp chí Vice, tờ Huffington Post đánh giá rằng: \"Vice nghĩ họ đang tạo ra những bức ảnh nghệ thuật nhưng cuối cùng người ta chỉ thấy đó là những bức ảnh thiếu suy nghĩ và phản cảm\". Sau khi những bức hình tự tử này bị xóa đi, Vice đã đăng tải lời xin lỗi: \"Những bức ảnh thời trang của Vice luôn phi truyền thống và tiếp cận từ góc nhìn nghệ thuật hơn là những bức ảnh thuần túy giới thiệu xu hướng thời trang. Mục tiêu của chúng tôi là tạo ra những bức hình nghệ thuật để từ đó thông điệp thời trang sẽ đi theo sau, thay vì đi trước như nhiều tờ báo khác. Bộ hình \"Last Words\" (Những lời cuối cùng) được tạo ra theo tư duy này và tập trung vào cái chết của những nữ văn sĩ nổi tiếng mà các độc giả yêu quý họ hẳn đều mong rằng giá như cuộc đời của những tài năng này đừng ngắn ngủi như vậy\". Lời xin lỗi của Vice bị cho là không thuyết phục. Thật khó hiểu tại sao ban biên tập của báo có thể để những bức ảnh này xuất hiện trên trang của mình. Tờ Huffington Post nhận định thêm: Vice đang cố tình giải thích lòng vòng xung quanh việc thực hiện một bộ ảnh vô nghĩa. Đối với một nhà văn, họ được công chúng nhớ tới nhờ những tác phẩm hay. Tại sao Vice không tôn vinh những tác phẩm đó mà lại nhấn mạnh vào những giây phút cuối đời đen tối, cô độc, với đầy nỗi sợ hãi và buồn khổ như vậy? Đối với tất cả những ai từng trải qua tâm trạng bi đát tới mức muốn tự tử hoặc có người thân từng tự tử, hẳn họ sẽ cảm thấy vô cùng khó chịu trước những bức ảnh này. \n",
      "Khoa học đã chứng minh rằng, đối với những ai từng xuất hiện trong đầu ý nghĩ đen tối về việc tự tử, việc báo chí hoặc truyền hình đưa ra những hình ảnh trực quan về hành động này không khác gì nhen nhóm lại ý tưởng đó trong đầu người xem. Đối với những nữ nhà văn mà tờ Vice đề cập tới, cách tiếp cận đề tài của họ bị đánh giá là thiếu tôn trọng người đã khuất. Tóm tắt: \n",
      "\n",
      "### Tóm tắt:\n",
      "﻿Tạp chí Vice của Mỹ  vừa bị lên án vì thực hiện bộ ảnh \"thời trang tự tử\", tái hiện lại giây phút cuối đời của những nữ văn sĩ nổi tiếng. Trong những năm gần dây, các tờ tạp chí thườnng mạnh dạn đưa ra những ý tưởng mới lạ, nhiều khi trở thành quá đà. Sau khi những bức hình tự tử này bị xóa đi, Vice đã đăng tải lời xin lỗi, bị cho là không thuyết phục. Khoa học đã chứng minh, đối với những ai từng xuất hiện trong đầu ý nghĩ đen tối về việc tự tử, việc báo chí hoặc truyền hình đưa ra những hình ảnh trực quan về hành động này không khác gì nhen nhóm lại ý tưởng đó trong đầu người xem. Cách tiếp cận đề tài  bị đánh giá là thiếu tôn trọng người đã khuất.\n",
      "[Reference] ﻿Tạp chí Vice của Mỹ  vừa bị lên án vì thực hiện bộ ảnh \"thời trang tự tử\", tái hiện lại giây phút cuối đời của những nữ văn sĩ nổi tiếng. Trong những năm gần dây, các tờ tạp chí thườnng mạnh dạn đưa ra những ý tưởng mới lạ, nhiều khi trở thành quá đà. Sau khi những bức hình tự tử này bị xóa đi, Vice đã đăng tải lời xin lỗi, bị cho là không thuyết phục. Khoa học đã chứng minh, đối với những ai từng xuất hiện trong đầu ý nghĩ đen tối về việc tự tử, việc báo chí hoặc truyền hình đưa ra những hình ảnh trực quan về hành động này không khác gì nhen nhóm lại ý tưởng đó trong đầu người xem. Cách tiếp cận đề tài  bị đánh giá là thiếu tôn trọng người đã khuất.\n",
      "[Generated] ### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Lệnh:\n",
      "Bạn là một trợ lý tóm tắt văn bản. Hãy cung cấp bản tóm tắt ngắn gọn và chính xác trong 150 chữ cho bài viết sau. Bài viết: ﻿Tạp chí Mỹ bị lên án vì chụp ảnh \"thời trang tự tử\"\n",
      " Tạp chí Vice của Mỹ vừa bị lên án vì thực hiện bộ ảnh \"thời trang tự tử\", tái hiện lại giây phút cuối đời của những nữ văn sĩ nổi tiếng. Trong những năm gần đây, để cạnh tranh với những báo khác, các tờ tạp chí thường mạnh dạn đưa ra những ý tưởng mới lạ, nhiều khi trở thành quá đà.  Đối với ý tưởng sáng tạo \"thảm họa\" lần này của tạp chí Vice, tờ Huffington Post đánh giá rằng: \"Vice nghĩ họ đang tạo ra những bức ảnh nghệ thuật nhưng cuối cùng người ta chỉ thấy đó là những bức ảnh thiếu suy nghĩ và phản cảm\". Sau khi những bức hình tự tử này bị xóa đi, Vice đã đăng tải lời xin lỗi: \"Những bức ảnh thời trang của Vice luôn phi truyền thống và tiếp cận từ góc nhìn nghệ thuật hơn là những bức ảnh thuần túy giới thiệu xu hướng thời trang. Mục tiêu của chúng tôi là tạo ra những bức hình nghệ thuật để từ đó thông điệp thời trang sẽ đi theo sau, thay vì đi trước như nhiều tờ báo khác. Bộ hình \"Last Words\" (Những lời cuối cùng) được tạo ra theo tư duy này và tập trung vào cái chết của những nữ văn sĩ nổi tiếng mà các độc giả yêu quý họ hẳn đều mong rằng giá như cuộc đời của những tài năng này đừng ngắn ngủi như vậy\". Lời xin lỗi của Vice bị cho là không thuyết phục. Thật khó hiểu tại sao ban biên tập của báo có thể để những bức ảnh này xuất hiện trên trang của mình. Tờ Huffington Post nhận định thêm: Vice đang cố tình giải thích lòng vòng xung quanh việc thực hiện một bộ ảnh vô nghĩa. Đối với một nhà văn, họ được công chúng nhớ tới nhờ những tác phẩm hay. Tại sao Vice không tôn vinh những tác phẩm đó mà lại nhấn mạnh vào những giây phút cuối đời đen tối, cô độc, với đầy nỗi sợ hãi và buồn khổ như vậy? Đối với tất cả những ai từng trải qua tâm trạng bi đát tới mức muốn tự tử hoặc có người thân từng tự tử, hẳn họ sẽ cảm thấy vô cùng khó chịu trước những bức ảnh này. \n",
      "Khoa học đã chứng minh rằng, đối với những ai từng xuất hiện trong đầu ý nghĩ đen tối về việc tự tử, việc báo chí hoặc truyền hình đưa ra những hình ảnh trực quan về hành động này không khác gì nhen nhóm lại ý tưởng đó trong đầu người xem. Đối với những nữ nhà văn mà tờ Vice đề cập tới, cách tiếp cận đề tài của họ bị đánh giá là thiếu tôn trọng người đã khuất. Tóm tắt: \n",
      "\n",
      "### Tóm tắt:\n",
      "﻿Tạp chí Vice của Mỹ  vừa bị lên án vì thực hiện bộ ảnh \"thời trang tự tử\", tái hiện lại giây phút cuối đời của những nữ văn sĩ nổi tiếng. Trong những năm gần dây, các tờ tạp chí thườnng mạnh dạn đưa ra những ý tưởng mới lạ, nhiều khi trở thành quá đà. Sau khi những bức hình tự tử này bị xóa đi, Vice đã đăng tải lời xin lỗi, bị cho là không thuyết phục. Khoa học đã chứng minh, đối với những ai từng xuất hiện trong đầu ý nghĩ đen tối về việc tự tử, việc báo chí hoặc truyền hình đưa ra những hình ảnh trực quan về hành động này không khác gì nhen nhóm lại ý tưởng đó trong đầu người xem. Cách tiếp cận đề tài  bị đánh giá là thiếu tôn trọng người đã khuất. Tóm tắt: Tạp chí Vice của Mỹ vừa bị lên án vì thực hiện bộ ảnh \"thời trang tự tử\", tái hiện lại giây phút cuối đời của những nữ văn sĩ nổi tiếng. Trong những năm gần dây, các tờ tạp chí thườnng mạnh dạn đưa ra những ý tưởng mới lạ, nhiều khi trở thành quá đà. Sau khi những bức hình tự tử này bị xóa đi, Vice đã đăng tải lời xin lỗi, bị cho là không thuyết phục. Khoa học đã chứng minh, đối với những ai từng xuất hiện trong đầu ý nghĩ đen tối về việc tự tử, việc báo\n",
      "\n",
      "--- Sample 2 ---\n",
      "[Prompt]    ### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Lệnh:\n",
      "Bạn là một trợ lý tóm tắt văn bản. Hãy cung cấp bản tóm tắt ngắn gọn và chính xác trong 150 chữ cho bài viết sau. Bài viết: ﻿Lốc xoáy, sét đánh nhiều nhà tan tành.\n",
      "Một trận mưa lớn, kèm theo lốc xoáy và những tiếng nổ lớn xảy ra trên địa bàn xã miền núi Quỳ Châu, Nghệ An làm nhiều người dân phát hoảng.\n",
      "Vào khoảng 19h tối qua ( 20/4 ), người ở bản Xốp Hốc, xã Diên Lãm, huyện Quỳ Châu ( Nghệ An ) đang chuẩn bị bữa tối thì trời bắt đầu có mưa kèm theo gió lốc.\n",
      "Cơn mưa nặng hạt và bất ngờ một tiếng nổ lớn vang lên làm rung chuyển cả một khu vực.\n",
      "Cùng lúc đó nhiều người nhìn thấy khói và mùi khét lẹt bốc lên nghi ngút tại 3 căn nhà là nhà anh Hà Văn Chung, Vi Văn Phương và Lương Văn Chung.\n",
      "Sau khi hoàn hồn, mọi người thấy nhà của 3 gia đình nói trên bị hư hỏng nặng, mái nhà lợp bằng phibroximăng bị vỡ vụn rơi xuống đất hàng chục tấm, nhiều đồ đạc trong nhà như tivi, đài, ấm chén và các loại vật dụng khác trong nhà bị hư hỏng hoàn toàn.\n",
      "Dù hầu hết các thành viên của 3 gia đình bị sét đánh hư hỏng nói trên đều đang ở trong nhà nhưng rất may không có ai bị thương.\n",
      "\" Tôi đang lúi húi trong nhà để nấu cơm cho 2 đứa con ăn bữa tối, bất ngờ một tiếng \" sầm \" lớn làm rung chuyển căn nhà sàn.\n",
      "Những mảnh vỡ của tấm lợp phibroximăng rơi tung tóe khắp nhà.\n",
      "Hai đứa con nhỏ khóc thét lên.\n",
      "Rất may 3 mẹ con tôi không việc gì \", chị Vi Thị Kiều, nạn nhân của vụ sét đánh tối qua tại huyện Quỳ Châu chưa hết bàng hoàng kể lại.\n",
      "Ông Lý Đại Châu - Chủ tịch UBND xã Diên Lãm, cho biết : \" Chiều tối qua ( 20/4 ), tại hai xã Châu Hoàn và Diên Lãm có xẩy ra một trận lốc xoáy làm hư hỏng nhiều tài sản, tốc mái nhiều căn nhà.\n",
      "Hiện, chúng tôi đang chỉ đạo anh em văn phòng phối hợp với các trưởng bản tiến hành kiểm tra thống kê và báo cáo thiệt hại lên UBND huyện vào sáng thứ 2 tuần tới.\n",
      "Riêng trường hợp 3 căn nhà ở bản Xốp Hốc bị sét đánh tôi cũng vừa mới nghe qua nhưng chưa rõ tình hình thiệt hại thế nào \". Tóm tắt: \n",
      "\n",
      "### Tóm tắt:\n",
      "﻿Một trận mưa lớn, kèm theo lốc xoáy và những tiếng nổ lớn xảy ra trên địa bàn xã miền núi Quỳ Châu, Nghệ An làm nhiều người dân phát hoảng. \n",
      "Nhiều người nhìn thấy khói và mùi khét lẹt bốc lên nghi ngút tại 3 căn nhà là nhà anh Hà Văn Chung,Vi Văn Phương và  Lương VăChung. \n",
      "Mọi người thấy nhà của 3 gia đình nói trên bị hư hỏng nặng.  \n",
      "Dù hầu hết các thành viên của 3 gia đình bị sét đánh hư hỏng nói trên đều đang ở trong nhà nhưng rất may không có ai bị thương.\n",
      "[Reference] ﻿Một trận mưa lớn, kèm theo lốc xoáy và những tiếng nổ lớn xảy ra trên địa bàn xã miền núi Quỳ Châu, Nghệ An làm nhiều người dân phát hoảng. \n",
      "Nhiều người nhìn thấy khói và mùi khét lẹt bốc lên nghi ngút tại 3 căn nhà là nhà anh Hà Văn Chung,Vi Văn Phương và  Lương VăChung. \n",
      "Mọi người thấy nhà của 3 gia đình nói trên bị hư hỏng nặng.  \n",
      "Dù hầu hết các thành viên của 3 gia đình bị sét đánh hư hỏng nói trên đều đang ở trong nhà nhưng rất may không có ai bị thương.\n",
      "[Generated] ### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Lệnh:\n",
      "Bạn là một trợ lý tóm tắt văn bản. Hãy cung cấp bản tóm tắt ngắn gọn và chính xác trong 150 chữ cho bài viết sau. Bài viết: ﻿Lốc xoáy, sét đánh nhiều nhà tan tành.\n",
      "Một trận mưa lớn, kèm theo lốc xoáy và những tiếng nổ lớn xảy ra trên địa bàn xã miền núi Quỳ Châu, Nghệ An làm nhiều người dân phát hoảng.\n",
      "Vào khoảng 19h tối qua ( 20/4 ), người ở bản Xốp Hốc, xã Diên Lãm, huyện Quỳ Châu ( Nghệ An ) đang chuẩn bị bữa tối thì trời bắt đầu có mưa kèm theo gió lốc.\n",
      "Cơn mưa nặng hạt và bất ngờ một tiếng nổ lớn vang lên làm rung chuyển cả một khu vực.\n",
      "Cùng lúc đó nhiều người nhìn thấy khói và mùi khét lẹt bốc lên nghi ngút tại 3 căn nhà là nhà anh Hà Văn Chung, Vi Văn Phương và Lương Văn Chung.\n",
      "Sau khi hoàn hồn, mọi người thấy nhà của 3 gia đình nói trên bị hư hỏng nặng, mái nhà lợp bằng phibroximăng bị vỡ vụn rơi xuống đất hàng chục tấm, nhiều đồ đạc trong nhà như tivi, đài, ấm chén và các loại vật dụng khác trong nhà bị hư hỏng hoàn toàn.\n",
      "Dù hầu hết các thành viên của 3 gia đình bị sét đánh hư hỏng nói trên đều đang ở trong nhà nhưng rất may không có ai bị thương.\n",
      "\" Tôi đang lúi húi trong nhà để nấu cơm cho 2 đứa con ăn bữa tối, bất ngờ một tiếng \" sầm \" lớn làm rung chuyển căn nhà sàn.\n",
      "Những mảnh vỡ của tấm lợp phibroximăng rơi tung tóe khắp nhà.\n",
      "Hai đứa con nhỏ khóc thét lên.\n",
      "Rất may 3 mẹ con tôi không việc gì \", chị Vi Thị Kiều, nạn nhân của vụ sét đánh tối qua tại huyện Quỳ Châu chưa hết bàng hoàng kể lại.\n",
      "Ông Lý Đại Châu - Chủ tịch UBND xã Diên Lãm, cho biết : \" Chiều tối qua ( 20/4 ), tại hai xã Châu Hoàn và Diên Lãm có xẩy ra một trận lốc xoáy làm hư hỏng nhiều tài sản, tốc mái nhiều căn nhà.\n",
      "Hiện, chúng tôi đang chỉ đạo anh em văn phòng phối hợp với các trưởng bản tiến hành kiểm tra thống kê và báo cáo thiệt hại lên UBND huyện vào sáng thứ 2 tuần tới.\n",
      "Riêng trường hợp 3 căn nhà ở bản Xốp Hốc bị sét đánh tôi cũng vừa mới nghe qua nhưng chưa rõ tình hình thiệt hại thế nào \". Tóm tắt: \n",
      "\n",
      "### Tóm tắt:\n",
      "﻿Một trận mưa lớn, kèm theo lốc xoáy và những tiếng nổ lớn xảy ra trên địa bàn xã miền núi Quỳ Châu, Nghệ An làm nhiều người dân phát hoảng. \n",
      "Nhiều người nhìn thấy khói và mùi khét lẹt bốc lên nghi ngút tại 3 căn nhà là nhà anh Hà Văn Chung,Vi Văn Phương và  Lương VăChung. \n",
      "Mọi người thấy nhà của 3 gia đình nói trên bị hư hỏng nặng.  \n",
      "Dù hầu hết các thành viên của 3 gia đình bị sét đánh hư hỏng nói trên đều đang ở trong nhà nhưng rất may không có ai bị thương. \n",
      "\" Tôi đang lúi húi trong nhà để nấu cơm cho 2 đứa con ăn bữa tối, bất ngờ một tiếng \" sầm \" lớn làm rung chuyển căn nhà sàn. \n",
      "Những mảnh vỡ của tấm lợp phibroximăng rơi tung tóe khắp nhà. \n",
      "Hiện, chúng tôi đang chỉ đạo anh em văn phòng phối hợp với các trưởng bản tiến hành kiểm tra thống kê và báo cáo thiệt hại lên UBND huyện vào sáng thứ 2 tuần tới. \n",
      "Riêng trường hợp 3 căn nhà ở bản Xốp Hốc bị s\n",
      "\n",
      "--- Sample 3 ---\n",
      "[Prompt]    ### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Lệnh:\n",
      "Bạn là một trợ lý tóm tắt văn bản. Hãy cung cấp bản tóm tắt ngắn gọn và chính xác trong 150 chữ cho bài viết sau. Bài viết: Ngân hàng Nhà nước chính thức công bố khoản thu lãi tiết kiệm gây tranh cãi.\n",
      "Theo khẳng định của Ngân hàng Nhà nước, trong năm 2012, các tổ chức tín dụng đã trả lãi tiền gửi, tiền vay khoảng 408.000 tỷ đồng; còn thu lãi cho vay từ nền kinh tế khoảng 420.000 tỷ đồng.\n",
      "Trước những tranh cãi về con số mà nền kinh tế trả lãi cho ngân hàng trong năm 2012 lên tới 480.000 tỷ đồng, cơ quan Thanh tra Giám sát Ngân hàng thuộc Ngân hàng Nhà nước đã chính thức công bố những số liệu liên quan tới các khoản vay và cho vay của các tổ chức tín dụng.\n",
      "Theo số liệu từ cơ quan Thanh tra Giám sát Ngân hàng, ước tính từ số liệu báo cáo của các tổ chức tín dụng (TCTD), năm 2012 các TCTD đã trả lãi tiền gửi, tiền vay khoảng 408.000 tỷ đồng. Trong khi đó, các TCTD thu lãi cho vay từ nền kinh tế với khoảng 420.000 tỷ đồng.\n",
      "\n",
      "Như vậy, chênh lệch thu chi toàn ngành ngân hàng trong năm 2012 ước chỉ đạt hơn 20.000 tỷ đồng, là mức thấp nhất kể từ năm 2008 và cũng chỉ gần tương đương mức chênh lệch thu, chi của năm 2008 hay chỉ bằng khoảng 40% mức của năm 2011 do chênh lệch lãi suất đầu ra, đầu vào giảm, chi phí dự phòng rủi ro gia tăng, tín dụng tăng trưởng thấp.\n",
      "Ngoài ra, các chỉ số hiệu quả kinh doanh (ROA, ROE) cũng chỉ bằng khoảng 40% so với mức của năm 2011. Vì vậy, nhiều TCTD không chia cổ tức hoặc có tỷ lệ chia cổ tức thấp (dưới 10%).\n",
      "\n",
      "Cũng theo cơ quan Thanh tra Giám sát Ngân hàng, năm 2012, nền kinh tế gặp nhiều khó khăn, hoạt động sản xuất kinh tế trì trệ, do đó rủi ro hoạt động ngân hàng gia tăng và hiệu quả kinh doanh của các ngân hàng giảm sút. Điều này cũng phản ánh đúng thực trạng của nền kinh tế. Ngân hàng là trung gian tài chính, đi vay để cho vay. Khi đi vay, nhận tiền gửi, ngân hàng phải trả lãi cho người cho vay, người gửi tiền. Khi cho vay, ngân hàng thu lãi cho vay để có nguồn trả lãi huy động vốn. Mức độ lành mạnh, hiệu quả của người gửi tiền và người vay đều tác động đến hiệu quả kinh doanh và trạng thái rủi ro của ngân hàng. Lợi nhuận ngân hàng chủ yếu được hình thành từ chênh lệch lãi suất đầu ra, đầu vào.\n",
      "\n",
      "Do đó, “trong điều kiện kinh doanh khó khăn chung, cơ hội đạt được siêu lợi nhuận là rất khó ở bất cứ ngành, lĩnh vực nào và chỉ có thể có được ở một hoặc một vài thành viên thị trường cụ thể có sự sáng tạo và năng lực vượt trội. Nếu tình hình kinh tế không có sự cải thiện trong năm 2013 thì hoạt động ngân hàng sẽ còn gặp nhiều khó khăn do nợ xấu và chi phí hoạt động gia tăng. Vì vậy, việc cải thiện hiệu quả kinh doanh và lợi nhuận của các ngân hàng trong năm 2013 vẫn là một thách thức lớn”, cơ quan này cho hay. Tóm tắt: \n",
      "\n",
      "### Tóm tắt:\n",
      "Theo khẳng định của Ngân hàng Nhà nước, trong năm 2012, các tổ chức tín dụng đã trả lãi tiền gửi, tiền vay khoảng 408.000 tỷ đồng; còn thu lãi cho vay từ nền kinh tế khoảng 420.000 tỷ đồng.\n",
      "Như vậy, chênh lệch thu chi toàn ngành ngân hàng trong năm 2012 ước chỉ đạt hơn 20.000 tỷ đồng, là mức thấp nhất kể từ năm 2008 và cũng chỉ gần tương đương mức chênh lệch thu, chi của năm 2008 hay chỉ bằng khoảng 40% mức của năm 2011 do chênh lệch lãi suất đầu ra, đầu vào giảm, chi phí dự phòng rủi ro gia tăng, tín dụng tăng trưởng thấp.\n",
      "Năm 2012, nền kinh tế gặp nhiều khó khăn, hoạt động sản xuất kinh tế trì trệ, do đó rủi ro hoạt động ngân hàng gia tăng và hiệu quả kinh doanh của các ngân hàng giảm sút.\n",
      " Nếu tình hình kinh tế không có sự cải thiện trong năm 2013 thì hoạt động ngân hàng sẽ còn gặp nhiều khó khăn do nợ xấu và chi phí hoạt động gia tăng.\n",
      "[Reference] Theo khẳng định của Ngân hàng Nhà nước, trong năm 2012, các tổ chức tín dụng đã trả lãi tiền gửi, tiền vay khoảng 408.000 tỷ đồng; còn thu lãi cho vay từ nền kinh tế khoảng 420.000 tỷ đồng.\n",
      "Như vậy, chênh lệch thu chi toàn ngành ngân hàng trong năm 2012 ước chỉ đạt hơn 20.000 tỷ đồng, là mức thấp nhất kể từ năm 2008 và cũng chỉ gần tương đương mức chênh lệch thu, chi của năm 2008 hay chỉ bằng khoảng 40% mức của năm 2011 do chênh lệch lãi suất đầu ra, đầu vào giảm, chi phí dự phòng rủi ro gia tăng, tín dụng tăng trưởng thấp.\n",
      "Năm 2012, nền kinh tế gặp nhiều khó khăn, hoạt động sản xuất kinh tế trì trệ, do đó rủi ro hoạt động ngân hàng gia tăng và hiệu quả kinh doanh của các ngân hàng giảm sút.\n",
      " Nếu tình hình kinh tế không có sự cải thiện trong năm 2013 thì hoạt động ngân hàng sẽ còn gặp nhiều khó khăn do nợ xấu và chi phí hoạt động gia tăng.\n",
      "[Generated] ### Đây là dạng tóm tắt văn bản tin tức với độ dài tóm tắt đầu ra khoảng 150 từ:  ### Lệnh:\n",
      "Bạn là một trợ lý tóm tắt văn bản. Hãy cung cấp bản tóm tắt ngắn gọn và chính xác trong 150 chữ cho bài viết sau. Bài viết: Ngân hàng Nhà nước chính thức công bố khoản thu lãi tiết kiệm gây tranh cãi.\n",
      "Theo khẳng định của Ngân hàng Nhà nước, trong năm 2012, các tổ chức tín dụng đã trả lãi tiền gửi, tiền vay khoảng 408.000 tỷ đồng; còn thu lãi cho vay từ nền kinh tế khoảng 420.000 tỷ đồng.\n",
      "Trước những tranh cãi về con số mà nền kinh tế trả lãi cho ngân hàng trong năm 2012 lên tới 480.000 tỷ đồng, cơ quan Thanh tra Giám sát Ngân hàng thuộc Ngân hàng Nhà nước đã chính thức công bố những số liệu liên quan tới các khoản vay và cho vay của các tổ chức tín dụng.\n",
      "Theo số liệu từ cơ quan Thanh tra Giám sát Ngân hàng, ước tính từ số liệu báo cáo của các tổ chức tín dụng (TCTD), năm 2012 các TCTD đã trả lãi tiền gửi, tiền vay khoảng 408.000 tỷ đồng. Trong khi đó, các TCTD thu lãi cho vay từ nền kinh tế với khoảng 420.000 tỷ đồng.\n",
      "\n",
      "Như vậy, chênh lệch thu chi toàn ngành ngân hàng trong năm 2012 ước chỉ đạt hơn 20.000 tỷ đồng, là mức thấp nhất kể từ năm 2008 và cũng chỉ gần tương đương mức chênh lệch thu, chi của năm 2008 hay chỉ bằng khoảng 40% mức của năm 2011 do chênh lệch lãi suất đầu ra, đầu vào giảm, chi phí dự phòng rủi ro gia tăng, tín dụng tăng trưởng thấp.\n",
      "Ngoài ra, các chỉ số hiệu quả kinh doanh (ROA, ROE) cũng chỉ bằng khoảng 40% so với mức của năm 2011. Vì vậy, nhiều TCTD không chia cổ tức hoặc có tỷ lệ chia cổ tức thấp (dưới 10%).\n",
      "\n",
      "Cũng theo cơ quan Thanh tra Giám sát Ngân hàng, năm 2012, nền kinh tế gặp nhiều khó khăn, hoạt động sản xuất kinh tế trì trệ, do đó rủi ro hoạt động ngân hàng gia tăng và hiệu quả kinh doanh của các ngân hàng giảm sút. Điều này cũng phản ánh đúng thực trạng của nền kinh tế. Ngân hàng là trung gian tài chính, đi vay để cho vay. Khi đi vay, nhận tiền gửi, ngân hàng phải trả lãi cho người cho vay, người gửi tiền. Khi cho vay, ngân hàng thu lãi cho vay để có nguồn trả lãi huy động vốn. Mức độ lành mạnh, hiệu quả của người gửi tiền và người vay đều tác động đến hiệu quả kinh doanh và trạng thái rủi ro của ngân hàng. Lợi nhuận ngân hàng chủ yếu được hình thành từ chênh lệch lãi suất đầu ra, đầu vào.\n",
      "\n",
      "Do đó, “trong điều kiện kinh doanh khó khăn chung, cơ hội đạt được siêu lợi nhuận là rất khó ở bất cứ ngành, lĩnh vực nào và chỉ có thể có được ở một hoặc một vài thành viên thị trường cụ thể có sự sáng tạo và năng lực vượt trội. Nếu tình hình kinh tế không có sự cải thiện trong năm 2013 thì hoạt động ngân hàng sẽ còn gặp nhiều khó khăn do nợ xấu và chi phí hoạt động gia tăng. Vì vậy, việc cải thiện hiệu quả kinh doanh và lợi nhuận của các ngân hàng trong năm 2013 vẫn là một thách thức lớn”, cơ quan này cho hay. Tóm tắt: \n",
      "\n",
      "### Tóm tắt:\n",
      "Theo khẳng định của Ngân hàng Nhà nước, trong năm 2012, các tổ chức tín dụng đã trả lãi tiền gửi, tiền vay khoảng 408.000 tỷ đồng; còn thu lãi cho vay từ nền kinh tế khoảng 420.000 tỷ đồng.\n",
      "Như vậy, chênh lệch thu chi toàn ngành ngân hàng trong năm 2012 ước chỉ đạt hơn 20.000 tỷ đồng, là mức thấp nhất kể từ năm 2008 và cũng chỉ gần tương đương mức chênh lệch thu, chi của năm 2008 hay chỉ bằng khoảng 40% mức của năm 2011 do chênh lệch lãi suất đầu ra, đầu vào giảm, chi phí dự phòng rủi ro gia tăng, tín dụng tăng trưởng thấp.\n",
      "Năm 2012, nền kinh tế gặp nhiều khó khăn, hoạt động sản xuất kinh tế trì trệ, do đó rủi ro hoạt động ngân hàng gia tăng và hiệu quả kinh doanh của các ngân hàng giảm sút.\n",
      " Nếu tình hình kinh tế không có sự cải thiện trong năm 2013 thì hoạt động ngân hàng sẽ còn gặp nhiều khó khăn do nợ xấu và chi phí hoạt động gia tăng. Vì vậy, việc cải thiện hiệu quả kinh doanh và lợi nhuận của các ngân hàng trong năm 2013 vẫn là một thách thức lớn, cơ hội đạt được siêu lợi nhuận là rất khó ở bất cứ ngành, lĩnh vực nào và chỉ có thể có được ở một hoặc một vài thành viên thị trường cụ thể có sự sáng tạo và năng lực vượt trội. Nếu tình hình kinh tế không có sự cải thiện trong năm 2013 thì hoạt động ngân hàng sẽ còn gặp nhiều khó khăn do nợ xấu và chi phí hoạt động gia tăng. Vì vậy, việc cải thiện hiệu quả kinh doanh và lợi nhuận của các\n"
     ]
    }
   ],
   "source": [
    "evaluate_after_training(model, val_loader, tokenizer, device)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6959873,
     "sourceId": 11188231,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3853558,
     "sourceId": 6680012,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 294052,
     "modelInstanceId": 273094,
     "sourceId": 324282,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
