{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-07T01:47:37.710863Z",
     "iopub.status.busy": "2025-04-07T01:47:37.710564Z",
     "iopub.status.idle": "2025-04-07T01:47:45.862339Z",
     "shell.execute_reply": "2025-04-07T01:47:45.861435Z",
     "shell.execute_reply.started": "2025-04-07T01:47:37.710838Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install accelerate peft datasets bitsandbytes evaluate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:47:45.863722Z",
     "iopub.status.busy": "2025-04-07T01:47:45.863383Z",
     "iopub.status.idle": "2025-04-07T01:47:51.381050Z",
     "shell.execute_reply": "2025-04-07T01:47:51.380062Z",
     "shell.execute_reply.started": "2025-04-07T01:47:45.863668Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge_score) (2024.2.0)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=4e1d18159e8eb1d871bf02edc15d8fc3dd9b7d9a87f7f84b330785d633431c9a\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import library and load module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:53:24.675556Z",
     "iopub.status.busy": "2025-04-07T01:53:24.675215Z",
     "iopub.status.idle": "2025-04-07T01:53:24.679650Z",
     "shell.execute_reply": "2025-04-07T01:53:24.678947Z",
     "shell.execute_reply.started": "2025-04-07T01:53:24.675525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    default_data_collator,\n",
    ")\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:53:27.033922Z",
     "iopub.status.busy": "2025-04-07T01:53:27.033584Z",
     "iopub.status.idle": "2025-04-07T01:53:27.197763Z",
     "shell.execute_reply": "2025-04-07T01:53:27.197104Z",
     "shell.execute_reply.started": "2025-04-07T01:53:27.033896Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:53:28.729419Z",
     "iopub.status.busy": "2025-04-07T01:53:28.729088Z",
     "iopub.status.idle": "2025-04-07T01:53:28.733427Z",
     "shell.execute_reply": "2025-04-07T01:53:28.732464Z",
     "shell.execute_reply.started": "2025-04-07T01:53:28.729391Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Continue Pretrain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T18:27:05.922419Z",
     "iopub.status.busy": "2025-04-04T18:27:05.922114Z",
     "iopub.status.idle": "2025-04-04T18:27:22.302199Z",
     "shell.execute_reply": "2025-04-04T18:27:22.301258Z",
     "shell.execute_reply.started": "2025-04-04T18:27:05.922390Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3ddc438fbd40e3a9d74db36486215e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b5f23770704a91a064f8617cd758b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d894150b33f14f8fb8cb8fcf7780bb7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b1771c6a5e43fcba1abb049fd2d691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6cf8e6e25084c04b5d5d76ed4b36cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632f920fa05245cf82eb417861059de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "# 4. QLoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"khai2k48/vietnamese_news_17k\", split=\"train\")\n",
    "dataset = dataset.select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(examples[\"content\"], padding=\"max_length\", truncation=True, max_length=2048)\n",
    "    labels = tokenized[\"input_ids\"].copy()\n",
    "    labels = [-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "train_loader = DataLoader(tokenized_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T18:27:35.812567Z",
     "iopub.status.busy": "2025-04-04T18:27:35.812346Z",
     "iopub.status.idle": "2025-04-05T03:19:54.181088Z",
     "shell.execute_reply": "2025-04-05T03:19:54.180125Z",
     "shell.execute_reply.started": "2025-04-04T18:27:35.812547Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/1000 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 1/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [2:57:23<00:00, 10.64s/it, loss=0.828] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Loss: 1.2615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [2:57:27<00:00, 10.65s/it, loss=1.33]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Loss: 1.1460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [2:57:27<00:00, 10.65s/it, loss=0.825] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Loss: 1.1380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# S·ªë epoch\n",
    "n_epochs = 3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Th√™m tqdm ƒë·ªÉ hi·ªÉn th·ªã ti·∫øn tr√¨nh training\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\", leave=True)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        # ƒê∆∞a batch v√†o GPU\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backpropagation v√† c·∫≠p nh·∫≠t tr·ªçng s·ªë\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient Clipping (tr√°nh exploding gradients)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())  # Hi·ªÉn th·ªã loss t·ª´ng batch tr√™n tqdm\n",
    "\n",
    "    # T√≠nh loss trung b√¨nh c·ªßa epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} - Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T03:19:54.182368Z",
     "iopub.status.busy": "2025-04-05T03:19:54.182073Z",
     "iopub.status.idle": "2025-04-05T03:19:54.673721Z",
     "shell.execute_reply": "2025-04-05T03:19:54.672683Z",
     "shell.execute_reply.started": "2025-04-05T03:19:54.182345Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pretrained model saved at: /kaggle/working/llama3-qlora-continue-pretrained\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-continued-pretrain\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"‚úÖ Pretrained model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finetune with QA dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T06:02:26.516684Z",
     "iopub.status.busy": "2025-04-06T06:02:26.516365Z",
     "iopub.status.idle": "2025-04-06T06:02:26.520509Z",
     "shell.execute_reply": "2025-04-06T06:02:26.519554Z",
     "shell.execute_reply.started": "2025-04-06T06:02:26.516662Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"/kaggle/input/llama-qa-partially-finetune/transformers/default/1/llama-qa-finetune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T06:02:28.250286Z",
     "iopub.status.busy": "2025-04-06T06:02:28.249853Z",
     "iopub.status.idle": "2025-04-06T06:02:44.587657Z",
     "shell.execute_reply": "2025-04-06T06:02:44.586642Z",
     "shell.execute_reply.started": "2025-04-06T06:02:28.250219Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a38bd19c774c52b3437b347f9226a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc3c85c421648b2a035cf956d3f066f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c915b8f11d3d4d0688a8a637a0b17816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "# 4. QLoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T06:02:46.561139Z",
     "iopub.status.busy": "2025-04-06T06:02:46.560808Z",
     "iopub.status.idle": "2025-04-06T06:02:48.525658Z",
     "shell.execute_reply": "2025-04-06T06:02:48.524879Z",
     "shell.execute_reply.started": "2025-04-06T06:02:46.561110Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/vietnamese-squad/train-v2.0-translated.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    vi_squad_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T06:02:50.555155Z",
     "iopub.status.busy": "2025-04-06T06:02:50.554786Z",
     "iopub.status.idle": "2025-04-06T06:02:52.287125Z",
     "shell.execute_reply": "2025-04-06T06:02:52.286055Z",
     "shell.execute_reply.started": "2025-04-06T06:02:50.555123Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Train samples: 10000\n",
      "üì¶ Validation samples: 2000\n"
     ]
    }
   ],
   "source": [
    "train_data = vi_squad_data[:10000]\n",
    "\n",
    "# L·∫•y 2k m·∫´u ti·∫øp theo cho validation\n",
    "val_data = vi_squad_data[10000:12000]\n",
    "\n",
    "print(f\"üì¶ Train samples: {len(train_data)}\")\n",
    "print(f\"üì¶ Validation samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T06:02:54.174874Z",
     "iopub.status.busy": "2025-04-06T06:02:54.174549Z",
     "iopub.status.idle": "2025-04-06T06:02:54.179759Z",
     "shell.execute_reply": "2025-04-06T06:02:54.178756Z",
     "shell.execute_reply.started": "2025-04-06T06:02:54.174848Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Train samples: ['Beyonc√© Giselle Knowles-Carter (/ b i g√¨ …í n s e…™ / bee-YON-say) (sinh ng√†y 04 th√°ng 9 1981) l√† m·ªôt ca sƒ©, nh·∫°c sƒ©, nh√† s·∫£n xu·∫•t thu √¢m v√† n·ªØ di·ªÖn vi√™n ng∆∞·ªùi M·ªπ. Sinh ra v√† l·ªõn l√™n ·ªü Houston, Texas, c√¥ ƒë√£ bi·ªÉu di·ªÖn trong c√°c cu·ªôc thi ca h√°t v√† nh·∫£y m√∫a kh√°c nhau khi c√≤n nh·ªè, v√† n·ªïi ti·∫øng v√†o cu·ªëi nh·ªØng nƒÉm 1990 v·ªõi t∆∞ c√°ch l√† ca sƒ© ch√≠nh c·ªßa nh√≥m nh·∫°c n·ªØ R & B Destiny\\'s Child. ƒê∆∞·ª£c qu·∫£n l√Ω b·ªüi cha c√¥, Mathew Knowles, nh√≥m ƒë√£ tr·ªü th√†nh m·ªôt trong nh·ªØng nh√≥m nh·∫°c n·ªØ b√°n ch·∫°y nh·∫•t th·∫ø gi·ªõi m·ªçi th·ªùi ƒë·∫°i. S·ª± gi√°n ƒëo·∫°n c·ªßa h·ªç ƒë√£ ch·ª©ng ki·∫øn vi·ªác ph√°t h√†nh album ƒë·∫ßu tay c·ªßa Beyonc√©, Dangerously in Love (2003), gi√∫p c√¥ tr·ªü th√†nh m·ªôt ngh·ªá sƒ© solo tr√™n to√†n th·∫ø gi·ªõi, gi√†nh ƒë∆∞·ª£c nƒÉm gi·∫£i Grammy v√† c√≥ ƒëƒ©a ƒë∆°n qu√°n qu√¢n Billboard Hot 100 \"Crazy in Love\" v√† \"Baby Boy\".', 'Beyonce b·∫Øt ƒë·∫ßu n·ªïi ti·∫øng t·ª´ khi n√†o?', 'V√†o cu·ªëi nh·ªØng nƒÉm 1990']\n"
     ]
    }
   ],
   "source": [
    "print(f\"üì¶ Train samples: {train_data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T06:02:58.824477Z",
     "iopub.status.busy": "2025-04-06T06:02:58.824111Z",
     "iopub.status.idle": "2025-04-06T06:02:59.031274Z",
     "shell.execute_reply": "2025-04-06T06:02:59.030348Z",
     "shell.execute_reply.started": "2025-04-06T06:02:58.824447Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_list([{\"context\": item[0], \"question\": item[1], \"answer\": item[2]} for item in train_data])\n",
    "val_dataset = Dataset.from_list([{\"context\": item[0], \"question\": item[1], \"answer\": item[2]} for item in val_data])\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": val_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T06:03:00.894526Z",
     "iopub.status.busy": "2025-04-06T06:03:00.894068Z",
     "iopub.status.idle": "2025-04-06T06:03:00.906447Z",
     "shell.execute_reply": "2025-04-06T06:03:00.905652Z",
     "shell.execute_reply.started": "2025-04-06T06:03:00.894486Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. {'context': 'Beyonc√© Giselle Knowles-Carter (/ b i g√¨ …í n s e…™ / bee-YON-say) (sinh ng√†y 04 th√°ng 9 1981) l√† m·ªôt ca sƒ©, nh·∫°c sƒ©, nh√† s·∫£n xu·∫•t thu √¢m v√† n·ªØ di·ªÖn vi√™n ng∆∞·ªùi M·ªπ. Sinh ra v√† l·ªõn l√™n ·ªü Houston, Texas, c√¥ ƒë√£ bi·ªÉu di·ªÖn trong c√°c cu·ªôc thi ca h√°t v√† nh·∫£y m√∫a kh√°c nhau khi c√≤n nh·ªè, v√† n·ªïi ti·∫øng v√†o cu·ªëi nh·ªØng nƒÉm 1990 v·ªõi t∆∞ c√°ch l√† ca sƒ© ch√≠nh c·ªßa nh√≥m nh·∫°c n·ªØ R & B Destiny\\'s Child. ƒê∆∞·ª£c qu·∫£n l√Ω b·ªüi cha c√¥, Mathew Knowles, nh√≥m ƒë√£ tr·ªü th√†nh m·ªôt trong nh·ªØng nh√≥m nh·∫°c n·ªØ b√°n ch·∫°y nh·∫•t th·∫ø gi·ªõi m·ªçi th·ªùi ƒë·∫°i. S·ª± gi√°n ƒëo·∫°n c·ªßa h·ªç ƒë√£ ch·ª©ng ki·∫øn vi·ªác ph√°t h√†nh album ƒë·∫ßu tay c·ªßa Beyonc√©, Dangerously in Love (2003), gi√∫p c√¥ tr·ªü th√†nh m·ªôt ngh·ªá sƒ© solo tr√™n to√†n th·∫ø gi·ªõi, gi√†nh ƒë∆∞·ª£c nƒÉm gi·∫£i Grammy v√† c√≥ ƒëƒ©a ƒë∆°n qu√°n qu√¢n Billboard Hot 100 \"Crazy in Love\" v√† \"Baby Boy\".', 'question': 'Beyonce b·∫Øt ƒë·∫ßu n·ªïi ti·∫øng t·ª´ khi n√†o?', 'answer': 'V√†o cu·ªëi nh·ªØng nƒÉm 1990'}\n",
      "2. Beyonc√© Giselle Knowles-Carter (/ b i g√¨ …í n s e…™ / bee-YON-say) (sinh ng√†y 04 th√°ng 9 1981) l√† m·ªôt ca sƒ©, nh·∫°c sƒ©, nh√† s·∫£n xu·∫•t thu √¢m v√† n·ªØ di·ªÖn vi√™n ng∆∞·ªùi M·ªπ. Sinh ra v√† l·ªõn l√™n ·ªü Houston, Texas, c√¥ ƒë√£ bi·ªÉu di·ªÖn trong c√°c cu·ªôc thi ca h√°t v√† nh·∫£y m√∫a kh√°c nhau khi c√≤n nh·ªè, v√† n·ªïi ti·∫øng v√†o cu·ªëi nh·ªØng nƒÉm 1990 v·ªõi t∆∞ c√°ch l√† ca sƒ© ch√≠nh c·ªßa nh√≥m nh·∫°c n·ªØ R & B Destiny's Child. ƒê∆∞·ª£c qu·∫£n l√Ω b·ªüi cha c√¥, Mathew Knowles, nh√≥m ƒë√£ tr·ªü th√†nh m·ªôt trong nh·ªØng nh√≥m nh·∫°c n·ªØ b√°n ch·∫°y nh·∫•t th·∫ø gi·ªõi m·ªçi th·ªùi ƒë·∫°i. S·ª± gi√°n ƒëo·∫°n c·ªßa h·ªç ƒë√£ ch·ª©ng ki·∫øn vi·ªác ph√°t h√†nh album ƒë·∫ßu tay c·ªßa Beyonc√©, Dangerously in Love (2003), gi√∫p c√¥ tr·ªü th√†nh m·ªôt ngh·ªá sƒ© solo tr√™n to√†n th·∫ø gi·ªõi, gi√†nh ƒë∆∞·ª£c nƒÉm gi·∫£i Grammy v√† c√≥ ƒëƒ©a ƒë∆°n qu√°n qu√¢n Billboard Hot 100 \"Crazy in Love\" v√† \"Baby Boy\".\n",
      "3. Beyonce b·∫Øt ƒë·∫ßu n·ªïi ti·∫øng t·ª´ khi n√†o?\n",
      "4. V√†o cu·ªëi nh·ªØng nƒÉm 1990\n"
     ]
    }
   ],
   "source": [
    "print(f\"1. {dataset_dict['train'][0]}\")\n",
    "print(f\"2. {dataset_dict['train'][0]['context']}\")\n",
    "print(f\"3. {dataset_dict['train'][0]['question']}\")\n",
    "print(f\"4. {dataset_dict['train'][0]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T06:03:02.800024Z",
     "iopub.status.busy": "2025-04-06T06:03:02.799737Z",
     "iopub.status.idle": "2025-04-06T06:03:22.119544Z",
     "shell.execute_reply": "2025-04-06T06:03:22.118626Z",
     "shell.execute_reply.started": "2025-04-06T06:03:02.800004Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f76e2bec13b4197a9930549d1273aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110c5c7c141249d58d8a25166107de50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_qa(example, max_length=512):\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"]\n",
    "    prompt = f\"### ƒê√¢y l√† d·∫°ng c√¢u h·ªèi v√† tr·∫£ l·ªùi d·ª±a tr√™n n·ªôi dung ###\\n\\nC√¢u h·ªèi: {question}\\n\\nN·ªôi dung: {context}\\n\\nTr·∫£ l·ªùi:\"\n",
    "    completion = f\" {answer}\"\n",
    "    \n",
    "    prompt_ids = tokenizer(prompt, add_special_tokens=False).input_ids\n",
    "    completion_ids = tokenizer(completion, add_special_tokens=False).input_ids\n",
    "\n",
    "    input_ids = prompt_ids + completion_ids\n",
    "\n",
    "    labels = [-100] * len(prompt_ids) + completion_ids\n",
    "\n",
    "    if len(input_ids) > max_length:\n",
    "        input_ids = input_ids[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    \n",
    "    input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "    attention_mask = attention_mask + [0] * padding_length\n",
    "    labels = labels + [-100] * padding_length\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids),\n",
    "        \"attention_mask\": torch.tensor(attention_mask),\n",
    "        \"labels\": torch.tensor(labels)\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset_dict.map(preprocess_qa)\n",
    "\n",
    "tokenized_dataset.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T06:03:24.476077Z",
     "iopub.status.busy": "2025-04-06T06:03:24.475786Z",
     "iopub.status.idle": "2025-04-06T06:03:24.480534Z",
     "shell.execute_reply": "2025-04-06T06:03:24.479729Z",
     "shell.execute_reply.started": "2025-04-06T06:03:24.476056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "collator = default_data_collator\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"], batch_size=4, shuffle=True, collate_fn=collator\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_dataset[\"test\"], batch_size=4, shuffle=False, collate_fn=collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T06:03:33.414082Z",
     "iopub.status.busy": "2025-04-06T06:03:33.413767Z",
     "iopub.status.idle": "2025-04-06T13:02:16.682858Z",
     "shell.execute_reply": "2025-04-06T13:02:16.681821Z",
     "shell.execute_reply.started": "2025-04-06T06:03:33.414057Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:   0%|          | 0/2500 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 1/2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2500/2500 [3:15:37<00:00,  4.69s/it, loss=0.853]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1/2 - Train Loss: 1.6862\n",
      "üìä Validation Loss: 1.4126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2500/2500 [3:15:50<00:00,  4.70s/it, loss=0.749]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 2/2 - Train Loss: 1.3846\n",
      "üìä Validation Loss: 1.2764\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "n_epochs = 2\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\", leave=True)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"‚úÖ Epoch {epoch+1}/{n_epochs} - Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"üìä Validation Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T13:12:33.793626Z",
     "iopub.status.busy": "2025-04-06T13:12:33.793300Z",
     "iopub.status.idle": "2025-04-06T13:12:34.387391Z",
     "shell.execute_reply": "2025-04-06T13:12:34.386338Z",
     "shell.execute_reply.started": "2025-04-06T13:12:33.793600Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pretrained model saved at: /kaggle/working/llama3-qlora-qa-finetune\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-qa-finetune\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"‚úÖ Pretrained model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T13:20:00.418325Z",
     "iopub.status.busy": "2025-04-06T13:20:00.417933Z",
     "iopub.status.idle": "2025-04-06T13:20:01.193743Z",
     "shell.execute_reply": "2025-04-06T13:20:01.192933Z",
     "shell.execute_reply.started": "2025-04-06T13:20:00.418294Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='llama3-qlora-qa-finetune.zip' target='_blank'>llama3-qlora-qa-finetune.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/llama3-qlora-qa-finetune.zip"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import shutil\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n t·ªõi th∆∞ m·ª•c ch·ª©a m√¥ h√¨nh\n",
    "model_dir = '/kaggle/working/llama3-qlora-qa-finetune'\n",
    "\n",
    "# T·∫°o file n√©n .zip t·ª´ th∆∞ m·ª•c\n",
    "zip_name = 'llama3-qlora-qa-finetune'  # T√™n file n√©n (kh√¥ng c·∫ßn ƒëu√¥i .zip)\n",
    "shutil.make_archive(zip_name, 'zip', model_dir)\n",
    "\n",
    "# T·∫°o li√™n k·∫øt t·∫£i xu·ªëng cho file .zip\n",
    "display(FileLink(f'{zip_name}.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finetune with BBC News**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:53:12.711554Z",
     "iopub.status.busy": "2025-04-07T01:53:12.711217Z",
     "iopub.status.idle": "2025-04-07T01:53:12.715189Z",
     "shell.execute_reply": "2025-04-07T01:53:12.714226Z",
     "shell.execute_reply.started": "2025-04-07T01:53:12.711532Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"/kaggle/input/llama-qa-finetune/transformers/default/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:53:47.862407Z",
     "iopub.status.busy": "2025-04-07T01:53:47.862108Z",
     "iopub.status.idle": "2025-04-07T01:54:02.110257Z",
     "shell.execute_reply": "2025-04-07T01:54:02.109201Z",
     "shell.execute_reply.started": "2025-04-07T01:53:47.862383Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ffe6cba8cf34f67887fa2783e3a15b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78322ab504124d75b4e7186ef1bb5688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe8060324fe4274b9b5db516661ee6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "# 4. QLoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:54:06.073087Z",
     "iopub.status.busy": "2025-04-07T01:54:06.072743Z",
     "iopub.status.idle": "2025-04-07T01:54:06.078998Z",
     "shell.execute_reply": "2025-04-07T01:54:06.078190Z",
     "shell.execute_reply.started": "2025-04-07T01:54:06.073062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:54:56.559889Z",
     "iopub.status.busy": "2025-04-07T01:54:56.559521Z",
     "iopub.status.idle": "2025-04-07T01:54:57.085726Z",
     "shell.execute_reply": "2025-04-07T01:54:57.084876Z",
     "shell.execute_reply.started": "2025-04-07T01:54:56.559857Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428546799fbc4339b807623dc3909b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = \"/kaggle/input/data-llama-finetune/bbc_data_llama_finetune.json\"\n",
    "\n",
    "full_dataset = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
    "dataset = full_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:55:00.656526Z",
     "iopub.status.busy": "2025-04-07T01:55:00.656189Z",
     "iopub.status.idle": "2025-04-07T01:55:07.826029Z",
     "shell.execute_reply": "2025-04-07T01:55:07.825016Z",
     "shell.execute_reply.started": "2025-04-07T01:55:00.656498Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19802598833f476c84da88a5a8e4a8c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1914 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26779e63ff524362ad6c61cc71403372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/213 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(example, max_length=2048):\n",
    "    prompt = f\"### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:  ### Prompt:\\n{example['prompt']}\\n\\n### Summary:\\n\"\n",
    "    summary = example[\"summary\"]\n",
    "    prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    summary_ids = tokenizer.encode(summary, add_special_tokens=False)\n",
    "    \n",
    "    # Ki·ªÉm tra t·ªïng s·ªë token\n",
    "    total_length = len(prompt_ids) + len(summary_ids)\n",
    "    if total_length > max_length:\n",
    "        overflow = total_length - max_length\n",
    "        if overflow < len(prompt_ids):\n",
    "            prompt_ids = prompt_ids[:-overflow]\n",
    "        else:\n",
    "            prompt_ids = []\n",
    "    input_ids = prompt_ids + summary_ids\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    labels = [-100] * len(prompt_ids) + summary_ids\n",
    "\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "        attention_mask = attention_mask + [0] * padding_length\n",
    "        labels = labels + [-100] * padding_length\n",
    "    else:\n",
    "        input_ids = input_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "tokenized_dataset = dataset.map(tokenize, batched=False, fn_kwargs={\"max_length\": 2048})\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "collator = default_data_collator\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_dataset[\"test\"],\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:55:09.106339Z",
     "iopub.status.busy": "2025-04-07T01:55:09.106017Z",
     "iopub.status.idle": "2025-04-07T01:55:09.124606Z",
     "shell.execute_reply": "2025-04-07T01:55:09.123631Z",
     "shell.execute_reply.started": "2025-04-07T01:55:09.106316Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:55:11.722519Z",
     "iopub.status.busy": "2025-04-07T01:55:11.722159Z",
     "iopub.status.idle": "2025-04-07T10:22:39.183765Z",
     "shell.execute_reply": "2025-04-07T10:22:39.182869Z",
     "shell.execute_reply.started": "2025-04-07T01:55:11.722488Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1446: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 957/957 [2:49:01<00:00, 10.60s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1 completed ‚Äî Avg loss: 0.1809\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1074: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 957/957 [2:49:16<00:00, 10.61s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 2 completed ‚Äî Avg loss: 0.1497\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1099: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 957/957 [2:49:09<00:00, 10.61s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 3 completed ‚Äî Avg loss: 0.1431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in pbar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"‚úÖ Epoch {epoch+1} completed ‚Äî Avg loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T10:22:39.185425Z",
     "iopub.status.busy": "2025-04-07T10:22:39.185130Z",
     "iopub.status.idle": "2025-04-07T10:22:39.827664Z",
     "shell.execute_reply": "2025-04-07T10:22:39.826947Z",
     "shell.execute_reply.started": "2025-04-07T10:22:39.185402Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Finetuned model saved at: /kaggle/working/llama3-qlora-finetuned\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-bbc-news-finetuned\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"‚úÖ Finetuned model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T10:22:39.829665Z",
     "iopub.status.busy": "2025-04-07T10:22:39.829375Z",
     "iopub.status.idle": "2025-04-07T10:22:39.840440Z",
     "shell.execute_reply": "2025-04-07T10:22:39.839482Z",
     "shell.execute_reply.started": "2025-04-07T10:22:39.829641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_after_training(model, val_loader, tokenizer, device, num_samples=3):\n",
    "    import evaluate\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    samples_to_print = []\n",
    "\n",
    "    # Load ROUGE\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    def left_pad(inputs, pad_token_id):\n",
    "        \"\"\"Chuy·ªÉn batch input th√†nh left-padded\"\"\"\n",
    "        max_len = max(len(seq) for seq in inputs)\n",
    "        return torch.stack([\n",
    "            torch.cat([torch.full((max_len - len(seq),), pad_token_id, dtype=torch.long), seq])\n",
    "            for seq in inputs\n",
    "        ])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            # Chuy·ªÉn sang left padding th·ªß c√¥ng\n",
    "            input_ids = left_pad([x[x != tokenizer.pad_token_id] for x in input_ids], tokenizer.pad_token_id).to(device)\n",
    "            attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "\n",
    "            # Generate\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=128,\n",
    "                num_beams=4,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            # Decode\n",
    "            generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "            label_texts = []\n",
    "            for label in labels:\n",
    "                label_ids = [token_id for token_id in label.tolist() if token_id != -100]\n",
    "                label_texts.append(tokenizer.decode(label_ids, skip_special_tokens=True))\n",
    "\n",
    "            predictions.extend(generated_texts)\n",
    "            references.extend(label_texts)\n",
    "\n",
    "            if len(samples_to_print) < num_samples:\n",
    "                src_texts = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "                for src, ref, pred in zip(src_texts, label_texts, generated_texts):\n",
    "                    samples_to_print.append((src, ref, pred))\n",
    "                    if len(samples_to_print) >= num_samples:\n",
    "                        break\n",
    "\n",
    "    # T√≠nh ROUGE\n",
    "    results = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "\n",
    "    print(\"\\nüìä ROUGE Scores:\")\n",
    "    for key in results:\n",
    "        print(f\"{key}: {results[key]:.4f}\")\n",
    "\n",
    "    print(\"\\nüìù Sample Results:\")\n",
    "    for i, (src, ref, pred) in enumerate(samples_to_print):\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        print(f\"[Prompt]    {src}\")\n",
    "        print(f\"[Reference] {ref}\")\n",
    "        print(f\"[Generated] {pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T10:22:39.841789Z",
     "iopub.status.busy": "2025-04-07T10:22:39.841550Z",
     "iopub.status.idle": "2025-04-07T10:53:27.514816Z",
     "shell.execute_reply": "2025-04-07T10:53:27.513776Z",
     "shell.execute_reply.started": "2025-04-07T10:22:39.841769Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d5ba3335144246bf6cc852229f0217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/107 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 107/107 [30:23<00:00, 17.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä ROUGE Scores:\n",
      "rouge1: 0.4069\n",
      "rouge2: 0.4053\n",
      "rougeL: 0.4071\n",
      "rougeLsum: 0.4072\n",
      "\n",
      "üìù Sample Results:\n",
      "\n",
      "--- Sample 1 ---\n",
      "[Prompt]    ### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:  ### Prompt:\n",
      "You are a helpful summarization assistant. Summarize the following article in 150 words: Campaign 'cold calls' questioned\n",
      "\n",
      "Labour and the Conservatives are still telephoning the millions of people who have signed up to make sure they do not get marketing \"cold calls\".\n",
      "\n",
      "The parties say they can stick to the rules by ensuring that their calls are not marketing - for instance by asking about people's voting intentions. The Lib Dems are asking the watchdog overseeing the rules to stop the calls. The information commissioner's office says surveys are allowed but people had to be told if personal data was kept. Telephone call centres are expected to be used as never before by all the three major parties in the run-up to the general election.\n",
      "\n",
      "But seven million telephone numbers are on the Telephone Preference Service (TPS) lists, which ban unsolicited sales and marketing calls. Both schemes are run by the Direct Marketing Association and backed by EU directives on privacy and electronic communications.\n",
      "\n",
      "The rules on marketing calls apply as much to politicians as to private sector companies. But that does not mean Labour and the Tories are not calling people signed up to the TPS. A Labour Party spokesman told the BBC News website the party avoided those on TPS lists when telephoning people about membership or fundraising. But that did not happen for \"voter identification\" calls. \"When we ask which party they will vote for, that is not marketing and we have very clear legal advice that it is not,\" he said. \"So it is not covered by the Telephone Preference Service.\"\n",
      "\n",
      "He said the party always asked people if they would be happy to be contacted again and if they said no, they were not rung again. A Conservative spokeswoman said the party stuck to the rules when it rang TPS subscribers. She said: \"We do apply TPS but in line with the law. We would not do things that are not allowed in the law.\" Assistant information commissioner Phil Jones said it was classed as marketing if political parties telephoned people to encourage them to vote for them. But \"classic market research\", such as a poll of voter intentions, did not constitute direct marketing, he said. \"If a party is calling someone who is registered on TPS and records their voting intention with a view to using this information in the future, this should be clear to the voter concerned,\" said Mr Jones. \"If a party rings a person who is registered on TPS to ask about their voting intention and goes on to encourage that voter to support them, the party may well be in breach of the regulations. \"In summary, whether a party calling TPS registered voters to check their voting intentions will breach regulations will depend on the script used and whether the script is followed.\" Mr Jones said the watchdog received \"very few complaints\" on the issue.\n",
      "\n",
      "Earlier, Lib Dem chairman Matthew Taylor wrote to the watchdog saying: \"The advice we have received on several previous occasions is that such phone calls are illegal.\" He says evidence from local Lib Dem parties around the country suggests there are \"significant\" numbers of such calls. \"I hope you can therefore take swift and efficient action to ensure that this ceases,\" he tells the commissioner. Mr Taylor argues there should be new guidelines so all parties can act in the same way if the watchdog believes the rules allow parties to ring TPS numbers about voting intentions and later urge those people to vote for them. Summary: \n",
      "\n",
      "### Summary:\n",
      "Assistant information commissioner Phil Jones said it was classed as marketing if political parties telephoned people to encourage them to vote for them.Mr Taylor argues there should be new guidelines so all parties can act in the same way if the watchdog believes the rules allow parties to ring TPS numbers about voting intentions and later urge those people to vote for them.\"If a party is calling someone who is registered on TPS and records their voting intention with a view to using this information in the future, this should be clear to the voter concerned,\" said Mr Jones.\"When we ask which party they will vote for, that is not marketing and we have very clear legal advice that it is not,\" he said.A Conservative spokeswoman said the party stuck to the rules when it rang TPS subscribers.He said the party always asked people if they would be happy to be contacted again and if they said no, they were not rung again.\"If a party rings a person who is registered on TPS to ask about their voting intention and goes on to encourage that voter to support them, the party may well be in breach of the regulations.The parties say they can stick to the rules by ensuring that their calls are not marketing - for instance by asking about people's voting intentions.A Labour Party spokesman told the BBC News website the party avoided those on TPS lists when telephoning people about membership or fundraising.But seven million telephone numbers are on the Telephone Preference Service (TPS) lists, which ban unsolicited sales and marketing calls.She said: \"We do apply TPS but in line with the law.\n",
      "[Reference] Assistant information commissioner Phil Jones said it was classed as marketing if political parties telephoned people to encourage them to vote for them.Mr Taylor argues there should be new guidelines so all parties can act in the same way if the watchdog believes the rules allow parties to ring TPS numbers about voting intentions and later urge those people to vote for them.\"If a party is calling someone who is registered on TPS and records their voting intention with a view to using this information in the future, this should be clear to the voter concerned,\" said Mr Jones.\"When we ask which party they will vote for, that is not marketing and we have very clear legal advice that it is not,\" he said.A Conservative spokeswoman said the party stuck to the rules when it rang TPS subscribers.He said the party always asked people if they would be happy to be contacted again and if they said no, they were not rung again.\"If a party rings a person who is registered on TPS to ask about their voting intention and goes on to encourage that voter to support them, the party may well be in breach of the regulations.The parties say they can stick to the rules by ensuring that their calls are not marketing - for instance by asking about people's voting intentions.A Labour Party spokesman told the BBC News website the party avoided those on TPS lists when telephoning people about membership or fundraising.But seven million telephone numbers are on the Telephone Preference Service (TPS) lists, which ban unsolicited sales and marketing calls.She said: \"We do apply TPS but in line with the law.\n",
      "[Generated] ### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:  ### Prompt:\n",
      "You are a helpful summarization assistant. Summarize the following article in 150 words: Campaign 'cold calls' questioned\n",
      "\n",
      "Labour and the Conservatives are still telephoning the millions of people who have signed up to make sure they do not get marketing \"cold calls\".\n",
      "\n",
      "The parties say they can stick to the rules by ensuring that their calls are not marketing - for instance by asking about people's voting intentions. The Lib Dems are asking the watchdog overseeing the rules to stop the calls. The information commissioner's office says surveys are allowed but people had to be told if personal data was kept. Telephone call centres are expected to be used as never before by all the three major parties in the run-up to the general election.\n",
      "\n",
      "But seven million telephone numbers are on the Telephone Preference Service (TPS) lists, which ban unsolicited sales and marketing calls. Both schemes are run by the Direct Marketing Association and backed by EU directives on privacy and electronic communications.\n",
      "\n",
      "The rules on marketing calls apply as much to politicians as to private sector companies. But that does not mean Labour and the Tories are not calling people signed up to the TPS. A Labour Party spokesman told the BBC News website the party avoided those on TPS lists when telephoning people about membership or fundraising. But that did not happen for \"voter identification\" calls. \"When we ask which party they will vote for, that is not marketing and we have very clear legal advice that it is not,\" he said. \"So it is not covered by the Telephone Preference Service.\"\n",
      "\n",
      "He said the party always asked people if they would be happy to be contacted again and if they said no, they were not rung again. A Conservative spokeswoman said the party stuck to the rules when it rang TPS subscribers. She said: \"We do apply TPS but in line with the law. We would not do things that are not allowed in the law.\" Assistant information commissioner Phil Jones said it was classed as marketing if political parties telephoned people to encourage them to vote for them. But \"classic market research\", such as a poll of voter intentions, did not constitute direct marketing, he said. \"If a party is calling someone who is registered on TPS and records their voting intention with a view to using this information in the future, this should be clear to the voter concerned,\" said Mr Jones. \"If a party rings a person who is registered on TPS to ask about their voting intention and goes on to encourage that voter to support them, the party may well be in breach of the regulations. \"In summary, whether a party calling TPS registered voters to check their voting intentions will breach regulations will depend on the script used and whether the script is followed.\" Mr Jones said the watchdog received \"very few complaints\" on the issue.\n",
      "\n",
      "Earlier, Lib Dem chairman Matthew Taylor wrote to the watchdog saying: \"The advice we have received on several previous occasions is that such phone calls are illegal.\" He says evidence from local Lib Dem parties around the country suggests there are \"significant\" numbers of such calls. \"I hope you can therefore take swift and efficient action to ensure that this ceases,\" he tells the commissioner. Mr Taylor argues there should be new guidelines so all parties can act in the same way if the watchdog believes the rules allow parties to ring TPS numbers about voting intentions and later urge those people to vote for them. Summary: \n",
      "\n",
      "### Summary:\n",
      "Assistant information commissioner Phil Jones said it was classed as marketing if political parties telephoned people to encourage them to vote for them.Mr Taylor argues there should be new guidelines so all parties can act in the same way if the watchdog believes the rules allow parties to ring TPS numbers about voting intentions and later urge those people to vote for them.\"If a party is calling someone who is registered on TPS and records their voting intention with a view to using this information in the future, this should be clear to the voter concerned,\" said Mr Jones.\"When we ask which party they will vote for, that is not marketing and we have very clear legal advice that it is not,\" he said.A Conservative spokeswoman said the party stuck to the rules when it rang TPS subscribers.He said the party always asked people if they would be happy to be contacted again and if they said no, they were not rung again.\"If a party rings a person who is registered on TPS to ask about their voting intention and goes on to encourage that voter to support them, the party may well be in breach of the regulations.The parties say they can stick to the rules by ensuring that their calls are not marketing - for instance by asking about people's voting intentions.A Labour Party spokesman told the BBC News website the party avoided those on TPS lists when telephoning people about membership or fundraising.But seven million telephone numbers are on the Telephone Preference Service (TPS) lists, which ban unsolicited sales and marketing calls.She said: \"We do apply TPS but in line with the law.Both schemes are run by the Direct Marketing Association and backed by EU directives on privacy and electronic communications.Both schemes are run by the Direct Marketing Association and backed by EU directives on privacy and electronic communications.\"In summary, whether a party calling TPS registered voters to check their voting intentions will breach regulations will depend on the script used and whether the script is followed.\"Mr Jones said the watchdog received \"very few complaints\" on the issue.\n",
      "\n",
      "--- Sample 2 ---\n",
      "[Prompt]    ### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:  ### Prompt:\n",
      "You are a helpful summarization assistant. Summarize the following article in 150 words: Brown hits back in Blair rift row\n",
      "\n",
      "Gordon Brown has criticised a union leader who said conflict between himself and Tony Blair was harming the workings of government.\n",
      "\n",
      "Jonathan Baume, of the top civil servants' union, spoke of \"competing agendas\" between Mr Brown and Mr Blair. But the chancellor said Mr Baume was never at meetings between himself and the prime minister so could not judge. He said the union leader was trying to block civil service reform which threatened his members' jobs. It suited the purpose of Mr Baume's union, the First Division Association, to suggest there were two agendas battling against each other because the union was trying to resist the planned reforms, Mr Brown told BBC Radio 4's Today programme.\n",
      "\n",
      "Under the plans, unveiled in the Gershon report, some 84,000 civil servants jobs will be axed or changed and the savings ploughed back into frontline services. Mr Brown said: \"To be honest I don't think you can rely on his [Mr Baume's] judgement on this matter when it comes to the decisions that the government are making. \"Mr Blair and I are making exactly the same decisions on civil service reforms. We are determined to go on with the Gershon reforms.\" He also said that as Mr Baume was never present at meetings between himself and the prime minister, he was not in a position to judge. On Wednesday, ahead of the Chancellor's pre-Budget report, Mr Baume told BBC News there were sometimes \"conflicting and competing agendas for government\" between Number 10 and the Treasury.\n",
      "\n",
      "What the chancellor wanted was \"not by any means what Alan Milburn and the prime minister want to see\", Mr Baume said. \"Government departments get their money from the Treasury on the basis of public service agreements they sign up to, but at the same time the prime minister also has an agenda and that's not necessarily the same as the Treasury's and the prime minister is of course a very powerful figure in any government. \"He also sends instructions and messages and directions to departments about how he would like each secretary of state and each department to implement a policy agenda. \"The problem is that on many occasions these two don't add up and individual cabinet ministers as well as departments have to make sense of this battle.\" Number 10 said ministers were interested in governing and not a \"soap opera\" about Mr Blair and Mr Brown. Tory shadow chancellor Oliver Letwin said: \"The battle Royal that the top civil servants are now reporting on between the chancellor and Tony Blair is preventing them both from getting on with the business of getting taxpayers value for money.\" Summary: \n",
      "\n",
      "### Summary:\n",
      "But the chancellor said Mr Baume was never at meetings between himself and the prime minister so could not judge.He also said that as Mr Baume was never present at meetings between himself and the prime minister, he was not in a position to judge.Number 10 said ministers were interested in governing and not a \"soap opera\" about Mr Blair and Mr Brown.Jonathan Baume, of the top civil servants' union, spoke of \"competing agendas\" between Mr Brown and Mr Blair.What the chancellor wanted was \"not by any means what Alan Milburn and the prime minister want to see\", Mr Baume said.Mr Brown said: \"To be honest I don't think you can rely on his [Mr Baume's] judgement on this matter when it comes to the decisions that the government are making.\"Mr Blair and I are making exactly the same decisions on civil service reforms.\n",
      "[Reference] But the chancellor said Mr Baume was never at meetings between himself and the prime minister so could not judge.He also said that as Mr Baume was never present at meetings between himself and the prime minister, he was not in a position to judge.Number 10 said ministers were interested in governing and not a \"soap opera\" about Mr Blair and Mr Brown.Jonathan Baume, of the top civil servants' union, spoke of \"competing agendas\" between Mr Brown and Mr Blair.What the chancellor wanted was \"not by any means what Alan Milburn and the prime minister want to see\", Mr Baume said.Mr Brown said: \"To be honest I don't think you can rely on his [Mr Baume's] judgement on this matter when it comes to the decisions that the government are making.\"Mr Blair and I are making exactly the same decisions on civil service reforms.\n",
      "[Generated] ### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:  ### Prompt:\n",
      "You are a helpful summarization assistant. Summarize the following article in 150 words: Brown hits back in Blair rift row\n",
      "\n",
      "Gordon Brown has criticised a union leader who said conflict between himself and Tony Blair was harming the workings of government.\n",
      "\n",
      "Jonathan Baume, of the top civil servants' union, spoke of \"competing agendas\" between Mr Brown and Mr Blair. But the chancellor said Mr Baume was never at meetings between himself and the prime minister so could not judge. He said the union leader was trying to block civil service reform which threatened his members' jobs. It suited the purpose of Mr Baume's union, the First Division Association, to suggest there were two agendas battling against each other because the union was trying to resist the planned reforms, Mr Brown told BBC Radio 4's Today programme.\n",
      "\n",
      "Under the plans, unveiled in the Gershon report, some 84,000 civil servants jobs will be axed or changed and the savings ploughed back into frontline services. Mr Brown said: \"To be honest I don't think you can rely on his [Mr Baume's] judgement on this matter when it comes to the decisions that the government are making. \"Mr Blair and I are making exactly the same decisions on civil service reforms. We are determined to go on with the Gershon reforms.\" He also said that as Mr Baume was never present at meetings between himself and the prime minister, he was not in a position to judge. On Wednesday, ahead of the Chancellor's pre-Budget report, Mr Baume told BBC News there were sometimes \"conflicting and competing agendas for government\" between Number 10 and the Treasury.\n",
      "\n",
      "What the chancellor wanted was \"not by any means what Alan Milburn and the prime minister want to see\", Mr Baume said. \"Government departments get their money from the Treasury on the basis of public service agreements they sign up to, but at the same time the prime minister also has an agenda and that's not necessarily the same as the Treasury's and the prime minister is of course a very powerful figure in any government. \"He also sends instructions and messages and directions to departments about how he would like each secretary of state and each department to implement a policy agenda. \"The problem is that on many occasions these two don't add up and individual cabinet ministers as well as departments have to make sense of this battle.\" Number 10 said ministers were interested in governing and not a \"soap opera\" about Mr Blair and Mr Brown. Tory shadow chancellor Oliver Letwin said: \"The battle Royal that the top civil servants are now reporting on between the chancellor and Tony Blair is preventing them both from getting on with the business of getting taxpayers value for money.\" Summary: \n",
      "\n",
      "### Summary:\n",
      "But the chancellor said Mr Baume was never at meetings between himself and the prime minister so could not judge.He also said that as Mr Baume was never present at meetings between himself and the prime minister, he was not in a position to judge.Number 10 said ministers were interested in governing and not a \"soap opera\" about Mr Blair and Mr Brown.Jonathan Baume, of the top civil servants' union, spoke of \"competing agendas\" between Mr Brown and Mr Blair.What the chancellor wanted was \"not by any means what Alan Milburn and the prime minister want to see\", Mr Baume said.Mr Brown said: \"To be honest I don't think you can rely on his [Mr Baume's] judgement on this matter when it comes to the decisions that the government are making.\"Mr Blair and I are making exactly the same decisions on civil service reforms. \"Government departments get their money from the Treasury on the basis of public service agreements they sign up to, but at the same time the prime minister also has an agenda and that's not necessarily the same as the Treasury's and the prime minister is of course a very powerful figure in any government.Tory shadow chancellor Oliver Letwin said: \"The battle Royal that the top civil servants are now reporting on between the chancellor and Tony Blair is preventing them both from getting on with the business of getting taxpayers value for money.\"Under the plans, unveiled in the Gershon report, some 84,000 civil servants jobs will be axed or\n",
      "\n",
      "--- Sample 3 ---\n",
      "[Prompt]    ### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:  ### Prompt:\n",
      "You are a helpful summarization assistant. Summarize the following article in 150 words: Mixed reaction to Man Utd offer\n",
      "\n",
      "Shares in Manchester United were up over 5% by noon on Monday following a new offer from Malcolm Glazer.\n",
      "\n",
      "The board of Man Utd is expected to meet early this week to discuss the latest proposal from the US tycoon that values the club at ¬£800m ($1.5bn). Manchester United revealed on Sunday that it had received a detailed proposal from Mr Glazer. A senior source at the club told the BBC: \"This time it's different\". The board is obliged to consider this deal. But the Man Utd supporters club urged the club to reject the new deal. Manchester United past and present footballers Eric Cantona and Ole Gunnar Solskjaer, and club manager Sir Alex Ferguson, have lent their backing to the supporters' group, Shareholders United. They have all spoken out against the bid.\n",
      "\n",
      "A spokesman for the supporters club said: \"I can't see any difference (compared to Mr Glazer's previous proposals) other than ¬£200m less debt. \"He isn't bringing any money into the club; he'll use our money to buy it.\"\n",
      "\n",
      "Mr Glazer's latest move is being led by Mr Glazer's two sons, Avi and Joel, according to the Financial Times. A proposal was received by David Gill, United's chief executive, at the end of last week, pitched at about 300p a share. David Cummings, head of UK equities for Standard Life Investments, said he believed a \"well funded\" 300p a share bid would be enough for Mr Glazer to take control of the club. \"I do not think there is anything that Manchester United fans can do about it,\" he told the BBC. \"They can complain about it but it is curtains for them. They may not want him but they are going to get him.\" The US tycoon, who has been wooing the club for the last 12 months, has approached the United board with \"detailed proposals\", it has confirmed.\n",
      "\n",
      "Mr Glazer, who owns the Tampa Bay Buccaneers team, hopes this will lead to a formal bid being accepted. He is believed to have increased the amount of equity in the new proposal, though it is not clear by how much. For his proposal to succeed, he needs the support of United's largest shareholders, the Irish horseracing tycoons JP McManus and John Magnier. They own 29% of United through their Cubic Expression investment vehicle. Mr Glazer and his family hold a stake of 28.1%. But it is not yet known whether Mr McManus and Mr Magnier would support a Glazer bid. NM Rothschild, the investment bank, is advising Mr Glazer, according to the Financial Times. His previous adviser, JPMorgan, quit last year when Mr Glazer went ahead and voted against the appointment of three United directors to the board, against its advice. But the FT said it thought JP Morgan may still have had some role in financing Mr Glazer's latest financial proposal. Summary: \n",
      "\n",
      "### Summary:\n",
      "Manchester United revealed on Sunday that it had received a detailed proposal from Mr Glazer.But it is not yet known whether Mr McManus and Mr Magnier would support a Glazer bid.David Cummings, head of UK equities for Standard Life Investments, said he believed a \"well funded\" 300p a share bid would be enough for Mr Glazer to take control of the club.But the FT said it thought JP Morgan may still have had some role in financing Mr Glazer's latest financial proposal.His previous adviser, JPMorgan, quit last year when Mr Glazer went ahead and voted against the appointment of three United directors to the board, against its advice.Mr Glazer and his family hold a stake of 28.1%.NM Rothschild, the investment bank, is advising Mr Glazer, according to the Financial Times.Mr Glazer's latest move is being led by Mr Glazer's two sons, Avi and Joel, according to the Financial Times.A spokesman for the supporters club said: \"I can't see any difference (compared to Mr Glazer's previous proposals) other than ¬£200m less debt.The board of Man Utd is expected to meet early this week to discuss the latest proposal from the US tycoon that values the club at ¬£800m ($1.5bn).But the Man Utd supporters club urged the club to reject the new deal.\n",
      "[Reference] Manchester United revealed on Sunday that it had received a detailed proposal from Mr Glazer.But it is not yet known whether Mr McManus and Mr Magnier would support a Glazer bid.David Cummings, head of UK equities for Standard Life Investments, said he believed a \"well funded\" 300p a share bid would be enough for Mr Glazer to take control of the club.But the FT said it thought JP Morgan may still have had some role in financing Mr Glazer's latest financial proposal.His previous adviser, JPMorgan, quit last year when Mr Glazer went ahead and voted against the appointment of three United directors to the board, against its advice.Mr Glazer and his family hold a stake of 28.1%.NM Rothschild, the investment bank, is advising Mr Glazer, according to the Financial Times.Mr Glazer's latest move is being led by Mr Glazer's two sons, Avi and Joel, according to the Financial Times.A spokesman for the supporters club said: \"I can't see any difference (compared to Mr Glazer's previous proposals) other than ¬£200m less debt.The board of Man Utd is expected to meet early this week to discuss the latest proposal from the US tycoon that values the club at ¬£800m ($1.5bn).But the Man Utd supporters club urged the club to reject the new deal.\n",
      "[Generated] ### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:  ### Prompt:\n",
      "You are a helpful summarization assistant. Summarize the following article in 150 words: Mixed reaction to Man Utd offer\n",
      "\n",
      "Shares in Manchester United were up over 5% by noon on Monday following a new offer from Malcolm Glazer.\n",
      "\n",
      "The board of Man Utd is expected to meet early this week to discuss the latest proposal from the US tycoon that values the club at ¬£800m ($1.5bn). Manchester United revealed on Sunday that it had received a detailed proposal from Mr Glazer. A senior source at the club told the BBC: \"This time it's different\". The board is obliged to consider this deal. But the Man Utd supporters club urged the club to reject the new deal. Manchester United past and present footballers Eric Cantona and Ole Gunnar Solskjaer, and club manager Sir Alex Ferguson, have lent their backing to the supporters' group, Shareholders United. They have all spoken out against the bid.\n",
      "\n",
      "A spokesman for the supporters club said: \"I can't see any difference (compared to Mr Glazer's previous proposals) other than ¬£200m less debt. \"He isn't bringing any money into the club; he'll use our money to buy it.\"\n",
      "\n",
      "Mr Glazer's latest move is being led by Mr Glazer's two sons, Avi and Joel, according to the Financial Times. A proposal was received by David Gill, United's chief executive, at the end of last week, pitched at about 300p a share. David Cummings, head of UK equities for Standard Life Investments, said he believed a \"well funded\" 300p a share bid would be enough for Mr Glazer to take control of the club. \"I do not think there is anything that Manchester United fans can do about it,\" he told the BBC. \"They can complain about it but it is curtains for them. They may not want him but they are going to get him.\" The US tycoon, who has been wooing the club for the last 12 months, has approached the United board with \"detailed proposals\", it has confirmed.\n",
      "\n",
      "Mr Glazer, who owns the Tampa Bay Buccaneers team, hopes this will lead to a formal bid being accepted. He is believed to have increased the amount of equity in the new proposal, though it is not clear by how much. For his proposal to succeed, he needs the support of United's largest shareholders, the Irish horseracing tycoons JP McManus and John Magnier. They own 29% of United through their Cubic Expression investment vehicle. Mr Glazer and his family hold a stake of 28.1%. But it is not yet known whether Mr McManus and Mr Magnier would support a Glazer bid. NM Rothschild, the investment bank, is advising Mr Glazer, according to the Financial Times. His previous adviser, JPMorgan, quit last year when Mr Glazer went ahead and voted against the appointment of three United directors to the board, against its advice. But the FT said it thought JP Morgan may still have had some role in financing Mr Glazer's latest financial proposal. Summary: \n",
      "\n",
      "### Summary:\n",
      "Manchester United revealed on Sunday that it had received a detailed proposal from Mr Glazer.But it is not yet known whether Mr McManus and Mr Magnier would support a Glazer bid.David Cummings, head of UK equities for Standard Life Investments, said he believed a \"well funded\" 300p a share bid would be enough for Mr Glazer to take control of the club.But the FT said it thought JP Morgan may still have had some role in financing Mr Glazer's latest financial proposal.His previous adviser, JPMorgan, quit last year when Mr Glazer went ahead and voted against the appointment of three United directors to the board, against its advice.Mr Glazer and his family hold a stake of 28.1%.NM Rothschild, the investment bank, is advising Mr Glazer, according to the Financial Times.Mr Glazer's latest move is being led by Mr Glazer's two sons, Avi and Joel, according to the Financial Times.A spokesman for the supporters club said: \"I can't see any difference (compared to Mr Glazer's previous proposals) other than ¬£200m less debt.The board of Man Utd is expected to meet early this week to discuss the latest proposal from the US tycoon that values the club at ¬£800m ($1.5bn).But the Man Utd supporters club urged the club to reject the new deal. \"I do not think there is anything that Manchester United fans can do about it,\" he told the BBC.A senior source at the club told the BBC: \"This time it's different\".A proposal was received by David Gill, United's chief executive, at the end of last week, pitched at about 300p a share.\n"
     ]
    }
   ],
   "source": [
    "evaluate_after_training(model, val_loader, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finetune with donvanban**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T11:40:33.935551Z",
     "iopub.status.busy": "2025-04-07T11:40:33.935187Z",
     "iopub.status.idle": "2025-04-07T11:40:33.939346Z",
     "shell.execute_reply": "2025-04-07T11:40:33.938465Z",
     "shell.execute_reply.started": "2025-04-07T11:40:33.935523Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"/kaggle/working/llama3-qlora-finetuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T11:40:36.835699Z",
     "iopub.status.busy": "2025-04-07T11:40:36.835379Z",
     "iopub.status.idle": "2025-04-07T11:40:41.626707Z",
     "shell.execute_reply": "2025-04-07T11:40:41.626008Z",
     "shell.execute_reply.started": "2025-04-07T11:40:36.835652Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "# 4. QLoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T11:40:41.628209Z",
     "iopub.status.busy": "2025-04-07T11:40:41.627906Z",
     "iopub.status.idle": "2025-04-07T11:40:41.867622Z",
     "shell.execute_reply": "2025-04-07T11:40:41.866760Z",
     "shell.execute_reply.started": "2025-04-07T11:40:41.628186Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/kaggle/input/data-llama-finetune/vi_data_llama_finetune.json\"\n",
    "\n",
    "full_dataset = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
    "dataset = full_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T11:40:41.869383Z",
     "iopub.status.busy": "2025-04-07T11:40:41.869143Z",
     "iopub.status.idle": "2025-04-07T11:40:43.073794Z",
     "shell.execute_reply": "2025-04-07T11:40:43.072670Z",
     "shell.execute_reply.started": "2025-04-07T11:40:41.869362Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce1cf43234143f28baaf98cb230fe6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe7f7a600d045f1b1d33788cce2d272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(example, max_length=2048):\n",
    "    prompt = f\"### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:  ### L·ªánh:\\n{example['prompt']}\\n\\n### T√≥m t·∫Øt:\\n\"\n",
    "    summary = example[\"summary\"]\n",
    "    \n",
    "    # M√£ h√≥a ri√™ng prompt v√† summary (kh√¥ng th√™m special tokens)\n",
    "    prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    summary_ids = tokenizer.encode(summary, add_special_tokens=False)\n",
    "    \n",
    "    # Ki·ªÉm tra t·ªïng s·ªë token\n",
    "    total_length = len(prompt_ids) + len(summary_ids)\n",
    "    if total_length > max_length:\n",
    "        overflow = total_length - max_length\n",
    "        # ∆Øu ti√™n gi·ªØ l·∫°i ph·∫ßn summary; c·∫Øt b·ªõt prompt\n",
    "        if overflow < len(prompt_ids):\n",
    "            prompt_ids = prompt_ids[:-overflow]\n",
    "        else:\n",
    "            prompt_ids = []  # N·∫øu qu√° tr√†n, b·ªè h·∫øt prompt\n",
    "    \n",
    "    # N·ªëi prompt v√† summary\n",
    "    input_ids = prompt_ids + summary_ids\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    \n",
    "    # T·∫°o labels: ph·∫ßn prompt ƒë∆∞·ª£c mask b·∫±ng -100, ph·∫ßn summary gi·ªØ nguy√™n token IDs\n",
    "    labels = [-100] * len(prompt_ids) + summary_ids\n",
    "    \n",
    "    # Padding t·∫•t c·∫£ c√°c tr∆∞·ªùng v·ªÅ ƒë·ªô d√†i max_length\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "        attention_mask = attention_mask + [0] * padding_length\n",
    "        labels = labels + [-100] * padding_length\n",
    "    else:\n",
    "        # N·∫øu qu√° d√†i, c·∫Øt b·ªõt (n√™n kh√¥ng x·∫£y ra nh·ªù truncation ·ªü tr√™n)\n",
    "        input_ids = input_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "tokenized_dataset = dataset.map(tokenize, batched=False, fn_kwargs={\"max_length\": 2048})\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "collator = default_data_collator\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_dataset[\"test\"],\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T11:40:48.914314Z",
     "iopub.status.busy": "2025-04-07T11:40:48.913997Z",
     "iopub.status.idle": "2025-04-07T11:40:48.930785Z",
     "shell.execute_reply": "2025-04-07T11:40:48.930103Z",
     "shell.execute_reply.started": "2025-04-07T11:40:48.914289Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T11:41:15.929654Z",
     "iopub.status.busy": "2025-04-07T11:41:15.929343Z",
     "iopub.status.idle": "2025-04-07T12:45:29.102807Z",
     "shell.execute_reply": "2025-04-07T12:45:29.101844Z",
     "shell.execute_reply.started": "2025-04-07T11:41:15.929626Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3921: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 72/72 [12:51<00:00, 10.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1 completed ‚Äî Avg loss: 0.5270\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4752: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 72/72 [12:48<00:00, 10.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 2 completed ‚Äî Avg loss: 0.4871\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1806: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 72/72 [12:50<00:00, 10.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 3 completed ‚Äî Avg loss: 0.4527\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2761: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 72/72 [12:51<00:00, 10.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 4 completed ‚Äî Avg loss: 0.4381\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3524: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 72/72 [12:50<00:00, 10.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 5 completed ‚Äî Avg loss: 0.4144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in pbar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"‚úÖ Epoch {epoch+1} completed ‚Äî Avg loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T12:45:29.104503Z",
     "iopub.status.busy": "2025-04-07T12:45:29.104207Z",
     "iopub.status.idle": "2025-04-07T12:45:29.695624Z",
     "shell.execute_reply": "2025-04-07T12:45:29.694800Z",
     "shell.execute_reply.started": "2025-04-07T12:45:29.104481Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Finetuned model saved at: /kaggle/working/llama3-qlora-finetuned-all-1\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-finetuned-all-1\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"‚úÖ Finetuned model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T12:47:47.844662Z",
     "iopub.status.busy": "2025-04-07T12:47:47.844334Z",
     "iopub.status.idle": "2025-04-07T12:47:47.855642Z",
     "shell.execute_reply": "2025-04-07T12:47:47.854781Z",
     "shell.execute_reply.started": "2025-04-07T12:47:47.844633Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_after_training(model, val_loader, tokenizer, device, num_samples=3):\n",
    "    import evaluate\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    samples_to_print = []\n",
    "\n",
    "    # Load ROUGE\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    def left_pad(inputs, pad_token_id):\n",
    "        \"\"\"Chuy·ªÉn batch input th√†nh left-padded\"\"\"\n",
    "        max_len = max(len(seq) for seq in inputs)\n",
    "        return torch.stack([\n",
    "            torch.cat([torch.full((max_len - len(seq),), pad_token_id, dtype=torch.long), seq])\n",
    "            for seq in inputs\n",
    "        ])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            # Chuy·ªÉn sang left padding th·ªß c√¥ng\n",
    "            input_ids = left_pad([x[x != tokenizer.pad_token_id] for x in input_ids], tokenizer.pad_token_id).to(device)\n",
    "            attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "\n",
    "            # Generate\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=128,\n",
    "                num_beams=4,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            # Decode\n",
    "            generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "            label_texts = []\n",
    "            for label in labels:\n",
    "                label_ids = [token_id for token_id in label.tolist() if token_id != -100]\n",
    "                label_texts.append(tokenizer.decode(label_ids, skip_special_tokens=True))\n",
    "\n",
    "            predictions.extend(generated_texts)\n",
    "            references.extend(label_texts)\n",
    "\n",
    "            if len(samples_to_print) < num_samples:\n",
    "                src_texts = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "                for src, ref, pred in zip(src_texts, label_texts, generated_texts):\n",
    "                    samples_to_print.append((src, ref, pred))\n",
    "                    if len(samples_to_print) >= num_samples:\n",
    "                        break\n",
    "\n",
    "    # T√≠nh ROUGE\n",
    "    results = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "\n",
    "    print(\"\\nüìä ROUGE Scores:\")\n",
    "    for key in results:\n",
    "        print(f\"{key}: {results[key]:.4f}\")\n",
    "\n",
    "    print(\"\\nüìù Sample Results:\")\n",
    "    for i, (src, ref, pred) in enumerate(samples_to_print):\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        print(f\"[Prompt]    {src}\")\n",
    "        print(f\"[Reference] {ref}\")\n",
    "        print(f\"[Generated] {pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T12:47:56.995645Z",
     "iopub.status.busy": "2025-04-07T12:47:56.995358Z",
     "iopub.status.idle": "2025-04-07T12:51:36.623267Z",
     "shell.execute_reply": "2025-04-07T12:51:36.622399Z",
     "shell.execute_reply.started": "2025-04-07T12:47:56.995623Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [03:34<00:00, 23.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä ROUGE Scores:\n",
      "rouge1: 0.2992\n",
      "rouge2: 0.2967\n",
      "rougeL: 0.2985\n",
      "rougeLsum: 0.2977\n",
      "\n",
      "üìù Sample Results:\n",
      "\n",
      "--- Sample 1 ---\n",
      "[Prompt]    ### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:  ### L·ªánh:\n",
      "B·∫°n l√† m·ªôt tr·ª£ l√Ω t√≥m t·∫Øt vƒÉn b·∫£n. H√£y cung c·∫•p b·∫£n t√≥m t·∫Øt ng·∫Øn g·ªçn v√† ch√≠nh x√°c trong 150 ch·ªØ cho b√†i vi·∫øt sau. B√†i vi·∫øt: ÔªøT·∫°p ch√≠ M·ªπ b·ªã l√™n √°n v√¨ ch·ª•p ·∫£nh \"th·ªùi trang t·ª± t·ª≠\"\n",
      " T·∫°p ch√≠ Vice c·ªßa M·ªπ v·ª´a b·ªã l√™n √°n v√¨ th·ª±c hi·ªán b·ªô ·∫£nh \"th·ªùi trang t·ª± t·ª≠\", t√°i hi·ªán l·∫°i gi√¢y ph√∫t cu·ªëi ƒë·ªùi c·ªßa nh·ªØng n·ªØ vƒÉn sƒ© n·ªïi ti·∫øng. Trong nh·ªØng nƒÉm g·∫ßn ƒë√¢y, ƒë·ªÉ c·∫°nh tranh v·ªõi nh·ªØng b√°o kh√°c, c√°c t·ªù t·∫°p ch√≠ th∆∞·ªùng m·∫°nh d·∫°n ƒë∆∞a ra nh·ªØng √Ω t∆∞·ªüng m·ªõi l·∫°, nhi·ªÅu khi tr·ªü th√†nh qu√° ƒë√†.  ƒê·ªëi v·ªõi √Ω t∆∞·ªüng s√°ng t·∫°o \"th·∫£m h·ªça\" l·∫ßn n√†y c·ªßa t·∫°p ch√≠ Vice, t·ªù Huffington Post ƒë√°nh gi√° r·∫±ng: \"Vice nghƒ© h·ªç ƒëang t·∫°o ra nh·ªØng b·ª©c ·∫£nh ngh·ªá thu·∫≠t nh∆∞ng cu·ªëi c√πng ng∆∞·ªùi ta ch·ªâ th·∫•y ƒë√≥ l√† nh·ªØng b·ª©c ·∫£nh thi·∫øu suy nghƒ© v√† ph·∫£n c·∫£m\". Sau khi nh·ªØng b·ª©c h√¨nh t·ª± t·ª≠ n√†y b·ªã x√≥a ƒëi, Vice ƒë√£ ƒëƒÉng t·∫£i l·ªùi xin l·ªói: \"Nh·ªØng b·ª©c ·∫£nh th·ªùi trang c·ªßa Vice lu√¥n phi truy·ªÅn th·ªëng v√† ti·∫øp c·∫≠n t·ª´ g√≥c nh√¨n ngh·ªá thu·∫≠t h∆°n l√† nh·ªØng b·ª©c ·∫£nh thu·∫ßn t√∫y gi·ªõi thi·ªáu xu h∆∞·ªõng th·ªùi trang. M·ª•c ti√™u c·ªßa ch√∫ng t√¥i l√† t·∫°o ra nh·ªØng b·ª©c h√¨nh ngh·ªá thu·∫≠t ƒë·ªÉ t·ª´ ƒë√≥ th√¥ng ƒëi·ªáp th·ªùi trang s·∫Ω ƒëi theo sau, thay v√¨ ƒëi tr∆∞·ªõc nh∆∞ nhi·ªÅu t·ªù b√°o kh√°c. B·ªô h√¨nh \"Last Words\" (Nh·ªØng l·ªùi cu·ªëi c√πng) ƒë∆∞·ª£c t·∫°o ra theo t∆∞ duy n√†y v√† t·∫≠p trung v√†o c√°i ch·∫øt c·ªßa nh·ªØng n·ªØ vƒÉn sƒ© n·ªïi ti·∫øng m√† c√°c ƒë·ªôc gi·∫£ y√™u qu√Ω h·ªç h·∫≥n ƒë·ªÅu mong r·∫±ng gi√° nh∆∞ cu·ªôc ƒë·ªùi c·ªßa nh·ªØng t√†i nƒÉng n√†y ƒë·ª´ng ng·∫Øn ng·ªßi nh∆∞ v·∫≠y\". L·ªùi xin l·ªói c·ªßa Vice b·ªã cho l√† kh√¥ng thuy·∫øt ph·ª•c. Th·∫≠t kh√≥ hi·ªÉu t·∫°i sao ban bi√™n t·∫≠p c·ªßa b√°o c√≥ th·ªÉ ƒë·ªÉ nh·ªØng b·ª©c ·∫£nh n√†y xu·∫•t hi·ªán tr√™n trang c·ªßa m√¨nh. T·ªù Huffington Post nh·∫≠n ƒë·ªãnh th√™m: Vice ƒëang c·ªë t√¨nh gi·∫£i th√≠ch l√≤ng v√≤ng xung quanh vi·ªác th·ª±c hi·ªán m·ªôt b·ªô ·∫£nh v√¥ nghƒ©a. ƒê·ªëi v·ªõi m·ªôt nh√† vƒÉn, h·ªç ƒë∆∞·ª£c c√¥ng ch√∫ng nh·ªõ t·ªõi nh·ªù nh·ªØng t√°c ph·∫©m hay. T·∫°i sao Vice kh√¥ng t√¥n vinh nh·ªØng t√°c ph·∫©m ƒë√≥ m√† l·∫°i nh·∫•n m·∫°nh v√†o nh·ªØng gi√¢y ph√∫t cu·ªëi ƒë·ªùi ƒëen t·ªëi, c√¥ ƒë·ªôc, v·ªõi ƒë·∫ßy n·ªói s·ª£ h√£i v√† bu·ªìn kh·ªï nh∆∞ v·∫≠y? ƒê·ªëi v·ªõi t·∫•t c·∫£ nh·ªØng ai t·ª´ng tr·∫£i qua t√¢m tr·∫°ng bi ƒë√°t t·ªõi m·ª©c mu·ªën t·ª± t·ª≠ ho·∫∑c c√≥ ng∆∞·ªùi th√¢n t·ª´ng t·ª± t·ª≠, h·∫≥n h·ªç s·∫Ω c·∫£m th·∫•y v√¥ c√πng kh√≥ ch·ªãu tr∆∞·ªõc nh·ªØng b·ª©c ·∫£nh n√†y. \n",
      "Khoa h·ªçc ƒë√£ ch·ª©ng minh r·∫±ng, ƒë·ªëi v·ªõi nh·ªØng ai t·ª´ng xu·∫•t hi·ªán trong ƒë·∫ßu √Ω nghƒ© ƒëen t·ªëi v·ªÅ vi·ªác t·ª± t·ª≠, vi·ªác b√°o ch√≠ ho·∫∑c truy·ªÅn h√¨nh ƒë∆∞a ra nh·ªØng h√¨nh ·∫£nh tr·ª±c quan v·ªÅ h√†nh ƒë·ªông n√†y kh√¥ng kh√°c g√¨ nhen nh√≥m l·∫°i √Ω t∆∞·ªüng ƒë√≥ trong ƒë·∫ßu ng∆∞·ªùi xem. ƒê·ªëi v·ªõi nh·ªØng n·ªØ nh√† vƒÉn m√† t·ªù Vice ƒë·ªÅ c·∫≠p t·ªõi, c√°ch ti·∫øp c·∫≠n ƒë·ªÅ t√†i c·ªßa h·ªç b·ªã ƒë√°nh gi√° l√† thi·∫øu t√¥n tr·ªçng ng∆∞·ªùi ƒë√£ khu·∫•t. T√≥m t·∫Øt: \n",
      "\n",
      "### T√≥m t·∫Øt:\n",
      "ÔªøT·∫°p ch√≠ Vice c·ªßa M·ªπ  v·ª´a b·ªã l√™n √°n v√¨ th·ª±c hi·ªán b·ªô ·∫£nh \"th·ªùi trang t·ª± t·ª≠\", t√°i hi·ªán l·∫°i gi√¢y ph√∫t cu·ªëi ƒë·ªùi c·ªßa nh·ªØng n·ªØ vƒÉn sƒ© n·ªïi ti·∫øng. Trong nh·ªØng nƒÉm g·∫ßn d√¢y, c√°c t·ªù t·∫°p ch√≠ th∆∞·ªùnng m·∫°nh d·∫°n ƒë∆∞a ra nh·ªØng √Ω t∆∞·ªüng m·ªõi l·∫°, nhi·ªÅu khi tr·ªü th√†nh qu√° ƒë√†. Sau khi nh·ªØng b·ª©c h√¨nh t·ª± t·ª≠ n√†y b·ªã x√≥a ƒëi, Vice ƒë√£ ƒëƒÉng t·∫£i l·ªùi xin l·ªói, b·ªã cho l√† kh√¥ng thuy·∫øt ph·ª•c. Khoa h·ªçc ƒë√£ ch·ª©ng minh, ƒë·ªëi v·ªõi nh·ªØng ai t·ª´ng xu·∫•t hi·ªán trong ƒë·∫ßu √Ω nghƒ© ƒëen t·ªëi v·ªÅ vi·ªác t·ª± t·ª≠, vi·ªác b√°o ch√≠ ho·∫∑c truy·ªÅn h√¨nh ƒë∆∞a ra nh·ªØng h√¨nh ·∫£nh tr·ª±c quan v·ªÅ h√†nh ƒë·ªông n√†y kh√¥ng kh√°c g√¨ nhen nh√≥m l·∫°i √Ω t∆∞·ªüng ƒë√≥ trong ƒë·∫ßu ng∆∞·ªùi xem. C√°ch ti·∫øp c·∫≠n ƒë·ªÅ t√†i  b·ªã ƒë√°nh gi√° l√† thi·∫øu t√¥n tr·ªçng ng∆∞·ªùi ƒë√£ khu·∫•t.\n",
      "[Reference] ÔªøT·∫°p ch√≠ Vice c·ªßa M·ªπ  v·ª´a b·ªã l√™n √°n v√¨ th·ª±c hi·ªán b·ªô ·∫£nh \"th·ªùi trang t·ª± t·ª≠\", t√°i hi·ªán l·∫°i gi√¢y ph√∫t cu·ªëi ƒë·ªùi c·ªßa nh·ªØng n·ªØ vƒÉn sƒ© n·ªïi ti·∫øng. Trong nh·ªØng nƒÉm g·∫ßn d√¢y, c√°c t·ªù t·∫°p ch√≠ th∆∞·ªùnng m·∫°nh d·∫°n ƒë∆∞a ra nh·ªØng √Ω t∆∞·ªüng m·ªõi l·∫°, nhi·ªÅu khi tr·ªü th√†nh qu√° ƒë√†. Sau khi nh·ªØng b·ª©c h√¨nh t·ª± t·ª≠ n√†y b·ªã x√≥a ƒëi, Vice ƒë√£ ƒëƒÉng t·∫£i l·ªùi xin l·ªói, b·ªã cho l√† kh√¥ng thuy·∫øt ph·ª•c. Khoa h·ªçc ƒë√£ ch·ª©ng minh, ƒë·ªëi v·ªõi nh·ªØng ai t·ª´ng xu·∫•t hi·ªán trong ƒë·∫ßu √Ω nghƒ© ƒëen t·ªëi v·ªÅ vi·ªác t·ª± t·ª≠, vi·ªác b√°o ch√≠ ho·∫∑c truy·ªÅn h√¨nh ƒë∆∞a ra nh·ªØng h√¨nh ·∫£nh tr·ª±c quan v·ªÅ h√†nh ƒë·ªông n√†y kh√¥ng kh√°c g√¨ nhen nh√≥m l·∫°i √Ω t∆∞·ªüng ƒë√≥ trong ƒë·∫ßu ng∆∞·ªùi xem. C√°ch ti·∫øp c·∫≠n ƒë·ªÅ t√†i  b·ªã ƒë√°nh gi√° l√† thi·∫øu t√¥n tr·ªçng ng∆∞·ªùi ƒë√£ khu·∫•t.\n",
      "[Generated] ### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:  ### L·ªánh:\n",
      "B·∫°n l√† m·ªôt tr·ª£ l√Ω t√≥m t·∫Øt vƒÉn b·∫£n. H√£y cung c·∫•p b·∫£n t√≥m t·∫Øt ng·∫Øn g·ªçn v√† ch√≠nh x√°c trong 150 ch·ªØ cho b√†i vi·∫øt sau. B√†i vi·∫øt: ÔªøT·∫°p ch√≠ M·ªπ b·ªã l√™n √°n v√¨ ch·ª•p ·∫£nh \"th·ªùi trang t·ª± t·ª≠\"\n",
      " T·∫°p ch√≠ Vice c·ªßa M·ªπ v·ª´a b·ªã l√™n √°n v√¨ th·ª±c hi·ªán b·ªô ·∫£nh \"th·ªùi trang t·ª± t·ª≠\", t√°i hi·ªán l·∫°i gi√¢y ph√∫t cu·ªëi ƒë·ªùi c·ªßa nh·ªØng n·ªØ vƒÉn sƒ© n·ªïi ti·∫øng. Trong nh·ªØng nƒÉm g·∫ßn ƒë√¢y, ƒë·ªÉ c·∫°nh tranh v·ªõi nh·ªØng b√°o kh√°c, c√°c t·ªù t·∫°p ch√≠ th∆∞·ªùng m·∫°nh d·∫°n ƒë∆∞a ra nh·ªØng √Ω t∆∞·ªüng m·ªõi l·∫°, nhi·ªÅu khi tr·ªü th√†nh qu√° ƒë√†.  ƒê·ªëi v·ªõi √Ω t∆∞·ªüng s√°ng t·∫°o \"th·∫£m h·ªça\" l·∫ßn n√†y c·ªßa t·∫°p ch√≠ Vice, t·ªù Huffington Post ƒë√°nh gi√° r·∫±ng: \"Vice nghƒ© h·ªç ƒëang t·∫°o ra nh·ªØng b·ª©c ·∫£nh ngh·ªá thu·∫≠t nh∆∞ng cu·ªëi c√πng ng∆∞·ªùi ta ch·ªâ th·∫•y ƒë√≥ l√† nh·ªØng b·ª©c ·∫£nh thi·∫øu suy nghƒ© v√† ph·∫£n c·∫£m\". Sau khi nh·ªØng b·ª©c h√¨nh t·ª± t·ª≠ n√†y b·ªã x√≥a ƒëi, Vice ƒë√£ ƒëƒÉng t·∫£i l·ªùi xin l·ªói: \"Nh·ªØng b·ª©c ·∫£nh th·ªùi trang c·ªßa Vice lu√¥n phi truy·ªÅn th·ªëng v√† ti·∫øp c·∫≠n t·ª´ g√≥c nh√¨n ngh·ªá thu·∫≠t h∆°n l√† nh·ªØng b·ª©c ·∫£nh thu·∫ßn t√∫y gi·ªõi thi·ªáu xu h∆∞·ªõng th·ªùi trang. M·ª•c ti√™u c·ªßa ch√∫ng t√¥i l√† t·∫°o ra nh·ªØng b·ª©c h√¨nh ngh·ªá thu·∫≠t ƒë·ªÉ t·ª´ ƒë√≥ th√¥ng ƒëi·ªáp th·ªùi trang s·∫Ω ƒëi theo sau, thay v√¨ ƒëi tr∆∞·ªõc nh∆∞ nhi·ªÅu t·ªù b√°o kh√°c. B·ªô h√¨nh \"Last Words\" (Nh·ªØng l·ªùi cu·ªëi c√πng) ƒë∆∞·ª£c t·∫°o ra theo t∆∞ duy n√†y v√† t·∫≠p trung v√†o c√°i ch·∫øt c·ªßa nh·ªØng n·ªØ vƒÉn sƒ© n·ªïi ti·∫øng m√† c√°c ƒë·ªôc gi·∫£ y√™u qu√Ω h·ªç h·∫≥n ƒë·ªÅu mong r·∫±ng gi√° nh∆∞ cu·ªôc ƒë·ªùi c·ªßa nh·ªØng t√†i nƒÉng n√†y ƒë·ª´ng ng·∫Øn ng·ªßi nh∆∞ v·∫≠y\". L·ªùi xin l·ªói c·ªßa Vice b·ªã cho l√† kh√¥ng thuy·∫øt ph·ª•c. Th·∫≠t kh√≥ hi·ªÉu t·∫°i sao ban bi√™n t·∫≠p c·ªßa b√°o c√≥ th·ªÉ ƒë·ªÉ nh·ªØng b·ª©c ·∫£nh n√†y xu·∫•t hi·ªán tr√™n trang c·ªßa m√¨nh. T·ªù Huffington Post nh·∫≠n ƒë·ªãnh th√™m: Vice ƒëang c·ªë t√¨nh gi·∫£i th√≠ch l√≤ng v√≤ng xung quanh vi·ªác th·ª±c hi·ªán m·ªôt b·ªô ·∫£nh v√¥ nghƒ©a. ƒê·ªëi v·ªõi m·ªôt nh√† vƒÉn, h·ªç ƒë∆∞·ª£c c√¥ng ch√∫ng nh·ªõ t·ªõi nh·ªù nh·ªØng t√°c ph·∫©m hay. T·∫°i sao Vice kh√¥ng t√¥n vinh nh·ªØng t√°c ph·∫©m ƒë√≥ m√† l·∫°i nh·∫•n m·∫°nh v√†o nh·ªØng gi√¢y ph√∫t cu·ªëi ƒë·ªùi ƒëen t·ªëi, c√¥ ƒë·ªôc, v·ªõi ƒë·∫ßy n·ªói s·ª£ h√£i v√† bu·ªìn kh·ªï nh∆∞ v·∫≠y? ƒê·ªëi v·ªõi t·∫•t c·∫£ nh·ªØng ai t·ª´ng tr·∫£i qua t√¢m tr·∫°ng bi ƒë√°t t·ªõi m·ª©c mu·ªën t·ª± t·ª≠ ho·∫∑c c√≥ ng∆∞·ªùi th√¢n t·ª´ng t·ª± t·ª≠, h·∫≥n h·ªç s·∫Ω c·∫£m th·∫•y v√¥ c√πng kh√≥ ch·ªãu tr∆∞·ªõc nh·ªØng b·ª©c ·∫£nh n√†y. \n",
      "Khoa h·ªçc ƒë√£ ch·ª©ng minh r·∫±ng, ƒë·ªëi v·ªõi nh·ªØng ai t·ª´ng xu·∫•t hi·ªán trong ƒë·∫ßu √Ω nghƒ© ƒëen t·ªëi v·ªÅ vi·ªác t·ª± t·ª≠, vi·ªác b√°o ch√≠ ho·∫∑c truy·ªÅn h√¨nh ƒë∆∞a ra nh·ªØng h√¨nh ·∫£nh tr·ª±c quan v·ªÅ h√†nh ƒë·ªông n√†y kh√¥ng kh√°c g√¨ nhen nh√≥m l·∫°i √Ω t∆∞·ªüng ƒë√≥ trong ƒë·∫ßu ng∆∞·ªùi xem. ƒê·ªëi v·ªõi nh·ªØng n·ªØ nh√† vƒÉn m√† t·ªù Vice ƒë·ªÅ c·∫≠p t·ªõi, c√°ch ti·∫øp c·∫≠n ƒë·ªÅ t√†i c·ªßa h·ªç b·ªã ƒë√°nh gi√° l√† thi·∫øu t√¥n tr·ªçng ng∆∞·ªùi ƒë√£ khu·∫•t. T√≥m t·∫Øt: \n",
      "\n",
      "### T√≥m t·∫Øt:\n",
      "ÔªøT·∫°p ch√≠ Vice c·ªßa M·ªπ  v·ª´a b·ªã l√™n √°n v√¨ th·ª±c hi·ªán b·ªô ·∫£nh \"th·ªùi trang t·ª± t·ª≠\", t√°i hi·ªán l·∫°i gi√¢y ph√∫t cu·ªëi ƒë·ªùi c·ªßa nh·ªØng n·ªØ vƒÉn sƒ© n·ªïi ti·∫øng. Trong nh·ªØng nƒÉm g·∫ßn d√¢y, c√°c t·ªù t·∫°p ch√≠ th∆∞·ªùnng m·∫°nh d·∫°n ƒë∆∞a ra nh·ªØng √Ω t∆∞·ªüng m·ªõi l·∫°, nhi·ªÅu khi tr·ªü th√†nh qu√° ƒë√†. Sau khi nh·ªØng b·ª©c h√¨nh t·ª± t·ª≠ n√†y b·ªã x√≥a ƒëi, Vice ƒë√£ ƒëƒÉng t·∫£i l·ªùi xin l·ªói, b·ªã cho l√† kh√¥ng thuy·∫øt ph·ª•c. Khoa h·ªçc ƒë√£ ch·ª©ng minh, ƒë·ªëi v·ªõi nh·ªØng ai t·ª´ng xu·∫•t hi·ªán trong ƒë·∫ßu √Ω nghƒ© ƒëen t·ªëi v·ªÅ vi·ªác t·ª± t·ª≠, vi·ªác b√°o ch√≠ ho·∫∑c truy·ªÅn h√¨nh ƒë∆∞a ra nh·ªØng h√¨nh ·∫£nh tr·ª±c quan v·ªÅ h√†nh ƒë·ªông n√†y kh√¥ng kh√°c g√¨ nhen nh√≥m l·∫°i √Ω t∆∞·ªüng ƒë√≥ trong ƒë·∫ßu ng∆∞·ªùi xem. C√°ch ti·∫øp c·∫≠n ƒë·ªÅ t√†i  b·ªã ƒë√°nh gi√° l√† thi·∫øu t√¥n tr·ªçng ng∆∞·ªùi ƒë√£ khu·∫•t. T√≥m t·∫Øt: T·∫°p ch√≠ Vice c·ªßa M·ªπ v·ª´a b·ªã l√™n √°n v√¨ th·ª±c hi·ªán b·ªô ·∫£nh \"th·ªùi trang t·ª± t·ª≠\", t√°i hi·ªán l·∫°i gi√¢y ph√∫t cu·ªëi ƒë·ªùi c·ªßa nh·ªØng n·ªØ vƒÉn sƒ© n·ªïi ti·∫øng. Trong nh·ªØng nƒÉm g·∫ßn d√¢y, c√°c t·ªù t·∫°p ch√≠ th∆∞·ªùnng m·∫°nh d·∫°n ƒë∆∞a ra nh·ªØng √Ω t∆∞·ªüng m·ªõi l·∫°, nhi·ªÅu khi tr·ªü th√†nh qu√° ƒë√†. Sau khi nh·ªØng b·ª©c h√¨nh t·ª± t·ª≠ n√†y b·ªã x√≥a ƒëi, Vice ƒë√£ ƒëƒÉng t·∫£i l·ªùi xin l·ªói, b·ªã cho l√† kh√¥ng thuy·∫øt ph·ª•c. Khoa h·ªçc ƒë√£ ch·ª©ng minh, ƒë·ªëi v·ªõi nh·ªØng ai t·ª´ng xu·∫•t hi·ªán trong ƒë·∫ßu √Ω nghƒ© ƒëen t·ªëi v·ªÅ vi·ªác t·ª± t·ª≠, vi·ªác b√°o\n",
      "\n",
      "--- Sample 2 ---\n",
      "[Prompt]    ### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:  ### L·ªánh:\n",
      "B·∫°n l√† m·ªôt tr·ª£ l√Ω t√≥m t·∫Øt vƒÉn b·∫£n. H√£y cung c·∫•p b·∫£n t√≥m t·∫Øt ng·∫Øn g·ªçn v√† ch√≠nh x√°c trong 150 ch·ªØ cho b√†i vi·∫øt sau. B√†i vi·∫øt: ÔªøL·ªëc xo√°y, s√©t ƒë√°nh nhi·ªÅu nh√† tan t√†nh.\n",
      "M·ªôt tr·∫≠n m∆∞a l·ªõn, k√®m theo l·ªëc xo√°y v√† nh·ªØng ti·∫øng n·ªï l·ªõn x·∫£y ra tr√™n ƒë·ªãa b√†n x√£ mi·ªÅn n√∫i Qu·ª≥ Ch√¢u, Ngh·ªá An l√†m nhi·ªÅu ng∆∞·ªùi d√¢n ph√°t ho·∫£ng.\n",
      "V√†o kho·∫£ng 19h t·ªëi qua ( 20/4 ), ng∆∞·ªùi ·ªü b·∫£n X·ªëp H·ªëc, x√£ Di√™n L√£m, huy·ªán Qu·ª≥ Ch√¢u ( Ngh·ªá An ) ƒëang chu·∫©n b·ªã b·ªØa t·ªëi th√¨ tr·ªùi b·∫Øt ƒë·∫ßu c√≥ m∆∞a k√®m theo gi√≥ l·ªëc.\n",
      "C∆°n m∆∞a n·∫∑ng h·∫°t v√† b·∫•t ng·ªù m·ªôt ti·∫øng n·ªï l·ªõn vang l√™n l√†m rung chuy·ªÉn c·∫£ m·ªôt khu v·ª±c.\n",
      "C√πng l√∫c ƒë√≥ nhi·ªÅu ng∆∞·ªùi nh√¨n th·∫•y kh√≥i v√† m√πi kh√©t l·∫πt b·ªëc l√™n nghi ng√∫t t·∫°i 3 cƒÉn nh√† l√† nh√† anh H√† VƒÉn Chung, Vi VƒÉn Ph∆∞∆°ng v√† L∆∞∆°ng VƒÉn Chung.\n",
      "Sau khi ho√†n h·ªìn, m·ªçi ng∆∞·ªùi th·∫•y nh√† c·ªßa 3 gia ƒë√¨nh n√≥i tr√™n b·ªã h∆∞ h·ªèng n·∫∑ng, m√°i nh√† l·ª£p b·∫±ng phibroximƒÉng b·ªã v·ª° v·ª•n r∆°i xu·ªëng ƒë·∫•t h√†ng ch·ª•c t·∫•m, nhi·ªÅu ƒë·ªì ƒë·∫°c trong nh√† nh∆∞ tivi, ƒë√†i, ·∫•m ch√©n v√† c√°c lo·∫°i v·∫≠t d·ª•ng kh√°c trong nh√† b·ªã h∆∞ h·ªèng ho√†n to√†n.\n",
      "D√π h·∫ßu h·∫øt c√°c th√†nh vi√™n c·ªßa 3 gia ƒë√¨nh b·ªã s√©t ƒë√°nh h∆∞ h·ªèng n√≥i tr√™n ƒë·ªÅu ƒëang ·ªü trong nh√† nh∆∞ng r·∫•t may kh√¥ng c√≥ ai b·ªã th∆∞∆°ng.\n",
      "\" T√¥i ƒëang l√∫i h√∫i trong nh√† ƒë·ªÉ n·∫•u c∆°m cho 2 ƒë·ª©a con ƒÉn b·ªØa t·ªëi, b·∫•t ng·ªù m·ªôt ti·∫øng \" s·∫ßm \" l·ªõn l√†m rung chuy·ªÉn cƒÉn nh√† s√†n.\n",
      "Nh·ªØng m·∫£nh v·ª° c·ªßa t·∫•m l·ª£p phibroximƒÉng r∆°i tung t√≥e kh·∫Øp nh√†.\n",
      "Hai ƒë·ª©a con nh·ªè kh√≥c th√©t l√™n.\n",
      "R·∫•t may 3 m·∫π con t√¥i kh√¥ng vi·ªác g√¨ \", ch·ªã Vi Th·ªã Ki·ªÅu, n·∫°n nh√¢n c·ªßa v·ª• s√©t ƒë√°nh t·ªëi qua t·∫°i huy·ªán Qu·ª≥ Ch√¢u ch∆∞a h·∫øt b√†ng ho√†ng k·ªÉ l·∫°i.\n",
      "√îng L√Ω ƒê·∫°i Ch√¢u - Ch·ªß t·ªãch UBND x√£ Di√™n L√£m, cho bi·∫øt : \" Chi·ªÅu t·ªëi qua ( 20/4 ), t·∫°i hai x√£ Ch√¢u Ho√†n v√† Di√™n L√£m c√≥ x·∫©y ra m·ªôt tr·∫≠n l·ªëc xo√°y l√†m h∆∞ h·ªèng nhi·ªÅu t√†i s·∫£n, t·ªëc m√°i nhi·ªÅu cƒÉn nh√†.\n",
      "Hi·ªán, ch√∫ng t√¥i ƒëang ch·ªâ ƒë·∫°o anh em vƒÉn ph√≤ng ph·ªëi h·ª£p v·ªõi c√°c tr∆∞·ªüng b·∫£n ti·∫øn h√†nh ki·ªÉm tra th·ªëng k√™ v√† b√°o c√°o thi·ªát h·∫°i l√™n UBND huy·ªán v√†o s√°ng th·ª© 2 tu·∫ßn t·ªõi.\n",
      "Ri√™ng tr∆∞·ªùng h·ª£p 3 cƒÉn nh√† ·ªü b·∫£n X·ªëp H·ªëc b·ªã s√©t ƒë√°nh t√¥i c≈©ng v·ª´a m·ªõi nghe qua nh∆∞ng ch∆∞a r√µ t√¨nh h√¨nh thi·ªát h·∫°i th·∫ø n√†o \". T√≥m t·∫Øt: \n",
      "\n",
      "### T√≥m t·∫Øt:\n",
      "ÔªøM·ªôt tr·∫≠n m∆∞a l·ªõn, k√®m theo l·ªëc xo√°y v√† nh·ªØng ti·∫øng n·ªï l·ªõn x·∫£y ra tr√™n ƒë·ªãa b√†n x√£ mi·ªÅn n√∫i Qu·ª≥ Ch√¢u, Ngh·ªá An l√†m nhi·ªÅu ng∆∞·ªùi d√¢n ph√°t ho·∫£ng. \n",
      "Nhi·ªÅu ng∆∞·ªùi nh√¨n th·∫•y kh√≥i v√† m√πi kh√©t l·∫πt b·ªëc l√™n nghi ng√∫t t·∫°i 3 cƒÉn nh√† l√† nh√† anh H√† VƒÉn Chung,Vi VƒÉn Ph∆∞∆°ng v√†  L∆∞∆°ng VƒÉChung. \n",
      "M·ªçi ng∆∞·ªùi th·∫•y nh√† c·ªßa 3 gia ƒë√¨nh n√≥i tr√™n b·ªã h∆∞ h·ªèng n·∫∑ng.  \n",
      "D√π h·∫ßu h·∫øt c√°c th√†nh vi√™n c·ªßa 3 gia ƒë√¨nh b·ªã s√©t ƒë√°nh h∆∞ h·ªèng n√≥i tr√™n ƒë·ªÅu ƒëang ·ªü trong nh√† nh∆∞ng r·∫•t may kh√¥ng c√≥ ai b·ªã th∆∞∆°ng.\n",
      "[Reference] ÔªøM·ªôt tr·∫≠n m∆∞a l·ªõn, k√®m theo l·ªëc xo√°y v√† nh·ªØng ti·∫øng n·ªï l·ªõn x·∫£y ra tr√™n ƒë·ªãa b√†n x√£ mi·ªÅn n√∫i Qu·ª≥ Ch√¢u, Ngh·ªá An l√†m nhi·ªÅu ng∆∞·ªùi d√¢n ph√°t ho·∫£ng. \n",
      "Nhi·ªÅu ng∆∞·ªùi nh√¨n th·∫•y kh√≥i v√† m√πi kh√©t l·∫πt b·ªëc l√™n nghi ng√∫t t·∫°i 3 cƒÉn nh√† l√† nh√† anh H√† VƒÉn Chung,Vi VƒÉn Ph∆∞∆°ng v√†  L∆∞∆°ng VƒÉChung. \n",
      "M·ªçi ng∆∞·ªùi th·∫•y nh√† c·ªßa 3 gia ƒë√¨nh n√≥i tr√™n b·ªã h∆∞ h·ªèng n·∫∑ng.  \n",
      "D√π h·∫ßu h·∫øt c√°c th√†nh vi√™n c·ªßa 3 gia ƒë√¨nh b·ªã s√©t ƒë√°nh h∆∞ h·ªèng n√≥i tr√™n ƒë·ªÅu ƒëang ·ªü trong nh√† nh∆∞ng r·∫•t may kh√¥ng c√≥ ai b·ªã th∆∞∆°ng.\n",
      "[Generated] ### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:  ### L·ªánh:\n",
      "B·∫°n l√† m·ªôt tr·ª£ l√Ω t√≥m t·∫Øt vƒÉn b·∫£n. H√£y cung c·∫•p b·∫£n t√≥m t·∫Øt ng·∫Øn g·ªçn v√† ch√≠nh x√°c trong 150 ch·ªØ cho b√†i vi·∫øt sau. B√†i vi·∫øt: ÔªøL·ªëc xo√°y, s√©t ƒë√°nh nhi·ªÅu nh√† tan t√†nh.\n",
      "M·ªôt tr·∫≠n m∆∞a l·ªõn, k√®m theo l·ªëc xo√°y v√† nh·ªØng ti·∫øng n·ªï l·ªõn x·∫£y ra tr√™n ƒë·ªãa b√†n x√£ mi·ªÅn n√∫i Qu·ª≥ Ch√¢u, Ngh·ªá An l√†m nhi·ªÅu ng∆∞·ªùi d√¢n ph√°t ho·∫£ng.\n",
      "V√†o kho·∫£ng 19h t·ªëi qua ( 20/4 ), ng∆∞·ªùi ·ªü b·∫£n X·ªëp H·ªëc, x√£ Di√™n L√£m, huy·ªán Qu·ª≥ Ch√¢u ( Ngh·ªá An ) ƒëang chu·∫©n b·ªã b·ªØa t·ªëi th√¨ tr·ªùi b·∫Øt ƒë·∫ßu c√≥ m∆∞a k√®m theo gi√≥ l·ªëc.\n",
      "C∆°n m∆∞a n·∫∑ng h·∫°t v√† b·∫•t ng·ªù m·ªôt ti·∫øng n·ªï l·ªõn vang l√™n l√†m rung chuy·ªÉn c·∫£ m·ªôt khu v·ª±c.\n",
      "C√πng l√∫c ƒë√≥ nhi·ªÅu ng∆∞·ªùi nh√¨n th·∫•y kh√≥i v√† m√πi kh√©t l·∫πt b·ªëc l√™n nghi ng√∫t t·∫°i 3 cƒÉn nh√† l√† nh√† anh H√† VƒÉn Chung, Vi VƒÉn Ph∆∞∆°ng v√† L∆∞∆°ng VƒÉn Chung.\n",
      "Sau khi ho√†n h·ªìn, m·ªçi ng∆∞·ªùi th·∫•y nh√† c·ªßa 3 gia ƒë√¨nh n√≥i tr√™n b·ªã h∆∞ h·ªèng n·∫∑ng, m√°i nh√† l·ª£p b·∫±ng phibroximƒÉng b·ªã v·ª° v·ª•n r∆°i xu·ªëng ƒë·∫•t h√†ng ch·ª•c t·∫•m, nhi·ªÅu ƒë·ªì ƒë·∫°c trong nh√† nh∆∞ tivi, ƒë√†i, ·∫•m ch√©n v√† c√°c lo·∫°i v·∫≠t d·ª•ng kh√°c trong nh√† b·ªã h∆∞ h·ªèng ho√†n to√†n.\n",
      "D√π h·∫ßu h·∫øt c√°c th√†nh vi√™n c·ªßa 3 gia ƒë√¨nh b·ªã s√©t ƒë√°nh h∆∞ h·ªèng n√≥i tr√™n ƒë·ªÅu ƒëang ·ªü trong nh√† nh∆∞ng r·∫•t may kh√¥ng c√≥ ai b·ªã th∆∞∆°ng.\n",
      "\" T√¥i ƒëang l√∫i h√∫i trong nh√† ƒë·ªÉ n·∫•u c∆°m cho 2 ƒë·ª©a con ƒÉn b·ªØa t·ªëi, b·∫•t ng·ªù m·ªôt ti·∫øng \" s·∫ßm \" l·ªõn l√†m rung chuy·ªÉn cƒÉn nh√† s√†n.\n",
      "Nh·ªØng m·∫£nh v·ª° c·ªßa t·∫•m l·ª£p phibroximƒÉng r∆°i tung t√≥e kh·∫Øp nh√†.\n",
      "Hai ƒë·ª©a con nh·ªè kh√≥c th√©t l√™n.\n",
      "R·∫•t may 3 m·∫π con t√¥i kh√¥ng vi·ªác g√¨ \", ch·ªã Vi Th·ªã Ki·ªÅu, n·∫°n nh√¢n c·ªßa v·ª• s√©t ƒë√°nh t·ªëi qua t·∫°i huy·ªán Qu·ª≥ Ch√¢u ch∆∞a h·∫øt b√†ng ho√†ng k·ªÉ l·∫°i.\n",
      "√îng L√Ω ƒê·∫°i Ch√¢u - Ch·ªß t·ªãch UBND x√£ Di√™n L√£m, cho bi·∫øt : \" Chi·ªÅu t·ªëi qua ( 20/4 ), t·∫°i hai x√£ Ch√¢u Ho√†n v√† Di√™n L√£m c√≥ x·∫©y ra m·ªôt tr·∫≠n l·ªëc xo√°y l√†m h∆∞ h·ªèng nhi·ªÅu t√†i s·∫£n, t·ªëc m√°i nhi·ªÅu cƒÉn nh√†.\n",
      "Hi·ªán, ch√∫ng t√¥i ƒëang ch·ªâ ƒë·∫°o anh em vƒÉn ph√≤ng ph·ªëi h·ª£p v·ªõi c√°c tr∆∞·ªüng b·∫£n ti·∫øn h√†nh ki·ªÉm tra th·ªëng k√™ v√† b√°o c√°o thi·ªát h·∫°i l√™n UBND huy·ªán v√†o s√°ng th·ª© 2 tu·∫ßn t·ªõi.\n",
      "Ri√™ng tr∆∞·ªùng h·ª£p 3 cƒÉn nh√† ·ªü b·∫£n X·ªëp H·ªëc b·ªã s√©t ƒë√°nh t√¥i c≈©ng v·ª´a m·ªõi nghe qua nh∆∞ng ch∆∞a r√µ t√¨nh h√¨nh thi·ªát h·∫°i th·∫ø n√†o \". T√≥m t·∫Øt: \n",
      "\n",
      "### T√≥m t·∫Øt:\n",
      "ÔªøM·ªôt tr·∫≠n m∆∞a l·ªõn, k√®m theo l·ªëc xo√°y v√† nh·ªØng ti·∫øng n·ªï l·ªõn x·∫£y ra tr√™n ƒë·ªãa b√†n x√£ mi·ªÅn n√∫i Qu·ª≥ Ch√¢u, Ngh·ªá An l√†m nhi·ªÅu ng∆∞·ªùi d√¢n ph√°t ho·∫£ng. \n",
      "Nhi·ªÅu ng∆∞·ªùi nh√¨n th·∫•y kh√≥i v√† m√πi kh√©t l·∫πt b·ªëc l√™n nghi ng√∫t t·∫°i 3 cƒÉn nh√† l√† nh√† anh H√† VƒÉn Chung,Vi VƒÉn Ph∆∞∆°ng v√†  L∆∞∆°ng VƒÉChung. \n",
      "M·ªçi ng∆∞·ªùi th·∫•y nh√† c·ªßa 3 gia ƒë√¨nh n√≥i tr√™n b·ªã h∆∞ h·ªèng n·∫∑ng.  \n",
      "D√π h·∫ßu h·∫øt c√°c th√†nh vi√™n c·ªßa 3 gia ƒë√¨nh b·ªã s√©t ƒë√°nh h∆∞ h·ªèng n√≥i tr√™n ƒë·ªÅu ƒëang ·ªü trong nh√† nh∆∞ng r·∫•t may kh√¥ng c√≥ ai b·ªã th∆∞∆°ng. \n",
      "\" T√¥i ƒëang l√∫i h√∫i trong nh√† ƒë·ªÉ n·∫•u c∆°m cho 2 ƒë·ª©a con ƒÉn b·ªØa t·ªëi, b·∫•t ng·ªù m·ªôt ti·∫øng \" s·∫ßm \" l·ªõn l√†m rung chuy·ªÉn cƒÉn nh√† s√†n. \n",
      "Nh·ªØng m·∫£nh v·ª° c·ªßa t·∫•m l·ª£p phibroximƒÉng r∆°i tung t√≥e kh·∫Øp nh√†. \n",
      "Hi·ªán, ch√∫ng t√¥i ƒëang ch·ªâ ƒë·∫°o anh em vƒÉn ph√≤ng ph·ªëi h·ª£p v·ªõi c√°c tr∆∞·ªüng b·∫£n ti·∫øn h√†nh ki·ªÉm tra th·ªëng k√™ v√† b√°o c√°o thi·ªát h·∫°i l√™n UBND huy·ªán v√†o s√°ng th·ª© 2 tu·∫ßn t·ªõi. \n",
      "Ri√™ng tr∆∞·ªùng h·ª£p 3 cƒÉn nh√† ·ªü b·∫£n X·ªëp H·ªëc b·ªã s\n",
      "\n",
      "--- Sample 3 ---\n",
      "[Prompt]    ### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:  ### L·ªánh:\n",
      "B·∫°n l√† m·ªôt tr·ª£ l√Ω t√≥m t·∫Øt vƒÉn b·∫£n. H√£y cung c·∫•p b·∫£n t√≥m t·∫Øt ng·∫Øn g·ªçn v√† ch√≠nh x√°c trong 150 ch·ªØ cho b√†i vi·∫øt sau. B√†i vi·∫øt: Ng√¢n h√†ng Nh√† n∆∞·ªõc ch√≠nh th·ª©c c√¥ng b·ªë kho·∫£n thu l√£i ti·∫øt ki·ªám g√¢y tranh c√£i.\n",
      "Theo kh·∫≥ng ƒë·ªãnh c·ªßa Ng√¢n h√†ng Nh√† n∆∞·ªõc, trong nƒÉm 2012, c√°c t·ªï ch·ª©c t√≠n d·ª•ng ƒë√£ tr·∫£ l√£i ti·ªÅn g·ª≠i, ti·ªÅn vay kho·∫£ng 408.000 t·ª∑ ƒë·ªìng; c√≤n thu l√£i cho vay t·ª´ n·ªÅn kinh t·∫ø kho·∫£ng 420.000 t·ª∑ ƒë·ªìng.\n",
      "Tr∆∞·ªõc nh·ªØng tranh c√£i v·ªÅ con s·ªë m√† n·ªÅn kinh t·∫ø tr·∫£ l√£i cho ng√¢n h√†ng trong nƒÉm 2012 l√™n t·ªõi 480.000 t·ª∑ ƒë·ªìng, c∆° quan Thanh tra Gi√°m s√°t Ng√¢n h√†ng thu·ªôc Ng√¢n h√†ng Nh√† n∆∞·ªõc ƒë√£ ch√≠nh th·ª©c c√¥ng b·ªë nh·ªØng s·ªë li·ªáu li√™n quan t·ªõi c√°c kho·∫£n vay v√† cho vay c·ªßa c√°c t·ªï ch·ª©c t√≠n d·ª•ng.\n",
      "Theo s·ªë li·ªáu t·ª´ c∆° quan Thanh tra Gi√°m s√°t Ng√¢n h√†ng, ∆∞·ªõc t√≠nh t·ª´ s·ªë li·ªáu b√°o c√°o c·ªßa c√°c t·ªï ch·ª©c t√≠n d·ª•ng (TCTD), nƒÉm 2012 c√°c TCTD ƒë√£ tr·∫£ l√£i ti·ªÅn g·ª≠i, ti·ªÅn vay kho·∫£ng 408.000 t·ª∑ ƒë·ªìng. Trong khi ƒë√≥, c√°c TCTD thu l√£i cho vay t·ª´ n·ªÅn kinh t·∫ø v·ªõi kho·∫£ng 420.000 t·ª∑ ƒë·ªìng.\n",
      "\n",
      "Nh∆∞ v·∫≠y, ch√™nh l·ªách thu chi to√†n ng√†nh ng√¢n h√†ng trong nƒÉm 2012 ∆∞·ªõc ch·ªâ ƒë·∫°t h∆°n 20.000 t·ª∑ ƒë·ªìng, l√† m·ª©c th·∫•p nh·∫•t k·ªÉ t·ª´ nƒÉm 2008 v√† c≈©ng ch·ªâ g·∫ßn t∆∞∆°ng ƒë∆∞∆°ng m·ª©c ch√™nh l·ªách thu, chi c·ªßa nƒÉm 2008 hay ch·ªâ b·∫±ng kho·∫£ng 40% m·ª©c c·ªßa nƒÉm 2011 do ch√™nh l·ªách l√£i su·∫•t ƒë·∫ßu ra, ƒë·∫ßu v√†o gi·∫£m, chi ph√≠ d·ª± ph√≤ng r·ªßi ro gia tƒÉng, t√≠n d·ª•ng tƒÉng tr∆∞·ªüng th·∫•p.\n",
      "Ngo√†i ra, c√°c ch·ªâ s·ªë hi·ªáu qu·∫£ kinh doanh (ROA, ROE) c≈©ng ch·ªâ b·∫±ng kho·∫£ng 40% so v·ªõi m·ª©c c·ªßa nƒÉm 2011. V√¨ v·∫≠y, nhi·ªÅu TCTD kh√¥ng chia c·ªï t·ª©c ho·∫∑c c√≥ t·ª∑ l·ªá chia c·ªï t·ª©c th·∫•p (d∆∞·ªõi 10%).\n",
      "\n",
      "C≈©ng theo c∆° quan Thanh tra Gi√°m s√°t Ng√¢n h√†ng, nƒÉm 2012, n·ªÅn kinh t·∫ø g·∫∑p nhi·ªÅu kh√≥ khƒÉn, ho·∫°t ƒë·ªông s·∫£n xu·∫•t kinh t·∫ø tr√¨ tr·ªá, do ƒë√≥ r·ªßi ro ho·∫°t ƒë·ªông ng√¢n h√†ng gia tƒÉng v√† hi·ªáu qu·∫£ kinh doanh c·ªßa c√°c ng√¢n h√†ng gi·∫£m s√∫t. ƒêi·ªÅu n√†y c≈©ng ph·∫£n √°nh ƒë√∫ng th·ª±c tr·∫°ng c·ªßa n·ªÅn kinh t·∫ø. Ng√¢n h√†ng l√† trung gian t√†i ch√≠nh, ƒëi vay ƒë·ªÉ cho vay. Khi ƒëi vay, nh·∫≠n ti·ªÅn g·ª≠i, ng√¢n h√†ng ph·∫£i tr·∫£ l√£i cho ng∆∞·ªùi cho vay, ng∆∞·ªùi g·ª≠i ti·ªÅn. Khi cho vay, ng√¢n h√†ng thu l√£i cho vay ƒë·ªÉ c√≥ ngu·ªìn tr·∫£ l√£i huy ƒë·ªông v·ªën. M·ª©c ƒë·ªô l√†nh m·∫°nh, hi·ªáu qu·∫£ c·ªßa ng∆∞·ªùi g·ª≠i ti·ªÅn v√† ng∆∞·ªùi vay ƒë·ªÅu t√°c ƒë·ªông ƒë·∫øn hi·ªáu qu·∫£ kinh doanh v√† tr·∫°ng th√°i r·ªßi ro c·ªßa ng√¢n h√†ng. L·ª£i nhu·∫≠n ng√¢n h√†ng ch·ªß y·∫øu ƒë∆∞·ª£c h√¨nh th√†nh t·ª´ ch√™nh l·ªách l√£i su·∫•t ƒë·∫ßu ra, ƒë·∫ßu v√†o.\n",
      "\n",
      "Do ƒë√≥, ‚Äútrong ƒëi·ªÅu ki·ªán kinh doanh kh√≥ khƒÉn chung, c∆° h·ªôi ƒë·∫°t ƒë∆∞·ª£c si√™u l·ª£i nhu·∫≠n l√† r·∫•t kh√≥ ·ªü b·∫•t c·ª© ng√†nh, lƒ©nh v·ª±c n√†o v√† ch·ªâ c√≥ th·ªÉ c√≥ ƒë∆∞·ª£c ·ªü m·ªôt ho·∫∑c m·ªôt v√†i th√†nh vi√™n th·ªã tr∆∞·ªùng c·ª• th·ªÉ c√≥ s·ª± s√°ng t·∫°o v√† nƒÉng l·ª±c v∆∞·ª£t tr·ªôi. N·∫øu t√¨nh h√¨nh kinh t·∫ø kh√¥ng c√≥ s·ª± c·∫£i thi·ªán trong nƒÉm 2013 th√¨ ho·∫°t ƒë·ªông ng√¢n h√†ng s·∫Ω c√≤n g·∫∑p nhi·ªÅu kh√≥ khƒÉn do n·ª£ x·∫•u v√† chi ph√≠ ho·∫°t ƒë·ªông gia tƒÉng. V√¨ v·∫≠y, vi·ªác c·∫£i thi·ªán hi·ªáu qu·∫£ kinh doanh v√† l·ª£i nhu·∫≠n c·ªßa c√°c ng√¢n h√†ng trong nƒÉm 2013 v·∫´n l√† m·ªôt th√°ch th·ª©c l·ªõn‚Äù, c∆° quan n√†y cho hay. T√≥m t·∫Øt: \n",
      "\n",
      "### T√≥m t·∫Øt:\n",
      "Theo kh·∫≥ng ƒë·ªãnh c·ªßa Ng√¢n h√†ng Nh√† n∆∞·ªõc, trong nƒÉm 2012, c√°c t·ªï ch·ª©c t√≠n d·ª•ng ƒë√£ tr·∫£ l√£i ti·ªÅn g·ª≠i, ti·ªÅn vay kho·∫£ng 408.000 t·ª∑ ƒë·ªìng; c√≤n thu l√£i cho vay t·ª´ n·ªÅn kinh t·∫ø kho·∫£ng 420.000 t·ª∑ ƒë·ªìng.\n",
      "Nh∆∞ v·∫≠y, ch√™nh l·ªách thu chi to√†n ng√†nh ng√¢n h√†ng trong nƒÉm 2012 ∆∞·ªõc ch·ªâ ƒë·∫°t h∆°n 20.000 t·ª∑ ƒë·ªìng, l√† m·ª©c th·∫•p nh·∫•t k·ªÉ t·ª´ nƒÉm 2008 v√† c≈©ng ch·ªâ g·∫ßn t∆∞∆°ng ƒë∆∞∆°ng m·ª©c ch√™nh l·ªách thu, chi c·ªßa nƒÉm 2008 hay ch·ªâ b·∫±ng kho·∫£ng 40% m·ª©c c·ªßa nƒÉm 2011 do ch√™nh l·ªách l√£i su·∫•t ƒë·∫ßu ra, ƒë·∫ßu v√†o gi·∫£m, chi ph√≠ d·ª± ph√≤ng r·ªßi ro gia tƒÉng, t√≠n d·ª•ng tƒÉng tr∆∞·ªüng th·∫•p.\n",
      "NƒÉm 2012, n·ªÅn kinh t·∫ø g·∫∑p nhi·ªÅu kh√≥ khƒÉn, ho·∫°t ƒë·ªông s·∫£n xu·∫•t kinh t·∫ø tr√¨ tr·ªá, do ƒë√≥ r·ªßi ro ho·∫°t ƒë·ªông ng√¢n h√†ng gia tƒÉng v√† hi·ªáu qu·∫£ kinh doanh c·ªßa c√°c ng√¢n h√†ng gi·∫£m s√∫t.\n",
      " N·∫øu t√¨nh h√¨nh kinh t·∫ø kh√¥ng c√≥ s·ª± c·∫£i thi·ªán trong nƒÉm 2013 th√¨ ho·∫°t ƒë·ªông ng√¢n h√†ng s·∫Ω c√≤n g·∫∑p nhi·ªÅu kh√≥ khƒÉn do n·ª£ x·∫•u v√† chi ph√≠ ho·∫°t ƒë·ªông gia tƒÉng.\n",
      "[Reference] Theo kh·∫≥ng ƒë·ªãnh c·ªßa Ng√¢n h√†ng Nh√† n∆∞·ªõc, trong nƒÉm 2012, c√°c t·ªï ch·ª©c t√≠n d·ª•ng ƒë√£ tr·∫£ l√£i ti·ªÅn g·ª≠i, ti·ªÅn vay kho·∫£ng 408.000 t·ª∑ ƒë·ªìng; c√≤n thu l√£i cho vay t·ª´ n·ªÅn kinh t·∫ø kho·∫£ng 420.000 t·ª∑ ƒë·ªìng.\n",
      "Nh∆∞ v·∫≠y, ch√™nh l·ªách thu chi to√†n ng√†nh ng√¢n h√†ng trong nƒÉm 2012 ∆∞·ªõc ch·ªâ ƒë·∫°t h∆°n 20.000 t·ª∑ ƒë·ªìng, l√† m·ª©c th·∫•p nh·∫•t k·ªÉ t·ª´ nƒÉm 2008 v√† c≈©ng ch·ªâ g·∫ßn t∆∞∆°ng ƒë∆∞∆°ng m·ª©c ch√™nh l·ªách thu, chi c·ªßa nƒÉm 2008 hay ch·ªâ b·∫±ng kho·∫£ng 40% m·ª©c c·ªßa nƒÉm 2011 do ch√™nh l·ªách l√£i su·∫•t ƒë·∫ßu ra, ƒë·∫ßu v√†o gi·∫£m, chi ph√≠ d·ª± ph√≤ng r·ªßi ro gia tƒÉng, t√≠n d·ª•ng tƒÉng tr∆∞·ªüng th·∫•p.\n",
      "NƒÉm 2012, n·ªÅn kinh t·∫ø g·∫∑p nhi·ªÅu kh√≥ khƒÉn, ho·∫°t ƒë·ªông s·∫£n xu·∫•t kinh t·∫ø tr√¨ tr·ªá, do ƒë√≥ r·ªßi ro ho·∫°t ƒë·ªông ng√¢n h√†ng gia tƒÉng v√† hi·ªáu qu·∫£ kinh doanh c·ªßa c√°c ng√¢n h√†ng gi·∫£m s√∫t.\n",
      " N·∫øu t√¨nh h√¨nh kinh t·∫ø kh√¥ng c√≥ s·ª± c·∫£i thi·ªán trong nƒÉm 2013 th√¨ ho·∫°t ƒë·ªông ng√¢n h√†ng s·∫Ω c√≤n g·∫∑p nhi·ªÅu kh√≥ khƒÉn do n·ª£ x·∫•u v√† chi ph√≠ ho·∫°t ƒë·ªông gia tƒÉng.\n",
      "[Generated] ### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:  ### L·ªánh:\n",
      "B·∫°n l√† m·ªôt tr·ª£ l√Ω t√≥m t·∫Øt vƒÉn b·∫£n. H√£y cung c·∫•p b·∫£n t√≥m t·∫Øt ng·∫Øn g·ªçn v√† ch√≠nh x√°c trong 150 ch·ªØ cho b√†i vi·∫øt sau. B√†i vi·∫øt: Ng√¢n h√†ng Nh√† n∆∞·ªõc ch√≠nh th·ª©c c√¥ng b·ªë kho·∫£n thu l√£i ti·∫øt ki·ªám g√¢y tranh c√£i.\n",
      "Theo kh·∫≥ng ƒë·ªãnh c·ªßa Ng√¢n h√†ng Nh√† n∆∞·ªõc, trong nƒÉm 2012, c√°c t·ªï ch·ª©c t√≠n d·ª•ng ƒë√£ tr·∫£ l√£i ti·ªÅn g·ª≠i, ti·ªÅn vay kho·∫£ng 408.000 t·ª∑ ƒë·ªìng; c√≤n thu l√£i cho vay t·ª´ n·ªÅn kinh t·∫ø kho·∫£ng 420.000 t·ª∑ ƒë·ªìng.\n",
      "Tr∆∞·ªõc nh·ªØng tranh c√£i v·ªÅ con s·ªë m√† n·ªÅn kinh t·∫ø tr·∫£ l√£i cho ng√¢n h√†ng trong nƒÉm 2012 l√™n t·ªõi 480.000 t·ª∑ ƒë·ªìng, c∆° quan Thanh tra Gi√°m s√°t Ng√¢n h√†ng thu·ªôc Ng√¢n h√†ng Nh√† n∆∞·ªõc ƒë√£ ch√≠nh th·ª©c c√¥ng b·ªë nh·ªØng s·ªë li·ªáu li√™n quan t·ªõi c√°c kho·∫£n vay v√† cho vay c·ªßa c√°c t·ªï ch·ª©c t√≠n d·ª•ng.\n",
      "Theo s·ªë li·ªáu t·ª´ c∆° quan Thanh tra Gi√°m s√°t Ng√¢n h√†ng, ∆∞·ªõc t√≠nh t·ª´ s·ªë li·ªáu b√°o c√°o c·ªßa c√°c t·ªï ch·ª©c t√≠n d·ª•ng (TCTD), nƒÉm 2012 c√°c TCTD ƒë√£ tr·∫£ l√£i ti·ªÅn g·ª≠i, ti·ªÅn vay kho·∫£ng 408.000 t·ª∑ ƒë·ªìng. Trong khi ƒë√≥, c√°c TCTD thu l√£i cho vay t·ª´ n·ªÅn kinh t·∫ø v·ªõi kho·∫£ng 420.000 t·ª∑ ƒë·ªìng.\n",
      "\n",
      "Nh∆∞ v·∫≠y, ch√™nh l·ªách thu chi to√†n ng√†nh ng√¢n h√†ng trong nƒÉm 2012 ∆∞·ªõc ch·ªâ ƒë·∫°t h∆°n 20.000 t·ª∑ ƒë·ªìng, l√† m·ª©c th·∫•p nh·∫•t k·ªÉ t·ª´ nƒÉm 2008 v√† c≈©ng ch·ªâ g·∫ßn t∆∞∆°ng ƒë∆∞∆°ng m·ª©c ch√™nh l·ªách thu, chi c·ªßa nƒÉm 2008 hay ch·ªâ b·∫±ng kho·∫£ng 40% m·ª©c c·ªßa nƒÉm 2011 do ch√™nh l·ªách l√£i su·∫•t ƒë·∫ßu ra, ƒë·∫ßu v√†o gi·∫£m, chi ph√≠ d·ª± ph√≤ng r·ªßi ro gia tƒÉng, t√≠n d·ª•ng tƒÉng tr∆∞·ªüng th·∫•p.\n",
      "Ngo√†i ra, c√°c ch·ªâ s·ªë hi·ªáu qu·∫£ kinh doanh (ROA, ROE) c≈©ng ch·ªâ b·∫±ng kho·∫£ng 40% so v·ªõi m·ª©c c·ªßa nƒÉm 2011. V√¨ v·∫≠y, nhi·ªÅu TCTD kh√¥ng chia c·ªï t·ª©c ho·∫∑c c√≥ t·ª∑ l·ªá chia c·ªï t·ª©c th·∫•p (d∆∞·ªõi 10%).\n",
      "\n",
      "C≈©ng theo c∆° quan Thanh tra Gi√°m s√°t Ng√¢n h√†ng, nƒÉm 2012, n·ªÅn kinh t·∫ø g·∫∑p nhi·ªÅu kh√≥ khƒÉn, ho·∫°t ƒë·ªông s·∫£n xu·∫•t kinh t·∫ø tr√¨ tr·ªá, do ƒë√≥ r·ªßi ro ho·∫°t ƒë·ªông ng√¢n h√†ng gia tƒÉng v√† hi·ªáu qu·∫£ kinh doanh c·ªßa c√°c ng√¢n h√†ng gi·∫£m s√∫t. ƒêi·ªÅu n√†y c≈©ng ph·∫£n √°nh ƒë√∫ng th·ª±c tr·∫°ng c·ªßa n·ªÅn kinh t·∫ø. Ng√¢n h√†ng l√† trung gian t√†i ch√≠nh, ƒëi vay ƒë·ªÉ cho vay. Khi ƒëi vay, nh·∫≠n ti·ªÅn g·ª≠i, ng√¢n h√†ng ph·∫£i tr·∫£ l√£i cho ng∆∞·ªùi cho vay, ng∆∞·ªùi g·ª≠i ti·ªÅn. Khi cho vay, ng√¢n h√†ng thu l√£i cho vay ƒë·ªÉ c√≥ ngu·ªìn tr·∫£ l√£i huy ƒë·ªông v·ªën. M·ª©c ƒë·ªô l√†nh m·∫°nh, hi·ªáu qu·∫£ c·ªßa ng∆∞·ªùi g·ª≠i ti·ªÅn v√† ng∆∞·ªùi vay ƒë·ªÅu t√°c ƒë·ªông ƒë·∫øn hi·ªáu qu·∫£ kinh doanh v√† tr·∫°ng th√°i r·ªßi ro c·ªßa ng√¢n h√†ng. L·ª£i nhu·∫≠n ng√¢n h√†ng ch·ªß y·∫øu ƒë∆∞·ª£c h√¨nh th√†nh t·ª´ ch√™nh l·ªách l√£i su·∫•t ƒë·∫ßu ra, ƒë·∫ßu v√†o.\n",
      "\n",
      "Do ƒë√≥, ‚Äútrong ƒëi·ªÅu ki·ªán kinh doanh kh√≥ khƒÉn chung, c∆° h·ªôi ƒë·∫°t ƒë∆∞·ª£c si√™u l·ª£i nhu·∫≠n l√† r·∫•t kh√≥ ·ªü b·∫•t c·ª© ng√†nh, lƒ©nh v·ª±c n√†o v√† ch·ªâ c√≥ th·ªÉ c√≥ ƒë∆∞·ª£c ·ªü m·ªôt ho·∫∑c m·ªôt v√†i th√†nh vi√™n th·ªã tr∆∞·ªùng c·ª• th·ªÉ c√≥ s·ª± s√°ng t·∫°o v√† nƒÉng l·ª±c v∆∞·ª£t tr·ªôi. N·∫øu t√¨nh h√¨nh kinh t·∫ø kh√¥ng c√≥ s·ª± c·∫£i thi·ªán trong nƒÉm 2013 th√¨ ho·∫°t ƒë·ªông ng√¢n h√†ng s·∫Ω c√≤n g·∫∑p nhi·ªÅu kh√≥ khƒÉn do n·ª£ x·∫•u v√† chi ph√≠ ho·∫°t ƒë·ªông gia tƒÉng. V√¨ v·∫≠y, vi·ªác c·∫£i thi·ªán hi·ªáu qu·∫£ kinh doanh v√† l·ª£i nhu·∫≠n c·ªßa c√°c ng√¢n h√†ng trong nƒÉm 2013 v·∫´n l√† m·ªôt th√°ch th·ª©c l·ªõn‚Äù, c∆° quan n√†y cho hay. T√≥m t·∫Øt: \n",
      "\n",
      "### T√≥m t·∫Øt:\n",
      "Theo kh·∫≥ng ƒë·ªãnh c·ªßa Ng√¢n h√†ng Nh√† n∆∞·ªõc, trong nƒÉm 2012, c√°c t·ªï ch·ª©c t√≠n d·ª•ng ƒë√£ tr·∫£ l√£i ti·ªÅn g·ª≠i, ti·ªÅn vay kho·∫£ng 408.000 t·ª∑ ƒë·ªìng; c√≤n thu l√£i cho vay t·ª´ n·ªÅn kinh t·∫ø kho·∫£ng 420.000 t·ª∑ ƒë·ªìng.\n",
      "Nh∆∞ v·∫≠y, ch√™nh l·ªách thu chi to√†n ng√†nh ng√¢n h√†ng trong nƒÉm 2012 ∆∞·ªõc ch·ªâ ƒë·∫°t h∆°n 20.000 t·ª∑ ƒë·ªìng, l√† m·ª©c th·∫•p nh·∫•t k·ªÉ t·ª´ nƒÉm 2008 v√† c≈©ng ch·ªâ g·∫ßn t∆∞∆°ng ƒë∆∞∆°ng m·ª©c ch√™nh l·ªách thu, chi c·ªßa nƒÉm 2008 hay ch·ªâ b·∫±ng kho·∫£ng 40% m·ª©c c·ªßa nƒÉm 2011 do ch√™nh l·ªách l√£i su·∫•t ƒë·∫ßu ra, ƒë·∫ßu v√†o gi·∫£m, chi ph√≠ d·ª± ph√≤ng r·ªßi ro gia tƒÉng, t√≠n d·ª•ng tƒÉng tr∆∞·ªüng th·∫•p.\n",
      "NƒÉm 2012, n·ªÅn kinh t·∫ø g·∫∑p nhi·ªÅu kh√≥ khƒÉn, ho·∫°t ƒë·ªông s·∫£n xu·∫•t kinh t·∫ø tr√¨ tr·ªá, do ƒë√≥ r·ªßi ro ho·∫°t ƒë·ªông ng√¢n h√†ng gia tƒÉng v√† hi·ªáu qu·∫£ kinh doanh c·ªßa c√°c ng√¢n h√†ng gi·∫£m s√∫t.\n",
      " N·∫øu t√¨nh h√¨nh kinh t·∫ø kh√¥ng c√≥ s·ª± c·∫£i thi·ªán trong nƒÉm 2013 th√¨ ho·∫°t ƒë·ªông ng√¢n h√†ng s·∫Ω c√≤n g·∫∑p nhi·ªÅu kh√≥ khƒÉn do n·ª£ x·∫•u v√† chi ph√≠ ho·∫°t ƒë·ªông gia tƒÉng. V√¨ v·∫≠y, vi·ªác c·∫£i thi·ªán hi·ªáu qu·∫£ kinh doanh v√† l·ª£i nhu·∫≠n c·ªßa c√°c ng√¢n h√†ng trong nƒÉm 2013 v·∫´n l√† m·ªôt th√°ch th·ª©c l·ªõn, c∆° h·ªôi ƒë·∫°t ƒë∆∞·ª£c si√™u l·ª£i nhu·∫≠n l√† r·∫•t kh√≥ ·ªü b·∫•t c·ª© ng√†nh, lƒ©nh v·ª±c n√†o v√† ch·ªâ c√≥ th·ªÉ c√≥ ƒë∆∞·ª£c ·ªü m·ªôt ho·∫∑c m·ªôt v√†i th√†nh vi√™n th·ªã tr∆∞·ªùng c·ª• th·ªÉ c√≥ s·ª± s√°ng t·∫°o v√† nƒÉng l·ª±c v∆∞·ª£t tr·ªôi. N·∫øu t√¨nh h√¨nh kinh t·∫ø kh√¥ng c√≥ s·ª± c·∫£i thi·ªán trong nƒÉm 2013 th√¨ ho·∫°t ƒë·ªông ng√¢n h√†ng s·∫Ω c√≤n g·∫∑p nhi·ªÅu kh√≥ khƒÉn do n·ª£ x·∫•u v√† chi ph√≠ ho·∫°t ƒë·ªông gia tƒÉng. V√¨ v·∫≠y, vi·ªác c·∫£i thi·ªán hi·ªáu qu·∫£ kinh doanh v√† l·ª£i nhu·∫≠n c·ªßa c√°c\n"
     ]
    }
   ],
   "source": [
    "evaluate_after_training(model, val_loader, tokenizer, device)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6959873,
     "sourceId": 11188231,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3853558,
     "sourceId": 6680012,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 294052,
     "modelInstanceId": 273094,
     "sourceId": 324282,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
