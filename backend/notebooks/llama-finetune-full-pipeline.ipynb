{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-02T15:24:19.956512Z",
     "iopub.status.busy": "2025-06-02T15:24:19.956134Z",
     "iopub.status.idle": "2025-06-02T15:24:39.039462Z",
     "shell.execute_reply": "2025-06-02T15:24:39.038608Z",
     "shell.execute_reply.started": "2025-06-02T15:24:19.956483Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge_score) (2024.2.0)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=485f1f8d7c354640c82d5917f350c74e3ebc5bae5900fb9a3055ec269b1be44c\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n",
      "Collecting bert_score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.5.1+cu121)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.2.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.47.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.26.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.5)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2025.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.29.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bert_score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bert_score) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->bert_score) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->bert_score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->bert_score) (2024.2.0)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bert_score\n",
      "Successfully installed bert_score-0.3.13\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate peft datasets bitsandbytes evaluate -q\n",
    "!pip install rouge_score\n",
    "!pip install bert_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import library and load module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T15:24:39.041367Z",
     "iopub.status.busy": "2025-06-02T15:24:39.041001Z",
     "iopub.status.idle": "2025-06-02T15:25:02.303215Z",
     "shell.execute_reply": "2025-06-02T15:25:02.302567Z",
     "shell.execute_reply.started": "2025-06-02T15:24:39.041341Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from IPython.display import FileLink\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    default_data_collator,\n",
    ")\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "from peft import PeftModel, PeftConfig\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T15:25:02.305103Z",
     "iopub.status.busy": "2025-06-02T15:25:02.304534Z",
     "iopub.status.idle": "2025-06-02T15:25:02.508793Z",
     "shell.execute_reply": "2025-06-02T15:25:02.508132Z",
     "shell.execute_reply.started": "2025-06-02T15:25:02.305080Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T13:34:30.489251Z",
     "iopub.status.busy": "2025-06-02T13:34:30.488862Z",
     "iopub.status.idle": "2025-06-02T13:34:30.493421Z",
     "shell.execute_reply": "2025-06-02T13:34:30.492494Z",
     "shell.execute_reply.started": "2025-06-02T13:34:30.489213Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T09:06:36.200794Z",
     "iopub.status.busy": "2025-06-01T09:06:36.200407Z",
     "iopub.status.idle": "2025-06-01T09:06:36.222266Z",
     "shell.execute_reply": "2025-06-01T09:06:36.221029Z",
     "shell.execute_reply.started": "2025-06-01T09:06:36.200757Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T04:47:39.765551Z",
     "iopub.status.busy": "2025-05-31T04:47:39.765259Z",
     "iopub.status.idle": "2025-05-31T04:47:39.782880Z",
     "shell.execute_reply": "2025-05-31T04:47:39.782263Z",
     "shell.execute_reply.started": "2025-05-31T04:47:39.765523Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"/kaggle/input/llama-continued-pretrain-model/transformers/default/1/llama-continued-pretrain-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T13:31:22.370981Z",
     "iopub.status.busy": "2025-05-31T13:31:22.370704Z",
     "iopub.status.idle": "2025-05-31T13:31:22.382557Z",
     "shell.execute_reply": "2025-05-31T13:31:22.381889Z",
     "shell.execute_reply.started": "2025-05-31T13:31:22.370962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"/kaggle/input/llama3.2-qa-model/transformers/default/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T01:59:29.170333Z",
     "iopub.status.busy": "2025-06-02T01:59:29.170018Z",
     "iopub.status.idle": "2025-06-02T01:59:29.180598Z",
     "shell.execute_reply": "2025-06-02T01:59:29.179841Z",
     "shell.execute_reply.started": "2025-06-02T01:59:29.170306Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"/kaggle/input/llama-finetune-all/transformers/default/1/llama_finetune_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T15:25:02.510402Z",
     "iopub.status.busy": "2025-06-02T15:25:02.510152Z",
     "iopub.status.idle": "2025-06-02T15:25:02.965236Z",
     "shell.execute_reply": "2025-06-02T15:25:02.964309Z",
     "shell.execute_reply.started": "2025-06-02T15:25:02.510383Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"/kaggle/input/llama-vnd-finetuned-model-epoch-2/transformers/default/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T15:25:02.966401Z",
     "iopub.status.busy": "2025-06-02T15:25:02.966154Z",
     "iopub.status.idle": "2025-06-02T15:25:18.138841Z",
     "shell.execute_reply": "2025-06-02T15:25:18.137941Z",
     "shell.execute_reply.started": "2025-06-02T15:25:02.966380Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccedc548956841688a6286519ab53f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ffd78202144c549920b3bebf1b7405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c933e6f11f46fabd8ca46eb256eebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# Thi·∫øt l·∫≠p pad_token v√† padding side\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T15:25:18.140153Z",
     "iopub.status.busy": "2025-06-02T15:25:18.139840Z",
     "iopub.status.idle": "2025-06-02T15:25:18.145692Z",
     "shell.execute_reply": "2025-06-02T15:25:18.145047Z",
     "shell.execute_reply.started": "2025-06-02T15:25:18.140129Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    attention_masks = [item[\"attention_mask\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_masks,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T15:25:18.146804Z",
     "iopub.status.busy": "2025-06-02T15:25:18.146507Z",
     "iopub.status.idle": "2025-06-02T15:25:19.432542Z",
     "shell.execute_reply": "2025-06-02T15:25:19.431408Z",
     "shell.execute_reply.started": "2025-06-02T15:25:18.146776Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Continue Pretrain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"vukhai248/vietnamese_news_16k\", split=\"train\")\n",
    "dataset = dataset.select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:02:54.891173Z",
     "iopub.status.busy": "2025-05-25T03:02:54.890933Z",
     "iopub.status.idle": "2025-05-25T03:02:54.894619Z",
     "shell.execute_reply": "2025-05-25T03:02:54.893717Z",
     "shell.execute_reply.started": "2025-05-25T03:02:54.891150Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    return text.replace(\"\\n\", \" \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:02:54.895648Z",
     "iopub.status.busy": "2025-05-25T03:02:54.895450Z",
     "iopub.status.idle": "2025-05-25T03:03:04.441895Z",
     "shell.execute_reply": "2025-05-25T03:03:04.440976Z",
     "shell.execute_reply.started": "2025-05-25T03:02:54.895632Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_fn(text, max_length=2048):\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "    attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "    labels = input_ids.clone()\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:03:04.443066Z",
     "iopub.status.busy": "2025-05-25T03:03:04.442775Z",
     "iopub.status.idle": "2025-05-25T03:03:10.608088Z",
     "shell.execute_reply": "2025-05-25T03:03:10.607451Z",
     "shell.execute_reply.started": "2025-05-25T03:03:04.443043Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "texts = dataset[\"content\"]\n",
    "tokenized_data = [tokenize_fn(text) for text in texts]\n",
    "train_loader = DataLoader(\n",
    "    tokenized_data,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T03:03:10.609190Z",
     "iopub.status.busy": "2025-05-25T03:03:10.608887Z",
     "iopub.status.idle": "2025-05-25T10:51:54.609669Z",
     "shell.execute_reply": "2025-05-25T10:51:54.608737Z",
     "shell.execute_reply.started": "2025-05-25T03:03:10.609162Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, device, n_epochs=3, lr=1e-5):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\", leave=True)\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            # Forward pass with checkpointing\n",
    "            def forward_with_checkpoint(*args, **kwargs):\n",
    "                return torch.utils.checkpoint.checkpoint(\n",
    "                    lambda *args, **kwargs: model(*args, **kwargs),\n",
    "                    *args,\n",
    "                    **kwargs,\n",
    "                    use_reentrant=False\n",
    "                )\n",
    "\n",
    "            outputs = forward_with_checkpoint(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} - Loss: {avg_loss:.4f}\")\n",
    "train(model, train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T10:51:54.611200Z",
     "iopub.status.busy": "2025-05-25T10:51:54.610857Z",
     "iopub.status.idle": "2025-05-25T10:51:55.038424Z",
     "shell.execute_reply": "2025-05-25T10:51:55.037648Z",
     "shell.execute_reply.started": "2025-05-25T10:51:54.611169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-continued-pretrain\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"‚úÖ Pretrained model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T10:51:55.039477Z",
     "iopub.status.busy": "2025-05-25T10:51:55.039197Z",
     "iopub.status.idle": "2025-05-25T10:51:55.626208Z",
     "shell.execute_reply": "2025-05-25T10:51:55.625554Z",
     "shell.execute_reply.started": "2025-05-25T10:51:55.039447Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ƒê∆∞·ªùng d·∫´n t·ªõi th∆∞ m·ª•c ch·ª©a m√¥ h√¨nh\n",
    "model_dir = '/kaggle/working/llama3-qlora-continued-pretrain'\n",
    "\n",
    "# T·∫°o file n√©n .zip t·ª´ th∆∞ m·ª•c\n",
    "zip_name = 'llama3-qlora-continued-pretrain'  # T√™n file n√©n (kh√¥ng c·∫ßn ƒëu√¥i .zip)\n",
    "shutil.make_archive(zip_name, 'zip', model_dir)\n",
    "\n",
    "# T·∫°o li√™n k·∫øt t·∫£i xu·ªëng cho file .zip\n",
    "display(FileLink(f'{zip_name}.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finetune with QA dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T10:51:55.627282Z",
     "iopub.status.busy": "2025-05-25T10:51:55.626977Z",
     "iopub.status.idle": "2025-05-25T10:51:55.630429Z",
     "shell.execute_reply": "2025-05-25T10:51:55.629811Z",
     "shell.execute_reply.started": "2025-05-25T10:51:55.627251Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model_id = \"/kaggle/input/llama-qa-partially-finetune/transformers/default/1/llama-qa-finetune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T04:47:59.291864Z",
     "iopub.status.busy": "2025-05-31T04:47:59.291537Z",
     "iopub.status.idle": "2025-05-31T04:48:03.535775Z",
     "shell.execute_reply": "2025-05-31T04:48:03.534479Z",
     "shell.execute_reply.started": "2025-05-31T04:47:59.291835Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/vietnamese-squad/train-v2.0-translated.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    vi_squad_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T04:48:03.537072Z",
     "iopub.status.busy": "2025-05-31T04:48:03.536759Z",
     "iopub.status.idle": "2025-05-31T04:48:03.544935Z",
     "shell.execute_reply": "2025-05-31T04:48:03.542417Z",
     "shell.execute_reply.started": "2025-05-31T04:48:03.537042Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data = vi_squad_data[:10000]\n",
    "\n",
    "# L·∫•y 2k m·∫´u ti·∫øp theo cho validation\n",
    "val_data = vi_squad_data[10000:12000]\n",
    "\n",
    "print(f\"üì¶ Train samples: {len(train_data)}\")\n",
    "print(f\"üì¶ Validation samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T04:48:03.546266Z",
     "iopub.status.busy": "2025-05-31T04:48:03.545857Z",
     "iopub.status.idle": "2025-05-31T04:48:06.930020Z",
     "shell.execute_reply": "2025-05-31T04:48:06.929222Z",
     "shell.execute_reply.started": "2025-05-31T04:48:03.546242Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"üì¶ Train samples: {train_data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T04:48:06.931157Z",
     "iopub.status.busy": "2025-05-31T04:48:06.930840Z",
     "iopub.status.idle": "2025-05-31T04:48:07.134045Z",
     "shell.execute_reply": "2025-05-31T04:48:07.133405Z",
     "shell.execute_reply.started": "2025-05-31T04:48:06.931132Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_list([{\"context\": item[0], \"question\": item[1], \"answer\": item[2]} for item in train_data])\n",
    "val_dataset = Dataset.from_list([{\"context\": item[0], \"question\": item[1], \"answer\": item[2]} for item in val_data])\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": val_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T04:48:07.134968Z",
     "iopub.status.busy": "2025-05-31T04:48:07.134755Z",
     "iopub.status.idle": "2025-05-31T04:48:07.145990Z",
     "shell.execute_reply": "2025-05-31T04:48:07.145368Z",
     "shell.execute_reply.started": "2025-05-31T04:48:07.134950Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"1. {dataset_dict['train'][0]}\")\n",
    "print(f\"2. {dataset_dict['train'][0]['context']}\")\n",
    "print(f\"3. {dataset_dict['train'][0]['question']}\")\n",
    "print(f\"4. {dataset_dict['train'][0]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T04:48:07.147238Z",
     "iopub.status.busy": "2025-05-31T04:48:07.146924Z",
     "iopub.status.idle": "2025-05-31T04:48:24.323118Z",
     "shell.execute_reply": "2025-05-31T04:48:24.322086Z",
     "shell.execute_reply.started": "2025-05-31T04:48:07.147208Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_qa(example, max_length=512):\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"]\n",
    "    prompt = f\"### ƒê√¢y l√† d·∫°ng c√¢u h·ªèi v√† tr·∫£ l·ªùi d·ª±a tr√™n n·ªôi dung ###\\n\\nC√¢u h·ªèi: {question}\\n\\nN·ªôi dung: {context}\\n\\nTr·∫£ l·ªùi:\"\n",
    "    completion = f\" {answer}\"\n",
    "    \n",
    "    prompt_ids = tokenizer(prompt, add_special_tokens=False).input_ids\n",
    "    completion_ids = tokenizer(completion, add_special_tokens=False).input_ids\n",
    "\n",
    "    input_ids = prompt_ids + completion_ids\n",
    "\n",
    "    labels = [-100] * len(prompt_ids) + completion_ids\n",
    "\n",
    "    if len(input_ids) > max_length:\n",
    "        input_ids = input_ids[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    \n",
    "    input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "    attention_mask = attention_mask + [0] * padding_length\n",
    "    labels = labels + [-100] * padding_length\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids),\n",
    "        \"attention_mask\": torch.tensor(attention_mask),\n",
    "        \"labels\": torch.tensor(labels)\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset_dict.map(preprocess_qa)\n",
    "\n",
    "tokenized_dataset.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T04:48:24.324181Z",
     "iopub.status.busy": "2025-05-31T04:48:24.323947Z",
     "iopub.status.idle": "2025-05-31T04:48:24.328237Z",
     "shell.execute_reply": "2025-05-31T04:48:24.327567Z",
     "shell.execute_reply.started": "2025-05-31T04:48:24.324161Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"], batch_size=4, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_dataset[\"test\"], batch_size=4, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T04:48:24.329401Z",
     "iopub.status.busy": "2025-05-31T04:48:24.329069Z",
     "iopub.status.idle": "2025-05-31T11:22:10.859713Z",
     "shell.execute_reply": "2025-05-31T11:22:10.858838Z",
     "shell.execute_reply.started": "2025-05-31T04:48:24.329371Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "n_epochs = 2\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\", leave=True)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"‚úÖ Epoch {epoch+1}/{n_epochs} - Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"üìä Validation Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T11:22:10.860777Z",
     "iopub.status.busy": "2025-05-31T11:22:10.860551Z",
     "iopub.status.idle": "2025-05-31T11:22:11.355922Z",
     "shell.execute_reply": "2025-05-31T11:22:11.355189Z",
     "shell.execute_reply.started": "2025-05-31T11:22:10.860748Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-qa-finetune\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"‚úÖ Pretrained model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T11:26:33.659496Z",
     "iopub.status.busy": "2025-05-31T11:26:33.659203Z",
     "iopub.status.idle": "2025-05-31T11:26:34.259834Z",
     "shell.execute_reply": "2025-05-31T11:26:34.258965Z",
     "shell.execute_reply.started": "2025-05-31T11:26:33.659476Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_dir = '/kaggle/working/llama3-qlora-qa-finetune'\n",
    "\n",
    "# T·∫°o file n√©n .zip t·ª´ th∆∞ m·ª•c\n",
    "zip_name = 'llama3-qlora-qa-finetune'  # T√™n file n√©n (kh√¥ng c·∫ßn ƒëu√¥i .zip)\n",
    "shutil.make_archive(zip_name, 'zip', model_dir)\n",
    "\n",
    "# T·∫°o li√™n k·∫øt t·∫£i xu·ªëng cho file .zip\n",
    "display(FileLink(f'{zip_name}.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finetune with BBC News**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T13:31:49.220703Z",
     "iopub.status.busy": "2025-05-31T13:31:49.220357Z",
     "iopub.status.idle": "2025-05-31T13:31:49.228319Z",
     "shell.execute_reply": "2025-05-31T13:31:49.227332Z",
     "shell.execute_reply.started": "2025-05-31T13:31:49.220675Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T13:31:49.229254Z",
     "iopub.status.busy": "2025-05-31T13:31:49.229003Z",
     "iopub.status.idle": "2025-05-31T13:31:49.455800Z",
     "shell.execute_reply": "2025-05-31T13:31:49.454950Z",
     "shell.execute_reply.started": "2025-05-31T13:31:49.229226Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/kaggle/input/data-llama-finetune/bbc_data_llama_finetune.json\"\n",
    "\n",
    "full_dataset = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
    "dataset = full_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T13:31:49.456922Z",
     "iopub.status.busy": "2025-05-31T13:31:49.456671Z",
     "iopub.status.idle": "2025-05-31T13:31:56.440746Z",
     "shell.execute_reply": "2025-05-31T13:31:56.439867Z",
     "shell.execute_reply.started": "2025-05-31T13:31:49.456901Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize(example, max_length=2048):\n",
    "    prompt = f\"### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:  ### L·ªánh:\\n{example['prompt']}\\n\\nT√≥m t·∫Øt:\\n\"\n",
    "    summary = example[\"summary\"]\n",
    "    prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    summary_ids = tokenizer.encode(summary, add_special_tokens=False)\n",
    "    \n",
    "    # Ki·ªÉm tra t·ªïng s·ªë token\n",
    "    total_length = len(prompt_ids) + len(summary_ids)\n",
    "    if total_length > max_length:\n",
    "        overflow = total_length - max_length\n",
    "        if overflow < len(prompt_ids):\n",
    "            prompt_ids = prompt_ids[:-overflow]\n",
    "        else:\n",
    "            prompt_ids = []\n",
    "    input_ids = prompt_ids + summary_ids\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    labels = [-100] * len(prompt_ids) + summary_ids\n",
    "\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "        attention_mask = attention_mask + [0] * padding_length\n",
    "        labels = labels + [-100] * padding_length\n",
    "    else:\n",
    "        input_ids = input_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "tokenized_dataset = dataset.map(tokenize, batched=False, fn_kwargs={\"max_length\": 2048})\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "collator = default_data_collator\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_dataset[\"test\"],\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T13:31:56.442148Z",
     "iopub.status.busy": "2025-05-31T13:31:56.441804Z",
     "iopub.status.idle": "2025-05-31T13:31:56.459998Z",
     "shell.execute_reply": "2025-05-31T13:31:56.459372Z",
     "shell.execute_reply.started": "2025-05-31T13:31:56.442116Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T13:31:56.460936Z",
     "iopub.status.busy": "2025-05-31T13:31:56.460712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in pbar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"‚úÖ Epoch {epoch+1} completed ‚Äî Avg loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-bbc-finetuned\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"‚úÖ Finetuned model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_dir = '/kaggle/working/llama3-qlora-bbc-finetuned'\n",
    "\n",
    "# T·∫°o file n√©n .zip t·ª´ th∆∞ m·ª•c\n",
    "zip_name = 'llama3-qlora-bbc-finetune'  # T√™n file n√©n (kh√¥ng c·∫ßn ƒëu√¥i .zip)\n",
    "shutil.make_archive(zip_name, 'zip', model_dir)\n",
    "\n",
    "# T·∫°o li√™n k·∫øt t·∫£i xu·ªëng cho file .zip\n",
    "display(FileLink(f'{zip_name}.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finetune with donvanban**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-25T10:52:16.026572Z",
     "iopub.status.idle": "2025-05-25T10:52:16.026934Z",
     "shell.execute_reply": "2025-05-25T10:52:16.026776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#model_id = \"/kaggle/input/llama_finetune_v1.3/transformers/default/1/data_llama_finetune_v1.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-31T11:22:11.386386Z",
     "iopub.status.idle": "2025-05-31T11:22:11.386631Z",
     "shell.execute_reply": "2025-05-31T11:22:11.386533Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/kaggle/input/data-llama-finetune/vi_data_llama_finetune.json\"\n",
    "\n",
    "full_dataset = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
    "dataset = full_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-31T11:22:11.387374Z",
     "iopub.status.idle": "2025-05-31T11:22:11.387717Z",
     "shell.execute_reply": "2025-05-31T11:22:11.387567Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize(example, max_length=2048):\n",
    "    prompt = f\"### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:  ### L·ªánh:\\n{example['prompt']}\\n\\nT√≥m t·∫Øt:\\n\"\n",
    "    summary = example[\"summary\"]\n",
    "    \n",
    "    # M√£ h√≥a ri√™ng prompt v√† summary (kh√¥ng th√™m special tokens)\n",
    "    prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    summary_ids = tokenizer.encode(summary, add_special_tokens=False)\n",
    "    \n",
    "    # Ki·ªÉm tra t·ªïng s·ªë token\n",
    "    total_length = len(prompt_ids) + len(summary_ids)\n",
    "    if total_length > max_length:\n",
    "        overflow = total_length - max_length\n",
    "        # ∆Øu ti√™n gi·ªØ l·∫°i ph·∫ßn summary; c·∫Øt b·ªõt prompt\n",
    "        if overflow < len(prompt_ids):\n",
    "            prompt_ids = prompt_ids[:-overflow]\n",
    "        else:\n",
    "            prompt_ids = []  # N·∫øu qu√° tr√†n, b·ªè h·∫øt prompt\n",
    "    \n",
    "    # N·ªëi prompt v√† summary\n",
    "    input_ids = prompt_ids + summary_ids\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    \n",
    "    # T·∫°o labels: ph·∫ßn prompt ƒë∆∞·ª£c mask b·∫±ng -100, ph·∫ßn summary gi·ªØ nguy√™n token IDs\n",
    "    labels = [-100] * len(prompt_ids) + summary_ids\n",
    "    \n",
    "    # Padding t·∫•t c·∫£ c√°c tr∆∞·ªùng v·ªÅ ƒë·ªô d√†i max_length\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "        attention_mask = attention_mask + [0] * padding_length\n",
    "        labels = labels + [-100] * padding_length\n",
    "    else:\n",
    "        # N·∫øu qu√° d√†i, c·∫Øt b·ªõt (n√™n kh√¥ng x·∫£y ra nh·ªù truncation ·ªü tr√™n)\n",
    "        input_ids = input_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "tokenized_dataset = dataset.map(tokenize, batched=False, fn_kwargs={\"max_length\": 2048})\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "collator = default_data_collator\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_dataset[\"test\"],\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-31T11:22:11.388359Z",
     "iopub.status.idle": "2025-05-31T11:22:11.388598Z",
     "shell.execute_reply": "2025-05-31T11:22:11.388502Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-31T11:22:11.389278Z",
     "iopub.status.idle": "2025-05-31T11:22:11.389548Z",
     "shell.execute_reply": "2025-05-31T11:22:11.389443Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in pbar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"‚úÖ Epoch {epoch+1} completed ‚Äî Avg loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-31T11:22:11.390394Z",
     "iopub.status.idle": "2025-05-31T11:22:11.390639Z",
     "shell.execute_reply": "2025-05-31T11:22:11.390541Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_dir = \"/kaggle/working/llama3-qlora-finetuned-donvanban\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"‚úÖ Finetuned model saved at:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-31T11:22:11.391438Z",
     "iopub.status.idle": "2025-05-31T11:22:11.391768Z",
     "shell.execute_reply": "2025-05-31T11:22:11.391616Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_dir = '/kaggle/working/llama3-qlora-finetuned-donvanban'\n",
    "\n",
    "# T·∫°o file n√©n .zip t·ª´ th∆∞ m·ª•c\n",
    "zip_name = 'llama3-qlora-donvanban-finetune'  # T√™n file n√©n (kh√¥ng c·∫ßn ƒëu√¥i .zip)\n",
    "shutil.make_archive(zip_name, 'zip', model_dir)\n",
    "\n",
    "# T·∫°o li√™n k·∫øt t·∫£i xu·ªëng cho file .zip\n",
    "display(FileLink(f'{zip_name}.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finetune with vietnamese-news-data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T15:25:19.435961Z",
     "iopub.status.busy": "2025-06-02T15:25:19.435594Z",
     "iopub.status.idle": "2025-06-02T15:25:56.327372Z",
     "shell.execute_reply": "2025-06-02T15:25:56.326569Z",
     "shell.execute_reply.started": "2025-06-02T15:25:19.435928Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/vietnamese-news-data/Dataset_articles.csv')\n",
    "df = df.sample(n=min(3000, len(df)), random_state=42).reset_index(drop=True)\n",
    "# L√†m s·∫°ch d·ªØ li·ªáu\n",
    "df['Title'] = df['Title'].fillna('').astype(str)\n",
    "df['Contents'] = df['Contents'].fillna('').astype(str)\n",
    "df['Summary'] = df['Summary'].fillna('').astype(str)\n",
    "\n",
    "# G·ªôp Title + Contents ƒë·ªÉ t·∫°o prompt\n",
    "df['FullContent'] = df['Title'] + '. ' + df['Contents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T15:25:56.328806Z",
     "iopub.status.busy": "2025-06-02T15:25:56.328541Z",
     "iopub.status.idle": "2025-06-02T15:25:56.337291Z",
     "shell.execute_reply": "2025-06-02T15:25:56.336577Z",
     "shell.execute_reply.started": "2025-06-02T15:25:56.328786Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T15:25:56.338177Z",
     "iopub.status.busy": "2025-06-02T15:25:56.337946Z",
     "iopub.status.idle": "2025-06-02T15:25:56.368277Z",
     "shell.execute_reply": "2025-06-02T15:25:56.367611Z",
     "shell.execute_reply.started": "2025-06-02T15:25:56.338151Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>URL</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Contents</th>\n",
       "      <th>Date</th>\n",
       "      <th>Author(s)</th>\n",
       "      <th>Category</th>\n",
       "      <th>Tags</th>\n",
       "      <th>FullContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>293864</td>\n",
       "      <td>https://laodong.vn/bong-da-quoc-te/nhan-dinh-c...</td>\n",
       "      <td>Nh·∫≠n ƒë·ªãnh chung k·∫øt Asian Cup 2019: Nh·∫≠t B·∫£n v...</td>\n",
       "      <td>Soi k√®o, nh·∫≠n ƒë·ªãnh, d·ª± ƒëo√°n t·ªâ s·ªë, ƒë·ªôi h√¨nh d·ª±...</td>\n",
       "      <td>Sau g·∫ßn 1 th√°ng tranh t√†i, v√≤ng chung k·∫øt Asia...</td>\n",
       "      <td>Th·ª© s√°u, 01/02/2019 13:06 (GMT+7)</td>\n",
       "      <td>Ph∆∞∆°ng Anh</td>\n",
       "      <td>Th·ªÉ thao</td>\n",
       "      <td>['Nh·∫≠t B·∫£n', 'Nh·∫≠t B·∫£n vs Qatar', 'Soi k√®o Nh·∫≠...</td>\n",
       "      <td>Nh·∫≠n ƒë·ªãnh chung k·∫øt Asian Cup 2019: Nh·∫≠t B·∫£n v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>281563</td>\n",
       "      <td>https://laodong.vn/bong-da/doi-tuyen-viet-nam-...</td>\n",
       "      <td>ƒê·ªôi tuy·ªÉn Vi·ªát Nam c√≥ ph·∫£i c√°ch ly khi d·ª± V√≤ng...</td>\n",
       "      <td>ƒê·ªôi tuy·ªÉn Vi·ªát Nam c√≥ th·ªÉ kh√¥ng ph·∫£i c√°ch ly, ...</td>\n",
       "      <td>T·∫°i cu·ªôc h·ªçp tr·ª±c tuy·∫øn v·ªõi ƒë·∫°i di·ªán Li√™n ƒëo√†n...</td>\n",
       "      <td>Th·ª© ba, 23/02/2021 14:10 (GMT+7)</td>\n",
       "      <td>PH·∫†M ƒê√åNH</td>\n",
       "      <td>Th·ªÉ thao</td>\n",
       "      <td>['ƒê·ªôi tuy·ªÉn Vi·ªát Nam', 'World Cup 2022', 'b·∫£ng...</td>\n",
       "      <td>ƒê·ªôi tuy·ªÉn Vi·ªát Nam c√≥ ph·∫£i c√°ch ly khi d·ª± V√≤ng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>11201</td>\n",
       "      <td>https://laodong.vn/thoi-su/bat-dau-quy-trinh-n...</td>\n",
       "      <td>B·∫Øt ƒë·∫ßu quy tr√¨nh nh√¢n s·ª± ch·ªß ch·ªët Nh√† n∆∞·ªõc, Q...</td>\n",
       "      <td>Ng√†y 30.3, Qu·ªëc h·ªôi b·∫Øt ƒë·∫ßu ti·∫øn h√†nh quy tr√¨n...</td>\n",
       "      <td>Theo ch∆∞∆°ng tr√¨nh k·ª≥ h·ªçp th·ª© 11, Qu·ªëc h·ªôi kho√°...</td>\n",
       "      <td>Th·ª© ba, 30/03/2021 10:48 (GMT+7)</td>\n",
       "      <td>ƒê·∫∑ng Chung - ƒê√¥ng Ph∆∞∆°ng</td>\n",
       "      <td>Th·ªùi s·ª±</td>\n",
       "      <td>['Th·ªß t∆∞·ªõng ch√≠nh ph·ªß', 'Ch·ªß t·ªãch Qu·ªëc h·ªôi', '...</td>\n",
       "      <td>B·∫Øt ƒë·∫ßu quy tr√¨nh nh√¢n s·ª± ch·ªß ch·ªët Nh√† n∆∞·ªõc, Q...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                                URL  \\\n",
       "642      293864  https://laodong.vn/bong-da-quoc-te/nhan-dinh-c...   \n",
       "700      281563  https://laodong.vn/bong-da/doi-tuyen-viet-nam-...   \n",
       "226       11201  https://laodong.vn/thoi-su/bat-dau-quy-trinh-n...   \n",
       "\n",
       "                                                 Title  \\\n",
       "642  Nh·∫≠n ƒë·ªãnh chung k·∫øt Asian Cup 2019: Nh·∫≠t B·∫£n v...   \n",
       "700  ƒê·ªôi tuy·ªÉn Vi·ªát Nam c√≥ ph·∫£i c√°ch ly khi d·ª± V√≤ng...   \n",
       "226  B·∫Øt ƒë·∫ßu quy tr√¨nh nh√¢n s·ª± ch·ªß ch·ªët Nh√† n∆∞·ªõc, Q...   \n",
       "\n",
       "                                               Summary  \\\n",
       "642  Soi k√®o, nh·∫≠n ƒë·ªãnh, d·ª± ƒëo√°n t·ªâ s·ªë, ƒë·ªôi h√¨nh d·ª±...   \n",
       "700  ƒê·ªôi tuy·ªÉn Vi·ªát Nam c√≥ th·ªÉ kh√¥ng ph·∫£i c√°ch ly, ...   \n",
       "226  Ng√†y 30.3, Qu·ªëc h·ªôi b·∫Øt ƒë·∫ßu ti·∫øn h√†nh quy tr√¨n...   \n",
       "\n",
       "                                              Contents  \\\n",
       "642  Sau g·∫ßn 1 th√°ng tranh t√†i, v√≤ng chung k·∫øt Asia...   \n",
       "700  T·∫°i cu·ªôc h·ªçp tr·ª±c tuy·∫øn v·ªõi ƒë·∫°i di·ªán Li√™n ƒëo√†n...   \n",
       "226  Theo ch∆∞∆°ng tr√¨nh k·ª≥ h·ªçp th·ª© 11, Qu·ªëc h·ªôi kho√°...   \n",
       "\n",
       "                                  Date                 Author(s)  Category  \\\n",
       "642  Th·ª© s√°u, 01/02/2019 13:06 (GMT+7)                Ph∆∞∆°ng Anh  Th·ªÉ thao   \n",
       "700   Th·ª© ba, 23/02/2021 14:10 (GMT+7)                 PH·∫†M ƒê√åNH  Th·ªÉ thao   \n",
       "226   Th·ª© ba, 30/03/2021 10:48 (GMT+7)  ƒê·∫∑ng Chung - ƒê√¥ng Ph∆∞∆°ng   Th·ªùi s·ª±   \n",
       "\n",
       "                                                  Tags  \\\n",
       "642  ['Nh·∫≠t B·∫£n', 'Nh·∫≠t B·∫£n vs Qatar', 'Soi k√®o Nh·∫≠...   \n",
       "700  ['ƒê·ªôi tuy·ªÉn Vi·ªát Nam', 'World Cup 2022', 'b·∫£ng...   \n",
       "226  ['Th·ªß t∆∞·ªõng ch√≠nh ph·ªß', 'Ch·ªß t·ªãch Qu·ªëc h·ªôi', '...   \n",
       "\n",
       "                                           FullContent  \n",
       "642  Nh·∫≠n ƒë·ªãnh chung k·∫øt Asian Cup 2019: Nh·∫≠t B·∫£n v...  \n",
       "700  ƒê·ªôi tuy·ªÉn Vi·ªát Nam c√≥ ph·∫£i c√°ch ly khi d·ª± V√≤ng...  \n",
       "226  B·∫Øt ƒë·∫ßu quy tr√¨nh nh√¢n s·ª± ch·ªß ch·ªët Nh√† n∆∞·ªõc, Q...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T15:25:56.369233Z",
     "iopub.status.busy": "2025-06-02T15:25:56.369024Z",
     "iopub.status.idle": "2025-06-02T15:26:07.443834Z",
     "shell.execute_reply": "2025-06-02T15:26:07.442768Z",
     "shell.execute_reply.started": "2025-06-02T15:25:56.369215Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4241f77425a647b690fe77dfa88b210b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8212cfdfb647f9b8aaed0b87a3364d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(example, max_length=2048):\n",
    "    # T·∫°o prompt theo ƒë·ªãnh d·∫°ng chu·∫©n\n",
    "    prompt = (\n",
    "        \"### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:\\n\"\n",
    "        \"### L·ªánh:\\n\"\n",
    "        \"B·∫°n l√† m·ªôt tr·ª£ l√Ω t√≥m t·∫Øt vƒÉn b·∫£n. H√£y cung c·∫•p b·∫£n t√≥m t·∫Øt ng·∫Øn g·ªçn v√† ch√≠nh x√°c trong 150 ch·ªØ cho b√†i vi·∫øt sau.\\n\"\n",
    "        f\"B√†i vi·∫øt: {example['FullContent']}\\n\\nT√≥m t·∫Øt:\\n\"\n",
    "    )\n",
    "    summary = example[\"Summary\"]\n",
    "\n",
    "    # Token h√≥a prompt v√† summary ri√™ng bi·ªát, kh√¥ng th√™m special tokens\n",
    "    prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    summary_ids = tokenizer.encode(summary, add_special_tokens=False)\n",
    "\n",
    "    # C·∫Øt prompt n·∫øu qu√° d√†i ƒë·ªÉ ƒë·∫£m b·∫£o t·ªïng token <= max_length\n",
    "    total_length = len(prompt_ids) + len(summary_ids)\n",
    "    if total_length > max_length:\n",
    "        overflow = total_length - max_length\n",
    "        prompt_ids = prompt_ids[:-overflow] if overflow < len(prompt_ids) else []\n",
    "\n",
    "    # T·∫°o chu·ªói ƒë·∫ßu v√†o v√† attention mask\n",
    "    input_ids = prompt_ids + summary_ids\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Label: che ph·∫ßn prompt b·∫±ng -100 (kh√¥ng t√≠nh loss)\n",
    "    labels = [-100] * len(prompt_ids) + summary_ids\n",
    "\n",
    "    # Padding ƒë·∫øn max_length\n",
    "    pad_len = max_length - len(input_ids)\n",
    "    if pad_len > 0:\n",
    "        input_ids = [tokenizer.pad_token_id] * pad_len + input_ids\n",
    "        attention_mask = [0] * pad_len + attention_mask\n",
    "        labels = [-100] * pad_len + labels\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Tokenize t·ª´ng ph·∫ßn\n",
    "tokenized_train = train_dataset.map(tokenize, fn_kwargs={\"max_length\": 2048})\n",
    "tokenized_val = val_dataset.map(tokenize, fn_kwargs={\"max_length\": 2048})\n",
    "\n",
    "# Set format ƒë·ªÉ d√πng v·ªõi PyTorch DataLoader\n",
    "tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_val.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T15:26:07.445302Z",
     "iopub.status.busy": "2025-06-02T15:26:07.444951Z",
     "iopub.status.idle": "2025-06-02T15:26:07.449397Z",
     "shell.execute_reply": "2025-06-02T15:26:07.448761Z",
     "shell.execute_reply.started": "2025-06-02T15:26:07.445264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    tokenized_train,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_val,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T15:26:07.450398Z",
     "iopub.status.busy": "2025-06-02T15:26:07.450208Z",
     "iopub.status.idle": "2025-06-02T15:26:07.481403Z",
     "shell.execute_reply": "2025-06-02T15:26:07.480828Z",
     "shell.execute_reply.started": "2025-06-02T15:26:07.450382Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T02:00:32.832954Z",
     "iopub.status.busy": "2025-06-02T02:00:32.832735Z",
     "iopub.status.idle": "2025-06-02T02:00:32.836898Z",
     "shell.execute_reply": "2025-06-02T02:00:32.836102Z",
     "shell.execute_reply.started": "2025-06-02T02:00:32.832937Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_and_link_model_zip(model_dir, zip_name=None):\n",
    "    if zip_name is None:\n",
    "        zip_name = os.path.basename(os.path.normpath(model_dir))\n",
    "    \n",
    "    zip_path = shutil.make_archive(zip_name, 'zip', model_dir)\n",
    "    print(f\"‚úÖ Model saved and zipped to: {zip_path}\")\n",
    "    display(FileLink(f\"{zip_name}.zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T02:00:32.838201Z",
     "iopub.status.busy": "2025-06-02T02:00:32.837921Z",
     "iopub.status.idle": "2025-06-02T09:00:20.727368Z",
     "shell.execute_reply": "2025-06-02T09:00:20.726509Z",
     "shell.execute_reply.started": "2025-06-02T02:00:32.838173Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1200 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Loss: 1.5735: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1200/1200 [3:29:45<00:00, 10.49s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1 completed ‚Äî Avg loss: 1.6352\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1533: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1200/1200 [3:30:01<00:00, 10.50s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 2 completed ‚Äî Avg loss: 1.5741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in pbar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"‚úÖ Epoch {epoch+1} completed ‚Äî Avg loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T09:09:32.245847Z",
     "iopub.status.busy": "2025-06-02T09:09:32.245499Z",
     "iopub.status.idle": "2025-06-02T09:09:32.629234Z",
     "shell.execute_reply": "2025-06-02T09:09:32.628351Z",
     "shell.execute_reply.started": "2025-06-02T09:09:32.245817Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./llama3-qlora-finetuned-vnd/tokenizer_config.json',\n",
       " './llama3-qlora-finetuned-vnd/special_tokens_map.json',\n",
       " './llama3-qlora-finetuned-vnd/tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"./llama3-qlora-finetuned-vnd\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T09:09:38.306359Z",
     "iopub.status.busy": "2025-06-02T09:09:38.306081Z",
     "iopub.status.idle": "2025-06-02T09:09:38.923126Z",
     "shell.execute_reply": "2025-06-02T09:09:38.922111Z",
     "shell.execute_reply.started": "2025-06-02T09:09:38.306338Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='llama3-qlora-vnd-finetune.zip' target='_blank'>llama3-qlora-vnd-finetune.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/llama3-qlora-vnd-finetune.zip"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_dir = '/kaggle/working/llama3-qlora-finetuned-vnd'\n",
    "\n",
    "# T·∫°o file n√©n .zip t·ª´ th∆∞ m·ª•c\n",
    "zip_name = 'llama3-qlora-vnd-finetune'  # T√™n file n√©n (kh√¥ng c·∫ßn ƒëu√¥i .zip)\n",
    "shutil.make_archive(zip_name, 'zip', model_dir)\n",
    "\n",
    "# T·∫°o li√™n k·∫øt t·∫£i xu·ªëng cho file .zip\n",
    "display(FileLink(f'{zip_name}.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T16:00:38.762286Z",
     "iopub.status.busy": "2025-06-02T16:00:38.761923Z",
     "iopub.status.idle": "2025-06-02T16:00:38.772524Z",
     "shell.execute_reply": "2025-06-02T16:00:38.771485Z",
     "shell.execute_reply.started": "2025-06-02T16:00:38.762256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, tokenizer, device, num_samples=3, max_eval_samples=300):\n",
    "    model.eval()\n",
    "    predictions, references, samples_to_print = [], [], []\n",
    "\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bert_score = evaluate.load(\"bertscore\")\n",
    "\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    num_processed = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=128,\n",
    "                num_beams=2,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "\n",
    "            # Decode output and clean\n",
    "            raw_generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            cleaned_generated_texts = [extract_summary(text) for text in raw_generated_texts]\n",
    "\n",
    "            # Decode labels (lo·∫°i -100)\n",
    "            label_texts = [\n",
    "                tokenizer.decode([tid for tid in label.tolist() if tid != -100], skip_special_tokens=True)\n",
    "                for label in labels\n",
    "            ]\n",
    "\n",
    "            predictions.extend(cleaned_generated_texts)\n",
    "            references.extend(label_texts)\n",
    "\n",
    "            # L∆∞u l·∫°i v√†i m·∫´u ƒë·ªÉ in ra\n",
    "            raw_inputs = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "            for src, ref, pred in zip(raw_inputs, label_texts, cleaned_generated_texts):\n",
    "                if len(samples_to_print) < num_samples:\n",
    "                    samples_to_print.append((src, ref, pred))\n",
    "\n",
    "            num_processed += len(input_ids)\n",
    "            if num_processed >= max_eval_samples:\n",
    "                break\n",
    "\n",
    "    # ROUGE\n",
    "    rouge_results = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "\n",
    "    # BERTScore tr√™n 300 m·∫´u\n",
    "    bert_results = bert_score.compute(predictions=predictions[:300], references=references[:300], lang=\"vi\")\n",
    "    bert_f1 = np.mean(bert_results[\"f1\"])\n",
    "\n",
    "    return rouge_results, bert_f1, samples_to_print\n",
    "\n",
    "\n",
    "def print_evaluation_results(rouge_results, bert_f1, samples_to_print):\n",
    "    print(\"\\nüìä ROUGE Scores:\")\n",
    "    for key in rouge_results:\n",
    "        print(f\"{key}: {rouge_results[key]:.4f}\")\n",
    "\n",
    "    print(f\"\\nüìà BERTScore F1 (tr√™n 300 m·∫´u): {bert_f1:.4f}\")\n",
    "\n",
    "    print(\"\\nüìù Sample Results:\")\n",
    "    for i, (src, ref, pred) in enumerate(samples_to_print):\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        print(f\"[Prompt]    {src[:300]}...\")\n",
    "        print(f\"[Reference] {ref}\")\n",
    "        print(f\"[Generated] {pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T16:00:40.989160Z",
     "iopub.status.busy": "2025-06-02T16:00:40.988887Z",
     "iopub.status.idle": "2025-06-02T17:00:25.775250Z",
     "shell.execute_reply": "2025-06-02T17:00:25.774472Z",
     "shell.execute_reply.started": "2025-06-02T16:00:40.989139Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 149/300 [59:32<1:00:19, 23.97s/it]\n"
     ]
    }
   ],
   "source": [
    "rouge_results, bert_f1, samples = evaluate_model(model, val_loader, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T17:00:25.776466Z",
     "iopub.status.busy": "2025-06-02T17:00:25.776240Z",
     "iopub.status.idle": "2025-06-02T17:00:25.783428Z",
     "shell.execute_reply": "2025-06-02T17:00:25.782853Z",
     "shell.execute_reply.started": "2025-06-02T17:00:25.776446Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä ROUGE Scores:\n",
      "rouge1: 0.4547\n",
      "rouge2: 0.4494\n",
      "rougeL: 0.4541\n",
      "rougeLsum: 0.4541\n",
      "\n",
      "üìà BERTScore F1 (tr√™n 300 m·∫´u): 0.8032\n",
      "\n",
      "üìù Sample Results:\n",
      "\n",
      "--- Sample 1 ---\n",
      "[Prompt]    ### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:\n",
      "### L·ªánh:\n",
      "B·∫°n l√† m·ªôt tr·ª£ l√Ω t√≥m t·∫Øt vƒÉn b·∫£n. H√£y cung c·∫•p b·∫£n t√≥m t·∫Øt ng·∫Øn g·ªçn v√† ch√≠nh x√°c trong 150 ch·ªØ cho b√†i vi·∫øt sau.\n",
      "B√†i vi·∫øt: TP.H·ªì Ch√≠ Minh: M·ªát m·ªèi v·ªõi b·∫£ng gi√° ƒë·∫•t. UBND TPHCM v·ª´a giao S·ªü T√†i nguy√™n v√† M√¥i tr...\n",
      "[Reference] B·∫£ng gi√° ƒë·∫•t c·ªßa th√†nh ph·ªë H·ªì Ch√≠ Minh v·ª´a ƒë∆∞·ª£c ban h√†nh ch∆∞a ƒë∆∞·ª£c bao l√¢u nh∆∞ng ƒëang c√≥ nhi·ªÅu √Ω ki·∫øn tr√°i chi·ªÅu sau khi S·ªü T√†i nguy√™n v√† M√¥i tr∆∞·ªùng c√≥ vƒÉn b·∫£n h∆∞·ªõng d·∫´n v√† d·ª± ki·∫øn s·∫Øp t·ªõi ƒë√¢y l·∫°i c√≥ s·ª± thay ƒë·ªïi.\n",
      "[Generated] B·∫£ng gi√° ƒë·∫•t c·ªßa th√†nh ph·ªë H·ªì Ch√≠ Minh v·ª´a ƒë∆∞·ª£c ban h√†nh ch∆∞a ƒë∆∞·ª£c bao l√¢u nh∆∞ng ƒëang c√≥ nhi·ªÅu √Ω ki·∫øn tr√°i chi·ªÅu sau khi S·ªü T√†i nguy√™n v√† M√¥i tr∆∞·ªùng c√≥ vƒÉn b·∫£n h∆∞·ªõng d·∫´n v√† d·ª± ki·∫øn s·∫Øp t·ªõi ƒë√¢y l·∫°i c√≥ s·ª± thay ƒë·ªïi. Tuy nhi√™n, theo lu·∫≠t s∆∞ Ho√†ng Thu, ƒêo√†n Lu·∫≠t s∆∞ TPHCM, trong quy·∫øt ƒë·ªãnh ban h√†nh s·∫Øp t·ªõi v·ªÅ b·∫£ng gi√° ƒë·∫•t c≈©ng n√™n c√≥ m·ªôt ƒëi·ªÅu kho·∫£n chuy·ªÉn ti·∫øp, quy ƒë·ªãnh r√µ ƒëi·ªÅu n√†y. C·ª• th·ªÉ, n·∫øu quy·∫øt ƒë·ªãnh c≈© h·∫øt hi·ªáu l·ª±c m√† nh√† n∆∞·ªõc ch∆∞a ban h√†nh quy·∫øt ƒë·ªãnh m·ªõi quy ƒë·ªãnh v·ªÅ gi√° c√°c lo·∫°i ƒë·∫•t th√¨ m·∫∑c nhi√™n c√≥ th·ªÉ √°p d·ª•ng quy·∫øt ƒë·ªãnh c≈©, cho ƒë·∫øn khi c√≥ b·∫£ng gi√° ƒë·∫•t m·ªõi ban h√†nh. L√†m nh∆∞ v·∫≠y vi·ªác gi·∫£i quy·∫øt h·ªì s∆° nh√† ƒë·∫•t c·ªßa ng∆∞·ªùi d√¢n s·∫Ω kh√¥ng b·ªã √°ch t·∫Øc ng√†y n√†o.\n",
      "\n",
      "### L·ªánh:\n",
      "B·∫°n l√† m·ªôt tr·ª£ l√Ω t√≥m t·∫Øt vƒÉn b·∫£n\n",
      "\n",
      "--- Sample 2 ---\n",
      "[Prompt]    ### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:\n",
      "### L·ªánh:\n",
      "B·∫°n l√† m·ªôt tr·ª£ l√Ω t√≥m t·∫Øt vƒÉn b·∫£n. H√£y cung c·∫•p b·∫£n t√≥m t·∫Øt ng·∫Øn g·ªçn v√† ch√≠nh x√°c trong 150 ch·ªØ cho b√†i vi·∫øt sau.\n",
      "B√†i vi·∫øt: Agribank t·ªïng k·∫øt c√¥ng t√°c ƒê·∫£ng, ho·∫°t ƒë·ªông kinh doanh nƒÉm 2021 v√† tri·ªÉn khai nhi·ªám v·ª•...\n",
      "[Reference] Ng√¢n h√†ng N√¥ng nghi·ªáp v√† Ph√°t tri·ªÉn N√¥ng th√¥n Vi·ªát Nam (Agribank) v·ª´a t·ªï ch·ª©c H·ªôi ngh·ªã t·ªïng k·∫øt c√¥ng t√°c ƒê·∫£ng, ho·∫°t ƒë·ªông kinh doanh nƒÉm 2021 v√† tri·ªÉn khai nhi·ªám v·ª• nƒÉm 2022.\n",
      "[Generated] ### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:\n",
      "### L·ªánh:\n",
      "B·∫°n l√† m·ªôt tr·ª£ l√Ω t√≥m t·∫Øt vƒÉn b·∫£n. H√£y cung c·∫•p b·∫£n t√≥m t·∫Øt ng·∫Øn g·ªçn v√† ch√≠nh x√°c trong 150 ch·ªØ cho b√†i vi·∫øt sau.\n",
      "B√†i vi·∫øt: Agribank t·ªïng k·∫øt c√¥ng t√°c ƒê·∫£ng, ho·∫°t ƒë·ªông kinh doanh nƒÉm 2021 v√† tri·ªÉn khai nhi·ªám v·ª• nƒÉm 2022. NƒÉm 2021, ho·∫°t ƒë·ªông trong b·ªëi c·∫£nh nhi·ªÅu kh√≥ khƒÉn, th√°ch th·ª©c ƒë·ªëi v·ªõi ng√†nh ng√¢n h√†ng n√≥i ri√™ng v√† to√†n b·ªô n·ªÅn kinh t·∫ø n√≥i chung, b√°m s√°t c√°c ch·ªß tr∆∞∆°ng c·ªßa ƒê·∫£ng, Nh√† n∆∞·ªõc, Qu·ªëc h·ªôi, Ch√≠nh ph·ªß, ch·ªâ ƒë·∫°o c·ªßa ƒê·∫£ng ·ªßy Kh·ªëi Doanh nghi·ªáp Trung ∆∞∆°ng, Ng√¢n h√†ng Nh√† n∆∞·ªõc, s·ª± ƒë·ªìng h√†nh c·ªßa c√°c c∆° quan b·ªô ng√†nh TW v√† ƒë·ªãa ph∆∞∆°ng; s·ª± tin t∆∞·ªüng, ·ªßng h·ªô c·ªßa kh√°ch h√†ng, ƒë·ªëi t√°c, Agribank v·ªõi s·ª± quy·∫øt t√¢m, n·ªó l·ª±c chung c·ªßa to√†n h·ªá th·ªëng, t·∫≠p trung t·ªëi ƒëa m·ªçi ngu·ªìn l·ª±c v·ª´a nghi√™m t√∫c th·ª±c hi·ªán ph√≤ng ch·ªëng d·ªãch Covid-19, v·ª´a t√≠ch c·ª±c, ti√™n phong tri·ªÉn khai nhi·ªÅu ch√≠nh s√°ch, ho·∫°t ƒë·ªông h·ªó tr·ª£ kh√°ch h√†ng, ƒë·∫£m b·∫£o ho·∫°t ƒë·ªông an to√†n, th√¥ng su·ªët, hi·ªáu qu·∫£, ƒë·∫°t ƒë∆∞·ª£c k·∫øt qu·∫£ ƒë√°ng kh√≠ch l·ªá: ƒë·∫°t v√† v∆∞·ª£t 4/4 ch·ªâ ti√™u c√¥ng t√°c x√¢y d·ª±ng ƒê·∫£ng ƒë·ªÅ ra t·∫°i Ngh·ªã quy·∫øt ƒê·∫£ng b·ªô, ho√†n th√†nh to√†n di·ªán 9/9 ch·ªâ ti√™u k·∫ø ho·∫°ch kinh doanh nƒÉm 2021. ƒê·∫øn cu·ªëi nƒÉm 2021, t·ªïng t√†i s·∫£n c·ªßa Agribank ƒë·∫°t 1,68 tri·ªáu t·ªâ ƒë·ªìng; ngu·ªìn v·ªën ƒë·∫°t tr√™n 1,58 tri·ªáu t·ªâ ƒë·ªìng; t·ªïng d∆∞ n·ª£ cho vay n·ªÅn kinh t·∫ø ƒë·∫°t tr√™n 1,31 tri·ªáu t·ªâ ƒë·ªìng, trong ƒë√≥ g·∫ßn 70% d∆∞ n·ª£ cho vay ph·ª•c v·ª• ph√°t tri·ªÉn n√¥ng nghi·ªáp, n√¥ng d√¢n v√† n√¥ng th√¥n, chi·∫øm t·ª∑ tr·ªçng l·ªõn nh·∫•t trong d∆∞ n·ª£ t√≠n d·ª•ng ‚ÄúTam n√¥ng‚Äù t·∫°i Vi·ªát Nam; l·ª£i nhu·∫≠n ƒë·∫°t 14,5 ng√†n t·ªâ ƒë·ªìng; ƒë·∫£m b·∫£o c√°c t·ªâ l·ªá an to√†n ho·∫°t ƒë·ªông theo quy ƒë·ªãnh, ti·∫øp t·ª•c l√† m·ªôt trong nh·ªØng doanh nghi·ªáp c√≥ ƒë√≥ng g√≥p nhi·ªÅu nh·∫•t ƒë·ªëi v·ªõi ng√¢n s√°ch Nh√† n∆∞·ªõc. 2021 l√† nƒÉm Agribank th·ªÉ hi·ªán vai tr√≤ ti√™n phong, g∆∞∆°ng m·∫´u c·ªßa T·ªï ch·ª©c t√≠n d·ª•ng Nh√† n∆∞·ªõc trong vi·ªác ƒëi·ªÅu ch·ªânh gi·∫£m l√£i su·∫•t, ph√≠ d·ªãch v·ª• thanh to√°n h·ªó tr·ª£ kh√°ch h√†ng v∆∞·ª£t qua kh√≥ khƒÉn do ƒë·∫°i d·ªãch COVID-19. G·∫ßn 3,2 tri·ªáu kh√°ch h√†ng ƒë∆∞·ª£c Agribank ƒëi·ªÅu ch·ªânh gi·∫£m l√£i su·∫•t cho vay, t·ªïng s·ªë ti·ªÅn l√£i ƒë√£ ƒë∆∞·ª£c gi·∫£m l√† h∆°n 5.600 t·ªâ ƒë·ªìng, ƒë·ª©ng ƒë·∫ßu c√°c t·ªï ch·ª©c t√≠n d·ª•ng t·∫°i Vi·ªát Nam. Agribank tƒÉng c∆∞·ªùng c√°c ·ª©ng d·ª•ng thanh to√°n tr√™n n·ªÅn t·∫£ng c√¥ng ngh·ªá s·ªë, mi·ªÖn 100% ph√≠ d·ªãch v·ª• chuy·ªÉn ti·ªÅn trong n∆∞·ªõc. Tri·ªÉn khai nhi·ªÅu ch∆∞∆°ng tr√¨nh t√≠n d·ª•ng c√≥ quy m√¥ l·ªõn v·ªõi l√£i su·∫•t ∆∞u ƒë√£i d√†nh cho m·ªçi th√†nh ph·∫ßn kinh t·∫ø, t·∫≠p trung 5 lƒ©nh v·ª±c ∆∞u ti√™n trong ch√≠nh s√°ch t√≠n d·ª•ng g·ªìm: n√¥ng nghi·ªáp v√† n√¥ng th√¥n, xu·∫•t kh·∫©u, doanh nghi·ªáp nh·ªè v√† v·ª´a, c√¥ng nghi·ªáp h·ªó tr·ª£, ·ª©ng d·ª•ng c√¥ng ngh·ªá cao; t√≠ch c·ª±c tri·ªÉn khai hi·ªáu qu·∫£ 02 ch∆∞∆°ng tr√¨nh m·ª•c ti√™u qu·ªëc gia v·ªÅ x√¢y d·ª±ng n√¥ng th√¥n m·ªõi v√† gi·∫£m ngh√®o b·ªÅn v·ªØng. K·ªÉ t·ª´ khi d·ªãch COVID-19 xu·∫•t hi·ªán, Agribank ƒë√£ tri·ªÉn khai r·∫•t nhi·ªÅu ho·∫°t ƒë·ªông chung tay c√πng c·∫£ n∆∞·ªõc th√≠ch ·ª©ng an to√†n, ·ª©ng ph√≥ d·ªãch b·ªánh, nhanh ch√≥ng ·ªïn ƒë·ªãnh t√¨nh h√¨nh, s·ªõm ƒë∆∞a cu·ªôc s·ªëng tr·ªü l·∫°i tr·∫°ng th√°i b√¨nh th∆∞·ªùng m·ªõi. Trong nƒÉm 2021, to√†n h·ªá th·ªëng Agribank ƒë√£ ·ªßng h·ªô c√¥ng t√°c ph√≤ng ch·ªëng d·ªãch h∆°n 500 t·ª∑ ƒë·ªìng, Chung tay h·ªó tr·ª£ ti√™u th·ª• n√¥ng s·∫£n, Gian h√†ng 0 ƒë·ªìng, Tri·ªáu t√∫i an sinh, nh·ªØng ATM G·∫°o, ATM Oxy nghƒ©a t√¨nh trong t√¢m d·ªãch ƒë∆∞·ª£c Agribank ph√°t ƒë·ªông v√† tri·ªÉn khai r·ªông kh·∫Øp, lan t·ªèa tinh th·∫ßn, vƒÉn h√≥a s·∫ª chia c·ªßa ng∆∞·ªùi Agribank v·ªõi c·ªông ƒë·ªìng. Tr√°ch nhi·ªám x√£ h·ªôi c·ªßa Agribank c√≤n ƒë∆∞·ª£c kh·∫≥ng ƒë·ªãnh qua nhi·ªÅu ho·∫°t ƒë·ªông an sinh x√£ h·ªôi, t·∫≠p trung lƒ©nh v·ª±c y t·∫ø, gi√°o d·ª•c, x√¢y nh√† t√¨nh nghƒ©a, nh√† ƒë·∫°i ƒëo√†n k·∫øt, giao th√¥ng n√¥ng th√¥n. H∆∞·ªüng ·ª©ng th√¥ng ƒëi·ªáp ‚ÄúV√¨ m·ªôt Vi·ªát Nam xanh‚Äù¬† c·ªßa Th·ªß t∆∞·ªõng Ch√≠nh ph·ªß, Agribank ti·∫øp t·ª•c tri·ªÉn khai th·ª±c hi·ªán ch∆∞∆°ng tr√¨nh ‚ÄúV√¨ t∆∞∆°ng lai xanh‚Äù, tr·ªìng m·ªõi ƒë∆∞·ª£c h∆°n 1 tri·ªáu c√¢y xanh tr√™n to√†n qu·ªëc; h∆∞·ªüng ·ª©ng ph√°t ƒë·ªông c·ªßa Th·ªß t∆∞·ªõng Ch√≠nh ph·ªß ch∆∞∆°ng tr√¨nh \"S√≥ng v√† m√°y t√≠nh cho em\" h·ªó tr·ª£ c√°c em h·ªçc sinh, sinh vi√™n c√≥ ho√†n c·∫£nh kh√≥ khƒÉn, thi·∫øu ph∆∞∆°ng ti·ªán v√† ƒëi·ªÅu ki·ªán h·ªçc t·∫≠p tr·ª±c tuy·∫øn; trao t·∫∑ng t·ªß s√°ch, thi·∫øt b·ªã h·ªçc t·∫≠p ‚ÄúTh√™m con ch·ªØ, b·ªõt ƒë√≥i ngh√®o‚Äù ƒë∆∞·ª£c c·∫•p ·ªßy, ch√≠nh quy·ªÅn ƒë·ªãa ph∆∞∆°ng v√† c·ªông ƒë·ªìng x√£ h·ªôi ƒë√°nh gi√° cao v·ªÅ √Ω nghƒ©a v√† t√≠nh nh√¢n vƒÉn c·ªßa ch∆∞∆°ng tr√¨nh. Uy t√≠n, th∆∞∆°ng hi·ªáu Agribank ti·∫øp t·ª•c ƒë∆∞·ª£c n√¢ng cao. T·ªï ch·ª©c Moody‚Äôs x·∫øp h·∫°ng m·ª©c ƒë·ªô t√≠n nhi·ªám c·ªßa Agribank ·ªü m·ª©c Ba3, t∆∞∆°ng ƒë∆∞∆°ng m·ª©c t√≠n nhi·ªám qu·ªëc gia; t·∫°p ch√≠ qu·ªëc t·∫ø The Asian Banker x·∫øp h·∫°ng 138/500 ng√¢n h√†ng h√†ng ƒë·∫ßu khu v·ª±c Ch√¢u √Å Th√°i B√¨nh D∆∞∆°ng v·ªÅ quy m√¥ t√†i s·∫£n v√† tƒÉng 96 b·∫≠c x·∫øp h·∫°ng v·ªÅ ch·∫•t l∆∞·ª£ng ho·∫°t ƒë·ªông so v·ªõi c√¥ng b·ªë v√†o nƒÉm 2020. Agribank x·∫øp h·∫°ng cao nh·∫•t trong c√°c ng√¢n h√†ng Vi·ªát Nam t·∫°i B·∫£ng x·∫øp h·∫°ng 500 th∆∞∆°ng hi·ªáu ng√¢n h√†ng gi√° tr·ªã l·ªõn nh·∫•t to√†n c·∫ßu theo ƒë√°nh gi√° c·ªßa C√¥ng ty t∆∞ v·∫•n ƒë·ªãnh gi√° th∆∞∆°ng hi·ªáu h√†ng ƒë·∫ßu th·∫ø gi·ªõi Brand Finance v√† B·∫£ng x·∫øp h·∫°ng 500 Doanh nghi·ªáp l·ªõp nh·∫•t Vi·ªát Nam (VNR500). Thay m·∫∑t ƒê·∫£ng u·ª∑ Kh·ªëi DNTW, ƒë·ªìng ch√≠ L√™ VƒÉn Ch√¢u - Ph√≥ B√≠ th∆∞ ƒê·∫£ng b·ªô Kh·ªëi ghi nh·∫≠n, ƒë√°nh gi√° cao vai tr√≤ l√£nh ƒë·∫°o to√†n di·ªán c·ªßa ƒê·∫£ng ·ªßy Agribank ƒë·ªëi v·ªõi c√¥ng t√°c x√¢y d·ª±ng ƒë·∫£ng v√† l√£nh ƒë·∫°o th·ª±c hi·ªán nhi·ªám v·ª• ch√≠nh tr·ªã tr√™n c∆° s·ªü b√°m s√°t s·ª± ch·ªâ ƒë·∫°o, l√£nh ƒë·∫°o c·ªßa Trung ∆∞∆°ng, ƒê·∫£ng ·ªßy Kh·ªëi, k·ªãp th·ªùi x√¢y d·ª±ng ch∆∞∆°ng tr√¨nh, ngh·ªã quy·∫øt, k·∫ø ho·∫°ch c√¥ng t√°c, ƒë·∫£m b·∫£o tri·ªÉn khai nhi·ªám v·ª• ch√≠nh tr·ªã; qu√°n tri·ªát v√† t·ªï ch·ª©c th·ª±c hi·ªán nghi√™m t√∫c ch∆∞∆°ng tr√¨nh, k·∫ø ho·∫°ch th·ª±c hi·ªán Ngh·ªã quy·∫øt ƒë·∫°i h·ªôi ƒë·∫°i bi·ªÉu ƒê·∫£ng b·ªô Kh·ªëi v√† Ngh·ªã quy·∫øt ƒê·∫°i h·ªôi XIII c·ªßa ƒê·∫£ng; t√≠ch c·ª±c ƒë·ªïi m·ªõi n·ªôi dung, linh ho·∫°t h√¨nh th·ª©c th·ª±c hi·ªán, ƒë·∫£m b·∫£o c√¥ng t√°c ph√≤ng, ch·ªëng d·ªãch COVID-19, l√£nh ƒë·∫°o Agribank l√†m t·ªët vai tr√≤ c·ªßa ng√¢n h√†ng th∆∞∆°ng m·∫°i nh√† n∆∞·ªõc ch·ªß l·ª±c tr√™n th·ªã tr∆∞·ªùng t√†i ch√≠nh n√¥ng nghi·ªáp, n√¥ng th√¥n. Thay m·∫∑t l√£nh ƒë·∫°o Ng√¢n h√†ng Nh√† n∆∞·ªõc, ƒë·ªìng ch√≠ ƒêo√†n Th√°i S∆°n - ·ª¶y vi√™n Ban c√°n s·ª± ƒê·∫£ng, Ph√≥ Th·ªëng ƒë·ªëc ghi nh·∫≠n v√† ƒë√°nh gi√° cao nh·ªØng k·∫øt qu·∫£ n·ªïi b·∫≠t trong nƒÉm 2021 c·ªßa Agribank, nh·∫•t l√† ƒë√£ kh·∫≥ng ƒë·ªãnh vai tr√≤ ti√™n phong, g∆∞∆°ng m·∫´u trong th·ª±c thi hi·ªáu qu·∫£ c√°c ch·ªß tr∆∞∆°ng, ch√≠nh s√°ch c·ªßa ƒê·∫£ng, Nh√† n∆∞·ªõc, Ch√≠nh ph·ªß v√† c·ªßa ng√†nh, ƒë·∫∑c bi·ªát l√† trong ƒë·∫ßu t∆∞ ph√°t tri·ªÉn n√¥ng nghi·ªáp, n√¥ng th√¥n v√† k·ªãp th·ªùi tri·ªÉn khai nhi·ªÅu gi·∫£i ph√°p h·ªó tr·ª£ kh√°ch h√†ng v∆∞·ª£t qua kh√≥ khƒÉn do ·∫£nh h∆∞·ªüng d·ªãch COVID-19. ƒê·ªìng ch√≠ Ph√≥ Th·ªëng ƒë·ªëc c≈©ng l∆∞u √Ω m·ªôt s·ªë v·∫•n ƒë·ªÅ Agribank c·∫ßn quan t√¢m ƒë·ªÉ ho·∫°t ƒë·ªông t·ªët h∆°n n·ªØa trong nƒÉm 2022 tr∆∞·ªõc b·ªëi c·∫£nh d·ªãch b·ªánh c√≤n di·ªÖn bi·∫øn ph·ª©c t·∫°p v√† nh·ªØng th√°ch th·ª©c ƒë·∫∑t ra ƒë·ªëi v·ªõi ng√†nh ng√¢n h√†ng, trong ƒë√≥ c√≥ Agribank, nh·∫•t l√† v·ªÅ qu√° tr√¨nh chuy·ªÉn ƒë·ªïi s·ªë. Ti·∫øp thu √Ω ki·∫øn ch·ªâ ƒë·∫°o c·ªßa l√£nh ƒë·∫°o ƒê·∫£ng ·ªßy Kh·ªëi DNTW v√† NHNN, ƒë·ªìng ch√≠ Ph·∫°m ƒê·ª©c ·∫§n - B√≠ th∆∞ ƒê·∫£ng ·ªßy, Ch·ªß t·ªãch HƒêTV kh·∫≥ng ƒë·ªãnh c√πng v·ªõi vi·ªác kh·∫Øc ph·ª•c nh·ªØng v·∫•n ƒë·ªÅ c√≤n t·ªìn t·∫°i, to√†n h·ªá th·ªëng Agribank quy·∫øt t√¢m ti·∫øp t·ª•c th·ª±c hi·ªán hi·ªáu qu·∫£ Ngh·ªã quy·∫øt ƒê·∫°i h·ªôi ƒê·∫£ng to√†n qu·ªëc l·∫ßn th·ª© XIII, Ngh·ªã quy·∫øt ƒê·∫°i h·ªôi ƒê·∫£ng b·ªô Kh·ªëi Doanh nghi·ªáp Trung ∆∞∆°ng l·∫ßn th·ª© III, Ngh·ªã quy·∫øt ƒê·∫°i h·ªôi ƒê·∫£ng b·ªô Agribank l·∫ßn th·ª© X v√† c√°c Ngh·ªã quy·∫øt chuy√™n ƒë·ªÅ. ∆Øu ti√™n ngu·ªìn l·ª±c ph√°t tri·ªÉn khoa h·ªçc c√¥ng ngh·ªá, tri·ªÉn khai c√°c d·ª± √°n ng√¢n h√†ng s·ªë, ph√°t tri·ªÉn ngu·ªìn nh√¢n l·ª±c ch·∫•t l∆∞·ª£ng cao, n√¢ng cao nƒÉng l·ª±c qu·∫£n tr·ªã, nƒÉng l·ª±c t√†i ch√≠nh, c·∫£i ti·∫øn quy tr√¨nh, th·ªß t·ª•c, n√¢ng cao ch·∫•t l∆∞·ª£ng d·ªãch v·ª• ‚ÄúL·∫•y kh√°ch h√†ng l√†m trung t√¢m‚Äù. Ki√™n ƒë·ªãnh m·ª•c ti√™u ti·∫øp t·ª•c c√πng Ch√≠nh ph·ªß, ng√†nh Ng√¢n h√†ng v√† nh√¢n d√¢n c·∫£ n∆∞·ªõc ki·ªÉm so√°t d·ªãch b·ªánh, ph·ª•c h·ªìi v√† ph√°t tri·ªÉn s·∫£n xu·∫•t kinh doanh, gi·ªØ v·ªØng ·ªïn ƒë·ªãnh kinh t·∫ø vƒ© m√¥, b·∫£o ƒë·∫£m an sinh x√£ h·ªôi, ti·∫øp t·ª•c kh·∫≥ng ƒë·ªãnh v·ªã th·∫ø, vai tr√≤ Ng√¢n h√†ng th∆∞∆°ng m·∫°i h√†ng ƒë·∫ßu Vi·ªát Nam, ch·ªß l·ª±c tr√™n th·ªã tr∆∞·ªùng t√†i ch√≠nh, nh·∫•t l√† n√¥ng nghi·ªáp, n√¥ng th√¥n, chu·∫©n b·ªã cho qu√° tr√¨nh chuy·ªÉn ƒë·ªïi th√†nh ng√¢n h√†ng th∆∞∆°ng m·∫°i c·ªï ph·∫ßn. T·∫°i H·ªôi ngh·ªã, Agribank vinh d·ª± ƒë√≥n nh·∫≠n c√°c danh hi·ªáu thi ƒëua cao qu√Ω do Nh√† n∆∞·ªõc, Ng√¢n h√†ng Nh√† n∆∞·ªõc trao t·∫∑ng: Hu√¢n ch∆∞∆°ng Lao ƒë·ªông h·∫°ng Nh√¨, Ba; B·∫±ng khen c·ªßa Th·ªß t∆∞·ªõng Ch√≠nh ph·ªß; C·ªù Thi ƒëua c·ªßa Ch√≠nh ph·ªß; C·ªù thi ƒëua c·ªßa Ng√¢n h√†ng Nh√† n∆∞·ªõc v√† c√°c danh hi·ªáu thi ƒëua ho√†n th√†nh nhi·ªám v·ª• k·∫ø ho·∫°ch kinh doanh nƒÉm 2021 c·ªßa Agribank trao t·∫∑ng c√°c t·∫≠p th·ªÉ v√† c√° nh√¢n c√≥ th√†nh t√≠ch xu·∫•t s·∫Øc trong nƒÉm 2021. C≈©ng trong khu√¥n kh·ªï H·ªôi ngh·ªã, thay m·∫∑t Ban l√£nh ƒë·∫°o Agribank, ƒë·ªìng ch√≠ Nguy·ªÖn Vi·∫øt M·∫°nh - Th√†nh vi√™n HƒêTV ph√°t ƒë·ªông phong tr√†o thi ƒëua nƒÉm 2022 v·ªõi ch·ªßNg√¢n h√†ng N√¥ng nghi·ªáp v√† Ph√°t tri·ªÉn N√¥ng th√¥n Vi·ªát Nam (Agribank) v·ª´a t·ªï ch·ª©c H·ªôi ngh·ªã t·ªïng k·∫øt c√¥ng t√°c ƒê·∫£ng, ho·∫°t ƒë·ªông kinh doanh nƒÉm 2021 v√† tri·ªÉn khai nhi·ªám v·ª• nƒÉm 2022. ƒê·ªìng ch√≠ Nguy·ªÖn Vi·∫øt M·∫°nh - Th√†nh vi√™n HƒêTV ph√°t ƒë·ªông phong tr√†o thi ƒëua nƒÉm 2022 v·ªõi ch·ªßNg√¢n h√†ng N√¥ng nghi·ªáp v√† Ph√°t tri·ªÉn N√¥ng th√¥n Vi·ªát Nam (Agribank) v·ª´a t·ªï ch·ª©c H·ªôi ngh·ªã t·ªïng k·∫øt c√¥ng t√°c ƒê·∫£ng, ho·∫°t ƒë·ªông kinh doanh nƒÉm 2021 v√† tri·ªÉn khai nhi·ªám v·ª• nƒÉm 2022. C≈©ng trong khu√¥n kh·ªï H·ªôi ngh·ªã, thay m·∫∑t Ban l√£nh ƒë·∫°o Agribank, ƒë·ªìng ch√≠ Nguy·ªÖn Vi·∫øt M·∫°nh - Th√†nh vi√™n HƒêTV ph√°t ƒë·ªông phong tr√†o thi ƒëua nƒÉm 2022 v·ªõi ch·ªßNg√¢n h√†ng N√¥ng nghi·ªáp v√† Ph√°t tri·ªÉn\n",
      "\n",
      "--- Sample 3 ---\n",
      "[Prompt]    ### ƒê√¢y l√† d·∫°ng t√≥m t·∫Øt vƒÉn b·∫£n tin t·ª©c v·ªõi ƒë·ªô d√†i t√≥m t·∫Øt ƒë·∫ßu ra kho·∫£ng 150 t·ª´:\n",
      "### L·ªánh:\n",
      "B·∫°n l√† m·ªôt tr·ª£ l√Ω t√≥m t·∫Øt vƒÉn b·∫£n. H√£y cung c·∫•p b·∫£n t√≥m t·∫Øt ng·∫Øn g·ªçn v√† ch√≠nh x√°c trong 150 ch·ªØ cho b√†i vi·∫øt sau.\n",
      "B√†i vi·∫øt: M·∫°nh Tr∆∞·ªùng b√†y t·ªè t√¨nh y√™u v·ªõi v·ª£ nh√¢n k·ª∑ ni·ªám 13 nƒÉm ng√†y c∆∞·ªõi. K·∫øt h√¥n kh√° s·ªõm, M·∫°...\n",
      "[Reference] Cu·ªôc h√¥n nh√¢n sau nhi·ªÅu nƒÉm c·ªßa M·∫°nh Tr∆∞·ªùng v·∫´n lu√¥n ng·ªçt ng√†o, anh th∆∞·ªùng xuy√™n th·ªÉ hi·ªán s·ª± quan t√¢m ƒë·ªëi v·ªõi b√† x√£ c·ªßa m√¨nh qua nh·ªØng c·ª≠ ch·ªâ nh·ªè.\n",
      "[Generated] Cu·ªôc h√¥n nh√¢n sau nhi·ªÅu nƒÉm c·ªßa M·∫°nh Tr∆∞·ªùng v·∫´n lu√¥n ng·ªçt ng√†o, anh th∆∞·ªùng xuy√™n th·ªÉ hi·ªán s·ª± quan t√¢m ƒë·ªëi v·ªõi b√† x√£ c·ªßa m√¨nh qua nh·ªØng c·ª≠ ch·ªâ nh·ªè. M·∫°nh Tr∆∞·ªùng c≈©ng l√† ng∆∞·ªùi th·∫•u hi·ªÉu cho c√¥ng vi·ªác c·ªßa m√¨nh, kh√¥ng n·ªÅ h√† x√¥ng pha v√¨ nh·ªØng ng∆∞·ªùi th√¢n y√™u. Nam di·ªÖn vi√™n chia s·∫ª: \"Cho ƒë·∫øn b√¢y gi·ªù, d√π ƒë√£ s·ªëng v·ªõi nhau nhi·ªÅu nƒÉm nh∆∞ng t√¥i th·∫•y ch√∫ng t√¥i v·∫´n r·∫•t thanh ni√™n v√† t√¥i kh√¥ng th·∫•y m√¨nh kh√°c g√¨ so v·ªõi th·ªùi tr·∫ª c·∫£\". M·∫°nh Tr∆∞·ªùng ti·∫øt l·ªô, b√† x√£ l√† ng∆∞·ªùi r·∫•t th·∫•u hi·ªÉu cho c√¥ng vi·ªác c·ªßa anh: \"Ch√∫ng t√¥i y√™u nhau m·ªôt th·ªùi gian d√†i r·ªìi m·ªõi k·∫øt h√¥n n√™n c√†ng ng√†y c√¥ ·∫•y c√†ng hi·ªÉu t√≠nh c√°ch v√† ƒë·∫∑c th√π c√¥ng vi·ªác c·ªßa t√¥i. ƒê·∫øn th·ªùi ƒëi·ªÉm n√†y,\n"
     ]
    }
   ],
   "source": [
    "print_evaluation_results(rouge_results, bert_f1, samples)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2627880,
     "sourceId": 4701735,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3130639,
     "sourceId": 5403248,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 365387,
     "modelInstanceId": 344119,
     "sourceId": 422260,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
