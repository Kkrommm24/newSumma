import logging

from news.crawlers.baomoi.crawler import BaomoiCrawler
from news.services.news_service import save_articles_with_categories
from celery import shared_task

logger = logging.getLogger(__name__)

@shared_task
def crawl_baomoi_articles(limit=10) -> int:
    logger.info("ğŸ” Báº¯t Ä‘áº§u crawl bÃ i viáº¿t tá»« BÃ¡o Má»›i...")
    crawler = BaomoiCrawler()

    articles = crawler.crawl(limit=limit)

    if not articles:
        logger.warning("âš ï¸ KhÃ´ng cÃ³ bÃ i viáº¿t nÃ o Ä‘Æ°á»£c crawl.")
        return 0

    count = save_articles_with_categories("BÃ¡o Má»›i", "https://baomoi.com", articles)
    logger.info("âœ… HoÃ n táº¥t quÃ¡ trÃ¬nh crawl vÃ  lÆ°u bÃ i viáº¿t.")
    return count
    
